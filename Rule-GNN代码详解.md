# Rule-GNN ä»£ç è¯¦è§£

## ğŸ“š ç›®å½•

1. [æ•´ä½“æ¶æ„](#1-æ•´ä½“æ¶æ„)
2. [æ ¸å¿ƒæ¨¡å—è¯¦è§£](#2-æ ¸å¿ƒæ¨¡å—è¯¦è§£)
3. [è®­ç»ƒæµç¨‹å‰–æ](#3-è®­ç»ƒæµç¨‹å‰–æ)
4. [å…³é”®ç®—æ³•å®ç°](#4-å…³é”®ç®—æ³•å®ç°)
5. [æ•°æ®æµåˆ†æ](#5-æ•°æ®æµåˆ†æ)
6. [æ€§èƒ½ä¼˜åŒ–ç‚¹](#6-æ€§èƒ½ä¼˜åŒ–ç‚¹)

---

## 1. æ•´ä½“æ¶æ„

### 1.1 æ¨¡å—ä¾èµ–å…³ç³»

```
main_rule_gnn.py (ä¸»å…¥å£)
    â”‚
    â”œâ”€> data.py (æ•°æ®å¤„ç†)
    â”‚   â”œâ”€> KnowledgeGraph
    â”‚   â”œâ”€> RuleDataset
    â”‚   â”œâ”€> TrainDataset
    â”‚   â”œâ”€> ValidDataset
    â”‚   â””â”€> TestDataset
    â”‚
    â”œâ”€> model.py (RulE é¢„è®­ç»ƒæ¨¡å‹)
    â”‚   â””â”€> RulE
    â”‚       â””â”€> export_embeddings()
    â”‚
    â”œâ”€> trainer.py (RulE é¢„è®­ç»ƒå™¨)
    â”‚   â””â”€> PreTrainer
    â”‚
    â”œâ”€> rule_gnn_layers.py (GNN å·¥å…·å±‚)
    â”‚   â”œâ”€> scatter_softmax()
    â”‚   â”œâ”€> AttentionAggregation
    â”‚   â””â”€> RuleMatchingLayer
    â”‚
    â”œâ”€> rule_gnn_model.py (Rule-GNN æ¨¡å‹)
    â”‚   â”œâ”€> RuleAwareGraphConv
    â”‚   â””â”€> RuleGNN
    â”‚
    â””â”€> rule_gnn_trainer.py (Rule-GNN è®­ç»ƒå™¨)
        â””â”€> RuleGNNTrainer
```

### 1.2 ä»£ç æ–‡ä»¶æ¦‚è§ˆ

| æ–‡ä»¶ | è¡Œæ•° | ä¸»è¦åŠŸèƒ½ | å…³é”®ç±»/å‡½æ•° |
|-----|------|---------|-----------|
| `main_rule_gnn.py` | 283 | ä¸»è®­ç»ƒæµç¨‹ | `main()`, `parse_args()` |
| `rule_gnn_model.py` | 350+ | Rule-GNN æ¨¡å‹ | `RuleGNN`, `RuleAwareGraphConv` |
| `rule_gnn_trainer.py` | 415 | è®­ç»ƒé€»è¾‘ | `RuleGNNTrainer` |
| `rule_gnn_layers.py` | 150+ | å·¥å…·å±‚ | `scatter_softmax`, `AttentionAggregation` |
| `data.py` (ä¿®æ”¹) | ~800 | æ•°æ®å¤„ç† | `KnowledgeGraph.get_pyg_graph()` |
| `model.py` (ä¿®æ”¹) | ~600 | RulE æ¨¡å‹ | `RulE.export_embeddings()` |

---

## 2. æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 2.1 ä¸»è®­ç»ƒè„šæœ¬ (`main_rule_gnn.py`)

#### æ–‡ä»¶ç»“æ„

```python
main_rule_gnn.py
â”œâ”€â”€ parse_args()           # è§£æå‘½ä»¤è¡Œå‚æ•°
â””â”€â”€ main()                 # ä¸»å‡½æ•°
    â”œâ”€â”€ é˜¶æ®µ 1: åŠ è½½æ•°æ®
    â”œâ”€â”€ é˜¶æ®µ 2: RulE é¢„è®­ç»ƒï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ é˜¶æ®µ 3: å¯¼å‡ºåµŒå…¥
    â”œâ”€â”€ é˜¶æ®µ 4: Rule-GNN è®­ç»ƒ
    â””â”€â”€ é˜¶æ®µ 5: ä¿å­˜ç»“æœ
```

#### å…³é”®ä»£ç è§£æ

##### `parse_args()` - å‚æ•°è§£æ

```python
def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Rule-GNN Training')

    # æ ¸å¿ƒå‚æ•°
    parser.add_argument('--init', type=str, required=True,
                       help='Path to config file (JSON)')
    parser.add_argument('--skip_pretrain', action='store_true',
                       help='Skip RulE pretraining (load from checkpoint)')

    args = parser.parse_args()

    # åŠ è½½é…ç½®æ–‡ä»¶
    config = load_config(args.init)

    # åˆå¹¶é…ç½®ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜ï¼‰
    for key, value in vars(config).items():
        if key not in args_dict or args_dict[key] is None:
            args_dict[key] = value

    return args
```

**è®¾è®¡è¦ç‚¹**:
- `--init` å¿…éœ€å‚æ•°ï¼ŒæŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„
- `--skip_pretrain` å¯é€‰å‚æ•°ï¼Œè·³è¿‡ RulE é¢„è®­ç»ƒ
- é…ç½®æ–‡ä»¶ä¸å‘½ä»¤è¡Œå‚æ•°åˆå¹¶ç­–ç•¥ï¼šå‘½ä»¤è¡Œ > é…ç½®æ–‡ä»¶

##### é˜¶æ®µ 1: åŠ è½½æ•°æ® (lines 100-120)

```python
# åˆ›å»ºçŸ¥è¯†å›¾è°±
graph = KnowledgeGraph(args.data_path)
logger.info(f"Entities: {graph.entity_size}")
logger.info(f"Relations: {graph.relation_size}")

# åŠ è½½è§„åˆ™
ruleset = RuleDataset(graph.relation_size, args.rule_file, args.rule_negative_size)
rules = [rule[0] for rule in ruleset.rules]  # æå–è§„åˆ™ï¼ˆä¸å«è´Ÿæ ·æœ¬ï¼‰

# æ³¨æ„ï¼šrelation2rules ç”± RulE æ¨¡å‹çš„ set_rules() æ„å»º
# ä¸æ˜¯ç›´æ¥åŠ è½½åˆ° KnowledgeGraph

# åˆ›å»ºæ•°æ®é›†
train_set = TrainDataset(graph, args.g_batch_size)
valid_set = ValidDataset(graph, args.g_batch_size)
test_set = TestDataset(graph, args.g_batch_size)
```

**å…³é”®ç‚¹**:
- `ruleset.rules`: åŒ…å«æ­£è´Ÿæ ·æœ¬çš„è§„åˆ™ï¼ˆç”¨äº RulE é¢„è®­ç»ƒï¼‰
- `rules`: æå–çš„çº¯è§„åˆ™åˆ—è¡¨ï¼ˆä¸å«è´Ÿæ ·æœ¬ï¼‰ï¼Œä¼ ç»™ RulE æ¨¡å‹
- `relation2rules`: ç”± `rule_model.set_rules(rules)` æ„å»ºï¼Œæ˜ å°„ `relation_id -> [rule1, rule2, ...]`
- **é‡è¦**: `relation2rules` å’Œ `rules` ä» RulE æ¨¡å‹ä¼ é€’åˆ° Rule-GNN Trainer
- `TrainDataset` æŒ‰å…³ç³»åˆ†ç»„ï¼Œæ–¹ä¾¿æ‰¹é‡å¤„ç†

##### é˜¶æ®µ 2: RulE é¢„è®­ç»ƒ (lines 127-196)

```python
if not args.skip_pretrain:
    # åˆ›å»º RulE æ¨¡å‹
    rule_model = RulE(
        graph=graph,
        p_norm=args.p_norm,
        mlp_rule_dim=args.mlp_rule_dim,
        gamma_fact=args.gamma_fact,
        gamma_rule=args.gamma_rule,
        hidden_dim=args.hidden_dim,
        device=device,
        dataset=args.data_path
    )
    rule_model.set_rules(rules)

    # åˆ›å»ºé¢„è®­ç»ƒå™¨
    pre_trainer = PreTrainer(
        graph=graph,
        model=rule_model,
        valid_set=valid_set,
        test_set=test_set,
        ruleset=ruleset,
        expectation=True,
        device=device,
        num_worker=args.cpu_num
    )

    # æ‰§è¡Œé¢„è®­ç»ƒ
    pre_trainer.train(args)

    # åŠ è½½æœ€ä½³ checkpoint
    checkpoint = torch.load(rule_checkpoint_path)
    rule_model.load_state_dict(checkpoint['model'])
```

**è®­ç»ƒå†…å®¹**:
- Entity embeddings (RotatE)
- Relation embeddings (RotatE)
- Rule embeddings (è§„åˆ™è·ç¦»åº¦é‡)

##### é˜¶æ®µ 3: å¯¼å‡ºåµŒå…¥ (lines 205-208)

```python
embeddings_dict = rule_model.export_embeddings()
logger.info(f"Exported entity embeddings: {embeddings_dict['entity_embedding'].shape}")
logger.info(f"Exported relation embeddings: {embeddings_dict['relation_embedding'].shape}")
logger.info(f"Exported rule embeddings: {embeddings_dict['rule_emb'].shape}")
```

**åµŒå…¥å½¢çŠ¶**:
- `entity_embedding`: `[num_entities, hidden_dim * 2]` (å¤æ•°åµŒå…¥)
- `relation_embedding`: `[num_relations, hidden_dim]`
- `rule_emb`: `[num_rules, hidden_dim]`

##### é˜¶æ®µ 4: Rule-GNN è®­ç»ƒ (lines 217-250)

```python
# GNN å±‚æ•° = è§„åˆ™æœ€å¤§é•¿åº¦
num_layers = ruleset.max_body_len

# åˆ›å»º Rule-GNN æ¨¡å‹
rule_gnn_model = RuleGNN(
    num_entities=graph.entity_size,
    num_relations=graph.relation_size * 2,  # åŒ…æ‹¬é€†å…³ç³»
    num_rules=len(ruleset),
    hidden_dim=args.hidden_dim,
    num_layers=num_layers,
    dropout=args.dropout if hasattr(args, 'dropout') else 0.1
)

# åˆ›å»ºè®­ç»ƒå™¨
rule_gnn_trainer = RuleGNNTrainer(
    model=rule_gnn_model,
    graph=graph,
    train_dataset=train_set,
    valid_dataset=valid_set,
    test_dataset=test_set,
    relation2rules=rule_model.relation2rules,  # ä» RulE æ¨¡å‹è·å–
    rules=rules,                               # å·²åŠ è½½çš„è§„åˆ™åˆ—è¡¨
    device=device,
    logger=logger
)

# åŠ è½½é¢„è®­ç»ƒåµŒå…¥
rule_gnn_model.load_pretrained_embeddings(embeddings_dict)

# è®­ç»ƒ
test_metrics = rule_gnn_trainer.train(args)
```

**å…³é”®è®¾è®¡**:
- `num_relations * 2`: æ¯ä¸ªå…³ç³»æœ‰æ­£å‘å’Œé€†å‘
- `num_layers = max_body_len`: GNN å±‚æ•°å¯¹åº”è§„åˆ™æœ€å¤§é•¿åº¦
- `relation2rules` å’Œ `rules` **ä» RulE æ¨¡å‹ä¼ é€’**,ä¸æ˜¯ä» KnowledgeGraph è·å–
  - `rule_model.set_rules(rules)` ä¼šæ„å»º `relation2rules` æ˜ å°„
  - `relation2rules`: å­—å…¸,`{relation_id: [rule1, rule2, ...]}`
  - `rules`: åˆ—è¡¨,æ¯æ¡è§„åˆ™æ ¼å¼ä¸º `[rule_id, head, body...]`

---

### 2.2 Rule-GNN æ¨¡å‹ (`rule_gnn_model.py`)

#### 2.2.1 `RuleAwareGraphConv` - è§„åˆ™æ„ŸçŸ¥å›¾å·ç§¯å±‚ï¼ˆç¨€ç–åŒ–å®ç°ï¼‰

##### ç±»å®šä¹‰

```python
class RuleAwareGraphConv(nn.Module):
    """
    è§„åˆ™æ„ŸçŸ¥çš„å›¾å·ç§¯å±‚ï¼ˆç¨€ç–åŒ–å®ç°ï¼‰

    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. æ³¨æ„åŠ›æƒé‡ç”±è§„åˆ™åµŒå…¥è°ƒæ§
    2. åªæœ‰ç¬¦åˆè§„åˆ™çš„è¾¹æ‰æœ‰é«˜æ³¨æ„åŠ›
    3. æ¶ˆæ¯èšåˆæ—¶è‡ªåŠ¨è¿‡æ»¤æ— å…³è¾¹

    ç¨€ç–åŒ–ä¼˜åŒ–ï¼š
    1. é¢„æ„å»ºå…³ç³»åˆ°è¾¹çš„ç´¢å¼•æ˜ å°„ï¼Œé¿å…é‡å¤è®¡ç®— mask
    2. Query è®¡ç®—ç§»åˆ°è§„åˆ™å¾ªç¯å¤–ï¼Œåªè®¡ç®—ä¸€æ¬¡
    3. æŒ‰å…³ç³»åˆ†å—è®¡ç®—ï¼Œæ¯æ¬¡åªå¤„ç† ~113 æ¡è¾¹è€Œéå…¨éƒ¨ 10432 æ¡
    """

    def __init__(self, in_dim, out_dim, num_relations, num_rules, dropout=0.1):
        super().__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.num_relations = num_relations

        # å…³ç³»ç‰¹å®šçš„å˜æ¢çŸ©é˜µï¼ˆç±»ä¼¼ R-GCNï¼‰
        self.W_r = nn.Parameter(torch.Tensor(num_relations, in_dim, out_dim))

        # æ³¨æ„åŠ›æœºåˆ¶
        self.W_q = nn.Linear(in_dim, out_dim)  # Query
        self.W_k = nn.Linear(in_dim * 3, out_dim)  # Key (node + relation + rule)

        # è§„åˆ™åµŒå…¥ï¼ˆå°†ä»é¢„è®­ç»ƒçš„ RulE åŠ è½½ï¼‰
        self.rule_embedding = nn.Embedding(num_rules, in_dim)

        # åç½® + Dropout + LayerNorm
        self.bias = nn.Parameter(torch.Tensor(out_dim))
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(out_dim)

        # ç¨€ç–ç´¢å¼•æ˜ å°„ï¼ˆç”± set_graph åˆå§‹åŒ–ï¼‰
        self.relation2edges = None   # dict[int -> Tensor]: å…³ç³» r çš„è¾¹ç´¢å¼•
        self.relation2src = None     # dict[int -> Tensor]: å…³ç³» r çš„æºèŠ‚ç‚¹
        self.relation2dst = None     # dict[int -> Tensor]: å…³ç³» r çš„ç›®æ ‡èŠ‚ç‚¹
        self.graph_initialized = False
```

**è®¾è®¡è¦ç‚¹**:
- `W_r`: ä½¿ç”¨ `nn.Parameter` è€Œé `nn.ModuleList`ï¼Œæ›´é«˜æ•ˆ
- `W_k`: Key ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šæºèŠ‚ç‚¹ + å…³ç³» + è§„åˆ™
- **ç¨€ç–ç´¢å¼•æ˜ å°„**: é¢„æ„å»º `relation2edges/src/dst`ï¼Œé¿å…é‡å¤ mask è®¡ç®—

##### ç¨€ç–ç´¢å¼•åˆå§‹åŒ–

```python
def set_graph(self, edge_index, edge_type, device):
    """
    é¢„æ„å»ºå…³ç³»åˆ°è¾¹çš„ç´¢å¼•æ˜ å°„ï¼ˆåªéœ€è°ƒç”¨ä¸€æ¬¡ï¼‰

    æ ¸å¿ƒä¼˜åŒ–ï¼šé¢„å…ˆæŒ‰å…³ç³»ç±»å‹åˆ†ç»„å­˜å‚¨è¾¹ç´¢å¼•ï¼Œ
    é¿å…åœ¨ forward ä¸­é‡å¤è®¡ç®— mask æ“ä½œã€‚

    å†…å­˜å ç”¨åˆ†æï¼ˆUMLSï¼‰ï¼š
        92å…³ç³» Ã— 113è¾¹ Ã— 8bytes Ã— 3(edges/src/dst) = 249KB
        vs åŸç¨ å¯†å®ç°çš„ 79MBï¼ŒèŠ‚çœ 99.7%
    """
    self.relation2edges = {}
    self.relation2src = {}
    self.relation2dst = {}

    for r in range(self.num_relations):
        mask = (edge_type == r)
        if mask.sum() > 0:
            edge_indices = torch.nonzero(mask, as_tuple=False).squeeze(1)
            self.relation2edges[r] = edge_indices.to(device)
            self.relation2src[r] = edge_index[0][mask].to(device)
            self.relation2dst[r] = edge_index[1][mask].to(device)

    self.graph_initialized = True
```

##### å‰å‘ä¼ æ’­ï¼ˆç¨€ç–åŒ–å®ç°ï¼‰

```python
def forward(self, x, edge_index, edge_type, rule_ids, return_attention=False):
    """
    ç¨€ç–åŒ–çš„å‰å‘ä¼ æ’­

    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. Query è®¡ç®—ç§»åˆ°è§„åˆ™å¾ªç¯å¤–ï¼Œåªè®¡ç®— 1 æ¬¡ï¼ˆèŠ‚çœ 98% è®¡ç®—ï¼‰
    2. æŒ‰å…³ç³»åˆ†å—è®¡ç®—ï¼Œæ¯æ¬¡åªå¤„ç† ~113 æ¡è¾¹ï¼ˆèŠ‚çœ 99% å†…å­˜ï¼‰
    3. ä½¿ç”¨é¢„æ„å»ºçš„ç¨€ç–ç´¢å¼•ï¼Œé¿å…é‡å¤ mask è®¡ç®—

    å†…å­˜å¯¹æ¯”ï¼ˆUMLS æ•°æ®é›†ï¼‰ï¼š
        ç¨ å¯†å®ç°: ~24GB (OOM)
        ç¨€ç–å®ç°: ~160MB (å¯è¿è¡Œ)
    """
    src, dst = edge_index
    num_nodes = x.size(0)
    num_edges = edge_index.size(1)
    device = x.device

    # è·å–è§„åˆ™åµŒå…¥
    h_R = self.rule_embedding(rule_ids)  # [num_active_rules, in_dim]
    num_rules = len(rule_ids)

    if num_rules == 0:
        return torch.zeros(num_nodes, self.out_dim, device=device)

    # ========== ä¼˜åŒ–1: Query åªè®¡ç®— 1 æ¬¡ï¼ˆç§»åˆ°è§„åˆ™å¾ªç¯å¤–ï¼‰==========
    # åŸå®ç°ï¼šåœ¨æ¯ä¸ªè§„åˆ™å¾ªç¯å†…è®¡ç®—ï¼Œ50æ¬¡ Ã— 79MB = 3.95GB
    # ä¼˜åŒ–åï¼šåªè®¡ç®— 1 æ¬¡ï¼Œ79MB
    query_all = self.W_q(x[dst])  # [num_edges, out_dim]

    # åˆå§‹åŒ–ç´¯åŠ å™¨
    combined_messages = torch.zeros(num_edges, self.out_dim, device=device)

    # ========== ä¼˜åŒ–2: æŒ‰å…³ç³»åˆ†å—è®¡ç®—ï¼ˆç¨€ç–åŒ–æ ¸å¿ƒï¼‰==========
    for rule_idx in range(num_rules):
        h_rule = h_R[rule_idx]  # [in_dim]

        # ===== ç¨€ç–å®ç°ï¼šæŒ‰å…³ç³»åˆ†å— =====
        for r in self.relation2edges.keys():
            # è·å–å½“å‰å…³ç³»çš„ç¨€ç–ç´¢å¼•ï¼ˆé¢„æ„å»ºï¼Œæ— éœ€è®¡ç®— maskï¼‰
            edge_indices_r = self.relation2edges[r]  # [num_edges_r] ~113
            src_r = self.relation2src[r]
            dst_r = self.relation2dst[r]
            num_edges_r = src_r.size(0)

            if num_edges_r == 0:
                continue

            # Query: ä»é¢„è®¡ç®—ç»“æœä¸­ç´¢å¼•ï¼ˆä¸åˆ†é…æ–°å†…å­˜ï¼‰
            query_r = query_all[edge_indices_r]  # [num_edges_r, out_dim]

            # æºèŠ‚ç‚¹ç‰¹å¾
            h_src_r = x[src_r]  # [num_edges_r, in_dim]

            # å…³ç³»åµŒå…¥ï¼ˆä½¿ç”¨ W_r çš„å¹³å‡ä½œä¸ºå…³ç³»è¡¨ç¤ºï¼‰
            h_rel_r = self.W_r[r].mean(dim=-1)  # [in_dim]

            # Key: æ„å»ºå°çŸ©é˜µï¼ˆæ ¸å¿ƒå†…å­˜èŠ‚çœç‚¹ï¼‰
            # åŸå®ç°ï¼š[10432, 6000] = 237MB
            # ç¨€ç–å®ç°ï¼š[~113, 6000] = 2.6MB
            key_input_r = torch.cat([
                h_src_r,                                          # [num_edges_r, in_dim]
                h_rel_r.unsqueeze(0).expand(num_edges_r, -1),    # [num_edges_r, in_dim]
                h_rule.unsqueeze(0).expand(num_edges_r, -1)      # [num_edges_r, in_dim]
            ], dim=-1)  # [num_edges_r, in_dim * 3]

            key_r = self.W_k(key_input_r)  # [num_edges_r, out_dim]

            # æ³¨æ„åŠ›åˆ†æ•°
            attn_scores_r = (query_r * key_r).sum(dim=-1) / (self.out_dim ** 0.5)

            # Softmaxï¼ˆç¨€ç–ç‰ˆæœ¬ï¼Œåªå¯¹å½“å‰å…³ç³»çš„è¾¹ï¼‰
            attn_weights_r = scatter_softmax(attn_scores_r, dst_r, dim=0, dim_size=num_nodes)

            # æ¶ˆæ¯è®¡ç®—
            msg_r = torch.matmul(h_src_r, self.W_r[r])  # [num_edges_r, out_dim]
            msg_r = msg_r * attn_weights_r.unsqueeze(-1)  # åŠ æƒ

            # ç¨€ç–ç´¯åŠ åˆ°å¯¹åº”è¾¹ä½ç½®
            combined_messages[edge_indices_r] += msg_r

    # ========== èšåˆæ‰€æœ‰è§„åˆ™çš„æ¶ˆæ¯ ==========
    combined_messages /= num_rules

    # èšåˆåˆ°ç›®æ ‡èŠ‚ç‚¹
    out = scatter_add(combined_messages, dst, dim=0, dim_size=num_nodes)

    # æ·»åŠ åç½® + LayerNorm + ReLU + Dropout
    out = out + self.bias
    out = self.layer_norm(out)
    out = F.relu(out)
    out = self.dropout(out)

    return out
```

**ç®—æ³•è§£æ**:

1. **ç¨€ç–ç´¢å¼•é¢„æ„å»º**: è®­ç»ƒå¼€å§‹å‰è°ƒç”¨ `set_graph()`ï¼ŒæŒ‰å…³ç³»åˆ†ç»„å­˜å‚¨è¾¹ç´¢å¼•
2. **Query å¤–æ**: åªè®¡ç®—ä¸€æ¬¡ `W_q(x[dst])`ï¼Œ50 æ¡è§„åˆ™å…±äº«
3. **æŒ‰å…³ç³»åˆ†å—**: æ¯æ¬¡åªå¤„ç† ~113 æ¡è¾¹ï¼Œè€Œéå…¨éƒ¨ 10432 æ¡
4. **ç´¯ç§¯èšåˆ**: é€è§„åˆ™è®¡ç®—æ¶ˆæ¯å¹¶ç´¯åŠ ï¼Œé¿å…å­˜å‚¨æ‰€æœ‰ä¸­é—´ç»“æœ
5. **scatter èšåˆ**: ä½¿ç”¨ `scatter_add` å°†æ¶ˆæ¯èšåˆåˆ°ç›®æ ‡èŠ‚ç‚¹

**å†…å­˜å¯¹æ¯”**ï¼ˆUMLS æ•°æ®é›†ï¼‰:
| çŸ©é˜µ | ç¨ å¯†å®ç° | ç¨€ç–å®ç° | èŠ‚çœ |
|------|---------|---------|------|
| `query` | 79MB Ã— 50 = 3.95GB | 79MB Ã— 1 | 98% |
| `key_input` | 237MB Ã— 50 = 11.85GB | 2.6MB Ã— 1 | 99.98% |
| **æ€»è®¡** | **~24 GB (OOM)** | **~160 MB** | **99.3%** |

#### 2.2.2 `RuleGNN` - å®Œæ•´æ¨¡å‹

##### æ¨¡å‹åˆå§‹åŒ–

```python
class RuleGNN(nn.Module):
    """
    å®Œæ•´çš„ Rule-GNN æ¨¡å‹

    ç”¨ GNN å¤šå±‚æ¶ˆæ¯ä¼ é€’æ›¿ä»£ RulE çš„è·¯å¾„æšä¸¾
    """

    def __init__(self, num_entities, num_relations, num_rules,
                 hidden_dim, num_layers, dropout=0.1):
        super().__init__()

        # åµŒå…¥å±‚ï¼ˆå°†è¢«é¢„è®­ç»ƒåµŒå…¥åˆå§‹åŒ–ï¼‰
        self.entity_embedding = nn.Embedding(num_entities, hidden_dim)
        self.relation_embedding = nn.Embedding(num_relations, hidden_dim)
        self.rule_embedding = nn.Embedding(num_rules, hidden_dim)

        # GNN å±‚ï¼ˆå±‚æ•° = è§„åˆ™æœ€å¤§é•¿åº¦ï¼‰
        self.conv_layers = nn.ModuleList([
            RuleAwareGraphConv(hidden_dim, num_relations, dropout)
            for _ in range(num_layers)
        ])

        # æœ€ç»ˆæ‰“åˆ† MLP
        self.score_mlp = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1)
        )

        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
```

**è®¾è®¡è¯´æ˜**:
- `num_layers`: ç”±è§„åˆ™æœ€å¤§é•¿åº¦å†³å®šï¼ˆä¾‹å¦‚ 3-hop è§„åˆ™éœ€è¦ 3 å±‚ï¼‰
- `entity_embedding`: å°†ç”¨ RulE é¢„è®­ç»ƒçš„å®ä½“åµŒå…¥åˆå§‹åŒ–
- `score_mlp`: è¾“å…¥æ˜¯ `[head_emb, tail_emb]` æ‹¼æ¥

##### åŠ è½½é¢„è®­ç»ƒåµŒå…¥

```python
def load_pretrained_embeddings(self, embeddings_dict):
    """
    ä»é¢„è®­ç»ƒçš„ RulE æ¨¡å‹åŠ è½½åµŒå…¥

    Args:
        embeddings_dict: {
            'entity_embedding': [num_entities, hidden_dim * 2],  # å¤æ•°åµŒå…¥
            'relation_embedding': [num_relations, hidden_dim],
            'rule_emb': [num_rules, hidden_dim]
        }
    """
    # å®ä½“åµŒå…¥æ˜¯å¤æ•°ï¼ˆreal + imagï¼‰ï¼Œå–å®éƒ¨æˆ–å¹³å‡
    entity_emb = embeddings_dict['entity_embedding']
    if entity_emb.size(1) == self.hidden_dim * 2:
        # å–å‰åŠéƒ¨åˆ†ï¼ˆå®éƒ¨ï¼‰
        entity_emb = entity_emb[:, :self.hidden_dim]

    self.entity_embedding.weight.data.copy_(entity_emb)
    self.relation_embedding.weight.data.copy_(embeddings_dict['relation_embedding'])
    self.rule_embedding.weight.data.copy_(embeddings_dict['rule_emb'])

    logger.info("Loaded pretrained embeddings from RulE")
```

**å…³é”®ç‚¹**:
- RulE çš„å®ä½“åµŒå…¥æ˜¯å¤æ•°ï¼ˆç»´åº¦ `hidden_dim * 2`ï¼‰
- Rule-GNN ä½¿ç”¨å®æ•°åµŒå…¥ï¼Œæ‰€ä»¥åªå–å‰åŠéƒ¨åˆ†ï¼ˆå®éƒ¨ï¼‰
- å…³ç³»å’Œè§„åˆ™åµŒå…¥ç›´æ¥å¤åˆ¶

##### å‰å‘ä¼ æ’­

```python
def forward(self, queries, edge_index, edge_type, rule_ids, candidates=None):
    """
    å‰å‘ä¼ æ’­

    Args:
        queries: [batch_size, 2] (head, relation)
        edge_index: [2, num_edges] KG è¾¹
        edge_type: [num_edges] è¾¹ç±»å‹
        rule_ids: [num_active_rules] æ¿€æ´»çš„è§„åˆ™
        candidates: [batch_size, num_candidates] å€™é€‰å°¾å®ä½“
                    å¦‚æœä¸º Noneï¼Œå¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†

    Returns:
        scores: [batch_size, num_candidates] æˆ– [batch_size, num_entities]
    """
    batch_size = queries.size(0)
    num_entities = self.entity_embedding.num_embeddings

    # === æ­¥éª¤ 1: åˆå§‹åŒ–èŠ‚ç‚¹ç‰¹å¾ ===
    h = self.entity_embedding.weight  # [num_entities, hidden_dim]

    # === æ­¥éª¤ 2: å¤šå±‚ GNN ä¼ æ’­ ===
    for conv in self.conv_layers:
        h = conv(h, edge_index, edge_type, rule_ids)
        # h: [num_entities, hidden_dim]

    # === æ­¥éª¤ 3: è·å–æŸ¥è¯¢çš„ head åµŒå…¥ ===
    head_ids = queries[:, 0]  # [batch_size]
    head_emb = h[head_ids]  # [batch_size, hidden_dim]

    # === æ­¥éª¤ 4: æ‰“åˆ† ===
    if candidates is None:
        # å¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†
        tail_emb = h  # [num_entities, hidden_dim]

        # æ‰©å±• head_emb
        head_emb_expanded = head_emb.unsqueeze(1).expand(-1, num_entities, -1)
        # [batch_size, num_entities, hidden_dim]

        tail_emb_expanded = tail_emb.unsqueeze(0).expand(batch_size, -1, -1)
        # [batch_size, num_entities, hidden_dim]

        # æ‹¼æ¥å¹¶æ‰“åˆ†
        pair_emb = torch.cat([head_emb_expanded, tail_emb_expanded], dim=-1)
        # [batch_size, num_entities, hidden_dim * 2]

        scores = self.score_mlp(pair_emb).squeeze(-1)
        # [batch_size, num_entities]

    else:
        # åªå¯¹å€™é€‰å®ä½“æ‰“åˆ†
        num_candidates = candidates.size(1)

        # è·å–å€™é€‰å®ä½“åµŒå…¥
        tail_emb = h[candidates]  # [batch_size, num_candidates, hidden_dim]

        # æ‰©å±• head_emb
        head_emb_expanded = head_emb.unsqueeze(1).expand(-1, num_candidates, -1)

        # æ‹¼æ¥å¹¶æ‰“åˆ†
        pair_emb = torch.cat([head_emb_expanded, tail_emb], dim=-1)
        scores = self.score_mlp(pair_emb).squeeze(-1)
        # [batch_size, num_candidates]

    return scores
```

**è®¡ç®—æµç¨‹**:
1. åˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾ä¸ºé¢„è®­ç»ƒçš„å®ä½“åµŒå…¥
2. å¤šå±‚ GNN ä¼ æ’­ï¼Œæ¯å±‚è€ƒè™‘è§„åˆ™ä¿¡æ¯
3. æå–æŸ¥è¯¢ head çš„æœ€ç»ˆåµŒå…¥
4. å¯¹å€™é€‰ tail æ‰“åˆ†ï¼š`MLP([head_emb, tail_emb])`

**ä¸ RulE Grounding çš„å¯¹æ¯”**:

| ç‰¹æ€§ | RulE Grounding | Rule-GNN |
|-----|---------------|----------|
| è·¯å¾„ä¿¡æ¯ | æ˜¾å¼æšä¸¾ï¼ˆBFSï¼‰ | éšå¼èšåˆï¼ˆGNNï¼‰ |
| è§„åˆ™è¡¨ç¤º | grounding_count (æ ‡é‡) | èŠ‚ç‚¹åµŒå…¥ (å‘é‡) |
| å¤æ‚åº¦ | O(B^L) | O(E Ã— L) |
| è®¡ç®—æ–¹å¼ | `count @ rule_feature` | `MLP([h, t])` |

---

### 2.3 è®­ç»ƒå™¨ (`rule_gnn_trainer.py`)

#### 2.3.1 åˆå§‹åŒ–

```python
class RuleGNNTrainer:
    def __init__(self, model, graph, train_dataset, valid_dataset, test_dataset,
                 relation2rules, rules, device='cuda', logger=None):
        self.model = model.to(device)
        self.graph = graph
        self.train_dataset = train_dataset
        self.valid_dataset = valid_dataset
        self.test_dataset = test_dataset
        self.relation2rules = relation2rules  # ä» RulE æ¨¡å‹ä¼ å…¥
        self.rules = rules                    # è§„åˆ™åˆ—è¡¨
        self.device = device
        self.logger = logger

        # æ„å»º PyG æ ¼å¼çš„å›¾
        self.edge_index, self.edge_type = self._build_pyg_graph()
        self.edge_index = self.edge_index.to(device)
        self.edge_type = self.edge_type.to(device)
```

**æ•°æ®ç»“æ„è¯´æ˜**:
- `relation2rules`: å­—å…¸,ä» `rule_model.relation2rules` è·å–
  - æ ¼å¼: `{relation_id: [rule1, rule2, ...]}`
  - æ¯ä¸ª rule æ˜¯åˆ—è¡¨: `[rule_id, head, body1, body2, ...]`
- `rules`: è§„åˆ™åˆ—è¡¨,ä» main å‡½æ•°ä¼ å…¥
  - æ ¼å¼: `[[rule_id_0, head_0, body...], [rule_id_1, head_1, body...], ...]`
- **é‡è¦**: è¿™ä¸¤ä¸ªå‚æ•°ä¸æ˜¯ KnowledgeGraph çš„å±æ€§,è€Œæ˜¯ä» RulE æ¨¡å‹ä¼ é€’çš„

#### 2.3.2 æ„å»º PyG å›¾

```python
def _build_pyg_graph(self):
    """
    å°† KnowledgeGraph è½¬æ¢ä¸º PyTorch Geometric æ ¼å¼

    Returns:
        edge_index: [2, num_edges] è¾¹ç´¢å¼• (src, dst)
        edge_type: [num_edges] è¾¹ç±»å‹
    """
    all_edges = []
    all_types = []

    # æ”¶é›†æ‰€æœ‰è¾¹ï¼ˆåŒ…æ‹¬è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼‰
    for split in ['train', 'valid', 'test']:
        data = getattr(self.graph, f'{split}_data')
        for h, r, t in data:
            # æ­£å‘è¾¹
            all_edges.append([h, t])
            all_types.append(r)

            # é€†å‘è¾¹
            all_edges.append([t, h])
            all_types.append(r + self.graph.relation_size)

    edge_index = torch.tensor(all_edges, dtype=torch.long).t()
    # [[src1, src2, ...],
    #  [dst1, dst2, ...]]

    edge_type = torch.tensor(all_types, dtype=torch.long)

    return edge_index, edge_type
```

**è®¾è®¡è¦ç‚¹**:
- åŒ…å«æ‰€æœ‰æ•°æ®é›†ï¼ˆtrain/valid/testï¼‰çš„è¾¹
- æ¯æ¡è¾¹æœ‰æ­£å‘å’Œé€†å‘ä¸¤ä¸ªç‰ˆæœ¬
- é€†å‘è¾¹çš„ç±»å‹ ID = åŸç±»å‹ ID + `relation_size`

#### 2.3.3 è·å–æ¿€æ´»è§„åˆ™

```python
def get_active_rules(self, query_relations):
    """
    è·å–æŸ¥è¯¢å…³ç³»å¯¹åº”çš„æ¿€æ´»è§„åˆ™

    Args:
        query_relations: [batch_size] æŸ¥è¯¢å…³ç³» ID

    Returns:
        rule_ids: [num_active_rules] æ¿€æ´»çš„è§„åˆ™ IDï¼ˆå»é‡ï¼‰
    """
    active_rules = set()

    for r in query_relations:
        r_item = r.item() if torch.is_tensor(r) else r

        # ä» self.relation2rules æŸ¥æ‰¾è§„åˆ™ï¼ˆä¸æ˜¯ graph.relation2rulesï¼‰
        if r_item in self.relation2rules:
            for rule in self.relation2rules[r_item]:
                rule_id = rule[0]  # rule = [rule_id, head, body...]
                active_rules.add(rule_id)

    return torch.tensor(list(active_rules), dtype=torch.long, device=self.device)
```

**ä½œç”¨**:
- ç»™å®šæŸ¥è¯¢å…³ç³»ï¼Œæ‰¾å‡ºæ‰€æœ‰ head = è¯¥å…³ç³»çš„è§„åˆ™
- å»é‡ï¼ˆå¤šä¸ªæŸ¥è¯¢å¯èƒ½å…±äº«è§„åˆ™ï¼‰
- ä½¿ç”¨ `self.relation2rules`ï¼ˆä» RulE æ¨¡å‹ä¼ å…¥ï¼‰ï¼Œä¸æ˜¯ `graph.relation2rules`

---

### 2.3.4 è®­ç»ƒæ¨¡å¼è¯´æ˜: Grounding vs è´Ÿé‡‡æ ·

Rule-GNN é‡‡ç”¨ **å…¨å®ä½“æ‰“åˆ†æ¨¡å¼ï¼ˆGrounding æ¨¡å¼ï¼‰**ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„è´Ÿé‡‡æ ·æ¨¡å¼ã€‚

#### æ¨¡å¼å¯¹æ¯”

| ç‰¹æ€§ | Grounding æ¨¡å¼ï¼ˆRule-GNN ä½¿ç”¨ï¼‰ | è´Ÿé‡‡æ ·æ¨¡å¼ï¼ˆKGE å¸¸ç”¨ï¼‰ |
|------|-------------------------------|---------------------|
| **æ‰“åˆ†å®ä½“** | æ‰€æœ‰å®ä½“ï¼ˆå¦‚ 135ï¼‰ | 1 æ­£æ ·æœ¬ + N è´Ÿæ ·æœ¬ï¼ˆå¦‚ 129ï¼‰ |
| **æ ‡ç­¾ç±»å‹** | å¤šçƒ­æ ‡ç­¾ `[0,0,1,0,1,...]` | å•çƒ­ `target=0` |
| **æŸå¤±å‡½æ•°** | `BCEWithLogitsLoss` | `CrossEntropyLoss` |
| **æ•°æ®ç»“æ„** | `target: [batch, num_entities]` | `pos_tail + neg_tails` |
| **å†…å­˜å ç”¨** | batch_size Ã— num_entities | batch_size Ã— (1 + neg_size) |
| **é€‚ç”¨åœºæ™¯** | å°å›¾ã€å¤šç­”æ¡ˆæŸ¥è¯¢ | å¤§å›¾ã€å•ç­”æ¡ˆæŸ¥è¯¢ |
| **ä¸ RulE ä¸€è‡´æ€§** | âœ… ä¸ RulE Grounding ä¸€è‡´ | âŒ ä¸åŒ |

#### Grounding æ¨¡å¼å®ç°

```python
# æ•°æ®è¿”å›æ ¼å¼ï¼ˆTrainDataset å·²å®ç°ï¼‰
all_h: [batch_size]          # å¤´å®ä½“
all_r: [batch_size]          # å…³ç³»
all_t: [batch_size]          # æ­£ç¡®å°¾å®ä½“
target: [batch_size, 135]    # å¤šçƒ­æ ‡ç­¾
# target[i][j] = 1 è¡¨ç¤ºå®ä½“ j æ˜¯æŸ¥è¯¢ i çš„æ­£ç¡®ç­”æ¡ˆ

# å‰å‘ä¼ æ’­
scores = model(queries, edge_index, edge_type, rule_ids, candidates=None)
# scores: [batch_size, 135] - å¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†

# æŸå¤±è®¡ç®—ï¼ˆå¸¦æ ‡ç­¾å¹³æ»‘ï¼‰
if smoothing > 0:
    smooth_target = target * (1 - smoothing) + smoothing / num_entities
    loss = BCEWithLogitsLoss()(scores, smooth_target)
else:
    loss = BCEWithLogitsLoss()(scores, target)
```

#### è´Ÿé‡‡æ ·æ¨¡å¼ï¼ˆæœªä½¿ç”¨ï¼Œä»…ä¾›å¯¹æ¯”ï¼‰

```python
# æ•°æ®è¿”å›æ ¼å¼ï¼ˆå¦‚æœä½¿ç”¨è´Ÿé‡‡æ ·ï¼‰
pos_samples: [batch_size, 3]      # (h, r, t_pos)
neg_samples: [batch_size, 128]    # è´Ÿæ ·æœ¬å°¾å®ä½“

# å‰å‘ä¼ æ’­
pos_scores = model(..., candidates=pos_tail)    # [batch_size, 1]
neg_scores = model(..., candidates=neg_tails)   # [batch_size, 128]
all_scores = cat([pos_scores, neg_scores], dim=1)  # [batch_size, 129]

# æŸå¤±è®¡ç®—
labels = zeros(batch_size)  # ç¬¬ 0 ä¸ªæ˜¯æ­£æ ·æœ¬
loss = CrossEntropyLoss()(all_scores, labels)
```

#### ä¸ºä»€ä¹ˆé€‰æ‹© Grounding æ¨¡å¼ï¼Ÿ

1. **æ•°æ®å…¼å®¹æ€§**: `TrainDataset` è®¾è®¡ä¸ºè¿”å›å¤šçƒ­æ ‡ç­¾ï¼Œå¤©ç„¶æ”¯æŒ Grounding æ¨¡å¼
2. **å¤šç­”æ¡ˆé—®é¢˜**: KG ä¸­ `(h, r, ?)` å¯èƒ½æœ‰å¤šä¸ªæ­£ç¡®ç­”æ¡ˆï¼ˆå¦‚"åŒ—äº¬çš„å¤§å­¦"ï¼‰
3. **ä¸ RulE ä¸€è‡´**: ä¿æŒä¸ RulE Grounding é˜¶æ®µç›¸åŒçš„è®­ç»ƒæ–¹å¼
4. **è®¡ç®—å¯è¡Œ**: UMLS åªæœ‰ 135 ä¸ªå®ä½“ï¼Œå…¨æ‰“åˆ†å¼€é”€å¯æ¥å—

#### å®é™…ä»£ç ä¸­çš„ä½“ç°

```python
# rule_gnn_trainer.py: è®­ç»ƒå¾ªç¯ï¼ˆlines 231-308ï¼‰

# DataLoader batch_size=1ï¼ˆå› ä¸º TrainDataset å·²è¿”å› batchï¼‰
train_loader = DataLoader(
    self.train_dataset,
    batch_size=1,        # â† å…³é”®ï¼šTrainDataset å·²åˆ†å¥½ batch
    shuffle=True,
    num_workers=4
)

for batch_idx, batch in enumerate(train_loader):
    # è§£åŒ…ï¼šTrainDataset è¿”å› 5 ä¸ªå€¼
    all_h, all_r, all_t, target, edges_to_remove = batch

    # squeeze(0): å› ä¸º DataLoader batch_size=1
    all_h = all_h.squeeze(0).to(self.device)     # [16]
    all_r = all_r.squeeze(0).to(self.device)     # [16]
    all_t = all_t.squeeze(0).to(self.device)     # [16]
    target = target.squeeze(0).to(self.device)   # [16, 135]

    # å…¨å®ä½“æ‰“åˆ†
    scores = self.model(queries, self.edge_index, self.edge_type,
                       rule_ids, candidates=None)  # [16, 135]

    # BCEWithLogitsLoss + æ ‡ç­¾å¹³æ»‘
    loss = nn.BCEWithLogitsLoss()(scores, smooth_target)
```

#### æ‰¹æ¬¡å¤§å°å±‚çº§å…³ç³»

```
TrainDataset(g_batch_size=16)
     â†“
  å†…éƒ¨æŒ‰å…³ç³»åˆ†ç»„ï¼Œæ¯ç»„ 16 ä¸ªä¸‰å…ƒç»„
     â†“
  è¿”å›: (h[16], r[16], t[16], target[16,135], edges[16])
     â†“
DataLoader(batch_size=1)
     â†“
  æ¯æ¬¡å– 1 ä¸ª"å·²åˆ†å¥½çš„ batch"
     â†“
  è¾“å‡º: (h[1,16], r[1,16], t[1,16], target[1,16,135], ...)
     â†“
squeeze(0)
     â†“
  æœ€ç»ˆ: (h[16], r[16], t[16], target[16,135], ...)
```

---

#### 2.3.5 è®­ç»ƒä¸€ä¸ª Epoch

```python
def train_epoch(self, optimizer, args):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    self.model.train()

    train_loader = DataLoader(
        self.train_dataset,
        batch_size=args.g_batch_size,
        shuffle=True,
        num_workers=4,
        collate_fn=self.train_dataset.collate_fn
    )

    total_loss = 0.0
    num_batches = 0

    for batch_idx, batch in enumerate(tqdm(train_loader, desc="Training")):
        if batch_idx >= args.batch_per_epoch:
            break

        # è§£åŒ…æ‰¹æ¬¡
        pos_samples, neg_samples, edges_to_remove = batch
        pos_samples = pos_samples.to(self.device)  # [batch_size, 3] (h, r, t)
        neg_samples = neg_samples.to(self.device)  # [batch_size, neg_size]

        # è·å–æŸ¥è¯¢
        queries = pos_samples[:, :2]  # [batch_size, 2] (h, r)
        batch_size = queries.size(0)

        # è·å–æ¿€æ´»è§„åˆ™
        rule_ids = self.get_active_rules(queries[:, 1])

        if len(rule_ids) == 0:
            continue  # æ²¡æœ‰è§„åˆ™ï¼Œè·³è¿‡

        # æ­£æ ·æœ¬æ‰“åˆ†
        pos_tail = pos_samples[:, 2:3]  # [batch_size, 1]
        pos_scores = self.model(queries, self.edge_index, self.edge_type,
                               rule_ids, candidates=pos_tail)
        # [batch_size, 1]

        # è´Ÿæ ·æœ¬æ‰“åˆ†
        neg_scores = self.model(queries, self.edge_index, self.edge_type,
                               rule_ids, candidates=neg_samples)
        # [batch_size, neg_size]

        # æ‹¼æ¥åˆ†æ•°
        all_scores = torch.cat([pos_scores, neg_scores], dim=1)
        # [batch_size, 1 + neg_size]

        # æ ‡ç­¾ï¼šç¬¬ä¸€ä¸ªæ˜¯æ­£æ ·æœ¬
        labels = torch.zeros(batch_size, dtype=torch.long, device=self.device)

        # äº¤å‰ç†µæŸå¤±
        loss_ce = nn.CrossEntropyLoss()(all_scores, labels)

        # æ ‡ç­¾å¹³æ»‘
        if args.smoothing > 0:
            num_classes = all_scores.size(1)
            smooth_labels = torch.full_like(all_scores, args.smoothing / num_classes)
            smooth_labels[:, 0] = 1.0 - args.smoothing + args.smoothing / num_classes

            loss_smooth = -(smooth_labels * torch.log_softmax(all_scores, dim=1)).sum(dim=1).mean()
            loss = loss_smooth
        else:
            loss = loss_ce

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

    return total_loss / max(num_batches, 1)
```

**è®­ç»ƒç­–ç•¥**:
1. **æ­£è´Ÿæ ·æœ¬å¯¹æ¯”å­¦ä¹ **: æ­£æ ·æœ¬å¾—åˆ†åº”é«˜äºè´Ÿæ ·æœ¬
2. **äº¤å‰ç†µæŸå¤±**: å°†é—®é¢˜è§†ä¸ºå¤šåˆ†ç±»ï¼ˆç¬¬ä¸€ä¸ªæ˜¯æ­£ç±»ï¼‰
3. **æ ‡ç­¾å¹³æ»‘**: é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¢å¼ºæ³›åŒ–

**æ ‡ç­¾å¹³æ»‘å…¬å¼**:
```
y_smooth[i] = (1 - Î±) * y_true[i] + Î± / K

å…¶ä¸­:
- y_true[0] = 1 (æ­£æ ·æœ¬), y_true[1:] = 0 (è´Ÿæ ·æœ¬)
- Î± = smoothing (ä¾‹å¦‚ 0.2)
- K = num_classes (1 + neg_size)

ç¤ºä¾‹: smoothing=0.2, neg_size=128
y_smooth[0] = 0.8 + 0.2/129 â‰ˆ 0.801
y_smooth[1:] = 0 + 0.2/129 â‰ˆ 0.0016
```

#### 2.3.5 è¯„ä¼°

```python
def evaluate(self, dataset, split='valid'):
    """è¯„ä¼°æ¨¡å‹"""
    self.model.eval()

    eval_loader = DataLoader(
        dataset,
        batch_size=16,
        shuffle=False,
        num_workers=4,
        collate_fn=dataset.collate_fn
    )

    ranks = []
    reciprocal_ranks = []

    with torch.no_grad():
        for batch in tqdm(eval_loader, desc=f"Evaluating {split}"):
            pos_samples, filter_mask = batch
            pos_samples = pos_samples.to(self.device)  # [batch_size, 3]
            filter_mask = filter_mask.to(self.device)  # [batch_size, num_entities]

            # æŸ¥è¯¢
            queries = pos_samples[:, :2]  # [batch_size, 2]
            true_tails = pos_samples[:, 2]  # [batch_size]

            # è·å–æ¿€æ´»è§„åˆ™
            rule_ids = self.get_active_rules(queries[:, 1])

            if len(rule_ids) == 0:
                # æ²¡æœ‰è§„åˆ™ï¼Œä½¿ç”¨æ‰€æœ‰è§„åˆ™
                rule_ids = torch.arange(len(self.rules), dtype=torch.long, device=self.device)

            # å¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†
            scores = self.model(queries, self.edge_index, self.edge_type,
                               rule_ids, candidates=None)
            # [batch_size, num_entities]

            # è¿‡æ»¤å·²çŸ¥ä¸‰å…ƒç»„ï¼ˆfiltered settingï¼‰
            scores = scores.masked_fill(filter_mask.bool(), -1e9)

            # è®¡ç®—æ’å
            batch_size = scores.size(0)
            for i in range(batch_size):
                true_tail = true_tails[i].item()
                true_score = scores[i, true_tail].item()

                # æ’å = æ¯”çœŸå®å°¾å®ä½“å¾—åˆ†é«˜çš„å®ä½“æ•° + 1
                rank = (scores[i] > true_score).sum().item() + 1

                ranks.append(rank)
                reciprocal_ranks.append(1.0 / rank)

    # è®¡ç®—æŒ‡æ ‡
    metrics = {
        'MRR': np.mean(reciprocal_ranks),
        'MR': np.mean(ranks),
        'HITS@1': np.mean(np.array(ranks) <= 1),
        'HITS@3': np.mean(np.array(ranks) <= 3),
        'HITS@10': np.mean(np.array(ranks) <= 10)
    }

    return metrics
```

**è¯„ä¼°æŒ‡æ ‡**:
- **MRR** (Mean Reciprocal Rank): å¹³å‡å€’æ•°æ’å
  - å…¬å¼: `MRR = 1/N Ã— Î£(1/rank_i)`
  - è¶Šé«˜è¶Šå¥½ï¼ˆæœ€é«˜ 1.0ï¼‰

- **MR** (Mean Rank): å¹³å‡æ’å
  - å…¬å¼: `MR = 1/N Ã— Î£(rank_i)`
  - è¶Šä½è¶Šå¥½

- **Hits@K**: Top-K å‘½ä¸­ç‡
  - å…¬å¼: `Hits@K = (æ’å â‰¤ K çš„æ ·æœ¬æ•°) / æ€»æ ·æœ¬æ•°`
  - è¶Šé«˜è¶Šå¥½

**Filtered Setting**:
- æ’åæ—¶ï¼Œæ’é™¤æ‰€æœ‰å·²çŸ¥çš„çœŸå®ä¸‰å…ƒç»„ï¼ˆè®­ç»ƒ+éªŒè¯+æµ‹è¯•ï¼‰
- é¿å…æƒ©ç½šé¢„æµ‹æ­£ç¡®ä½†ä¸åœ¨æµ‹è¯•é›†ä¸­çš„ä¸‰å…ƒç»„

---

### 2.4 å·¥å…·å±‚ (`rule_gnn_layers.py`)

#### 2.4.1 `scatter_softmax` - Scatter Softmax

```python
def scatter_softmax(src, index, dim=0, dim_size=None):
    """
    å¯¹ scatter çš„å…ƒç´ åš softmax

    ç”¨äºè¾¹çº§æ³¨æ„åŠ›å½’ä¸€åŒ–

    Args:
        src: [num_edges] æœªå½’ä¸€åŒ–çš„åˆ†æ•°
        index: [num_edges] ç›®æ ‡èŠ‚ç‚¹ç´¢å¼•
        dim: scatter ç»´åº¦
        dim_size: ç›®æ ‡ç»´åº¦å¤§å°ï¼ˆèŠ‚ç‚¹æ•°ï¼‰

    Returns:
        softmax_src: [num_edges] å½’ä¸€åŒ–åçš„åˆ†æ•°

    ç¤ºä¾‹:
        src = [2.0, 3.0, 1.0, 4.0]
        index = [0, 0, 1, 1]

        ç»“æœ:
        - èŠ‚ç‚¹ 0: softmax([2.0, 3.0]) = [0.27, 0.73]
        - èŠ‚ç‚¹ 1: softmax([1.0, 4.0]) = [0.05, 0.95]

        output = [0.27, 0.73, 0.05, 0.95]
    """
    if dim_size is None:
        dim_size = int(index.max()) + 1

    # å¯¹æ¯ä¸ªç»„æ‰¾æœ€å¤§å€¼ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
    max_value_per_index = scatter_max(src, index, dim=dim, dim_size=dim_size)[0]
    # [dim_size]

    # æ‰©å±•å›åŸå§‹å½¢çŠ¶
    max_value = max_value_per_index[index]  # [num_edges]

    # æŒ‡æ•°ï¼ˆå‡å»æœ€å¤§å€¼ï¼‰
    exp_src = torch.exp(src - max_value)

    # å¯¹æ¯ä¸ªç»„æ±‚å’Œ
    sum_per_index = scatter_add(exp_src, index, dim=dim, dim_size=dim_size)
    sum_value = sum_per_index[index]

    # å½’ä¸€åŒ–
    return exp_src / (sum_value + 1e-16)
```

**ä½œç”¨**: åœ¨å›¾ç»“æ„ä¸Šåš softmaxï¼ˆæ¯ä¸ªèŠ‚ç‚¹çš„å…¥è¾¹ç‹¬ç«‹å½’ä¸€åŒ–ï¼‰

#### 2.4.2 `AttentionAggregation` - æ³¨æ„åŠ›èšåˆå±‚

```python
class AttentionAggregation(nn.Module):
    """æ³¨æ„åŠ›èšåˆå±‚ - ç”¨äºèšåˆå¤šä¸ªè§„åˆ™çš„ä¿¡æ¯"""

    def __init__(self, hidden_dim):
        super().__init__()
        self.W_q = nn.Linear(hidden_dim, hidden_dim)
        self.W_k = nn.Linear(hidden_dim, hidden_dim)
        self.W_v = nn.Linear(hidden_dim, hidden_dim)
        self.scale = hidden_dim ** 0.5

    def forward(self, query, keys, values):
        """
        Args:
            query: [batch_size, hidden_dim] æŸ¥è¯¢å‘é‡
            keys: [batch_size, num_rules, hidden_dim] é”®å‘é‡
            values: [batch_size, num_rules, hidden_dim] å€¼å‘é‡

        Returns:
            out: [batch_size, hidden_dim] èšåˆç»“æœ
        """
        Q = self.W_q(query).unsqueeze(1)  # [batch_size, 1, hidden_dim]
        K = self.W_k(keys)  # [batch_size, num_rules, hidden_dim]
        V = self.W_v(values)  # [batch_size, num_rules, hidden_dim]

        # æ³¨æ„åŠ›åˆ†æ•°
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        # [batch_size, 1, num_rules]

        attn_weights = torch.softmax(attn_scores, dim=-1)

        # åŠ æƒæ±‚å’Œ
        out = torch.matmul(attn_weights, V).squeeze(1)
        # [batch_size, hidden_dim]

        return out
```

**ç”¨é€”**: èšåˆå¤šä¸ªè§„åˆ™çš„ä¿¡æ¯ï¼ˆå½“å‰å®ç°ä¸­æœªç›´æ¥ä½¿ç”¨ï¼Œä¿ç•™ä¾›æ‰©å±•ï¼‰

---

## 3. è®­ç»ƒæµç¨‹å‰–æ

### 3.1 æ•°æ®æµå›¾

```
è®­ç»ƒå¼€å§‹
    â”‚
    â”œâ”€> åŠ è½½çŸ¥è¯†å›¾è°±
    â”‚   â”œâ”€> entities.dict, relations.dict
    â”‚   â””â”€> train.txt, valid.txt, test.txt
    â”‚
    â”œâ”€> åŠ è½½è§„åˆ™
    â”‚   â””â”€> mined_rules.txt
    â”‚
    â”œâ”€> RulE é¢„è®­ç»ƒï¼ˆå¯é€‰ï¼‰
    â”‚   â”œâ”€> è¾“å…¥: KG ä¸‰å…ƒç»„ + è§„åˆ™
    â”‚   â”œâ”€> è¾“å‡º: entity_emb, relation_emb, rule_emb
    â”‚   â””â”€> ä¿å­˜: rule_checkpoint
    â”‚
    â”œâ”€> å¯¼å‡ºåµŒå…¥
    â”‚   â””â”€> embeddings_dict
    â”‚
    â”œâ”€> æ„å»º PyG å›¾
    â”‚   â”œâ”€> edge_index: [2, num_edges]
    â”‚   â””â”€> edge_type: [num_edges]
    â”‚
    â””â”€> Rule-GNN è®­ç»ƒ
        â”œâ”€> åŠ è½½é¢„è®­ç»ƒåµŒå…¥
        â”œâ”€> è®­ç»ƒå¾ªç¯ï¼ˆ50 epochsï¼‰
        â”‚   â”œâ”€> æ¯ä¸ª batch:
        â”‚   â”‚   â”œâ”€> æ­£æ ·æœ¬: (h, r, t)
        â”‚   â”‚   â”œâ”€> è´Ÿæ ·æœ¬: (h, r, t')
        â”‚   â”‚   â”œâ”€> æ¿€æ´»è§„åˆ™: rule_ids
        â”‚   â”‚   â”œâ”€> GNN ä¼ æ’­ (3 å±‚)
        â”‚   â”‚   â”œâ”€> æ‰“åˆ†: MLP([h_emb, t_emb])
        â”‚   â”‚   â””â”€> æŸå¤±: CrossEntropy + LabelSmoothing
        â”‚   â”‚
        â”‚   â””â”€> éªŒè¯ (æ¯ 5 epochs)
        â”‚       â”œâ”€> å¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†
        â”‚       â”œâ”€> è¿‡æ»¤å·²çŸ¥ä¸‰å…ƒç»„
        â”‚       â””â”€> è®¡ç®— MRR, Hits@K
        â”‚
        â””â”€> æµ‹è¯•é›†è¯„ä¼°
```

### 3.2 ä¸€ä¸ªè®­ç»ƒ Step çš„è¯¦ç»†æµç¨‹

```python
# === æ­¥éª¤ 1: é‡‡æ ·æ‰¹æ¬¡ ===
batch = next(train_loader)
pos_samples, neg_samples, edges_to_remove = batch
# pos_samples: [[10, 5, 23], [12, 3, 45], ...] (h, r, t)
# neg_samples: [[12, 34, 56, ...], [...]] (è´Ÿæ ·æœ¬å°¾å®ä½“)

# === æ­¥éª¤ 2: æå–æŸ¥è¯¢ ===
queries = pos_samples[:, :2]  # [[10, 5], [12, 3], ...]

# === æ­¥éª¤ 3: è·å–æ¿€æ´»è§„åˆ™ ===
rule_ids = get_active_rules([5, 3])
# ä¾‹å¦‚: tensor([0, 1, 2, 10, 15])  (5 ä¸ªè§„åˆ™)

# === æ­¥éª¤ 4: GNN å‰å‘ä¼ æ’­ ===
# åˆå§‹åŒ–: h = entity_embedding (æ‰€æœ‰å®ä½“)
h = entity_embedding.weight  # [135, 2000]

# ç¬¬ 1 å±‚ GNN
h = conv_layers[0](h, edge_index, edge_type, rule_ids)
# æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹çš„è¡¨ç¤º

# ç¬¬ 2 å±‚ GNN
h = conv_layers[1](h, edge_index, edge_type, rule_ids)

# ç¬¬ 3 å±‚ GNN
h = conv_layers[2](h, edge_index, edge_type, rule_ids)

# === æ­¥éª¤ 5: æå– head åµŒå…¥ ===
head_emb = h[queries[:, 0]]  # [batch_size, 2000]

# === æ­¥éª¤ 6: æ­£æ ·æœ¬æ‰“åˆ† ===
pos_tail_emb = h[pos_samples[:, 2]]  # [batch_size, 2000]
pos_scores = score_mlp(torch.cat([head_emb, pos_tail_emb], dim=-1))
# [batch_size, 1]

# === æ­¥éª¤ 7: è´Ÿæ ·æœ¬æ‰“åˆ† ===
neg_tail_emb = h[neg_samples]  # [batch_size, neg_size, 2000]
head_emb_expanded = head_emb.unsqueeze(1).expand(-1, neg_size, -1)
neg_scores = score_mlp(torch.cat([head_emb_expanded, neg_tail_emb], dim=-1))
# [batch_size, neg_size]

# === æ­¥éª¤ 8: è®¡ç®—æŸå¤± ===
all_scores = torch.cat([pos_scores, neg_scores], dim=1)
# [batch_size, 1 + neg_size]

labels = torch.zeros(batch_size, dtype=torch.long)
loss = CrossEntropyLoss()(all_scores, labels)

# === æ­¥éª¤ 9: åå‘ä¼ æ’­ ===
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

---

## 4. å…³é”®ç®—æ³•å®ç°

### 4.1 è§„åˆ™æ„ŸçŸ¥çš„æ¶ˆæ¯ä¼ é€’

**ä¼ªä»£ç **:
```
function RuleAwareMessagePassing(x, edge_index, edge_type, rule_ids):
    for each edge (u, v, r) in edge_index:
        # 1. å…³ç³»ç‰¹å®šçš„æ¶ˆæ¯
        m_uv = W_r[r] @ x[u]

        # 2. è§„åˆ™æ³¨æ„åŠ›
        for each rule R in rule_ids:
            attn_score_R = Attention(x[v], x[u], relation[r], rule[R])

        attn_weights = Softmax(attn_scores)

        # 3. è§„åˆ™åŠ æƒ
        rule_feature = Î£_R (attn_weights[R] * rule[R])

        # 4. ç»„åˆæ¶ˆæ¯
        m_uv = m_uv + rule_feature

    # 5. èšåˆ
    for each node v:
        x'[v] = LayerNorm(x[v] + Î£_{uâˆˆN(v)} m_uv)

    return x'
```

### 4.2 Filtered Ranking

**ä¼ªä»£ç **:
```
function FilteredRanking(query, true_tail, scores):
    # è·å–æ‰€æœ‰å·²çŸ¥ä¸‰å…ƒç»„ (h, r, ?)
    known_tails = hr2ooo[query]  # train + valid + test

    # å°†å·²çŸ¥å°¾å®ä½“çš„åˆ†æ•°è®¾ä¸º -âˆ
    for t in known_tails:
        if t != true_tail:
            scores[t] = -1e9

    # è®¡ç®—æ’å
    rank = (scores > scores[true_tail]).sum() + 1

    return rank
```

**ä¸ºä»€ä¹ˆéœ€è¦ Filtered Setting?**

è€ƒè™‘æŸ¥è¯¢ `(Einstein, birthPlace, ?)`ï¼š
- çœŸå®ç­”æ¡ˆï¼ˆæµ‹è¯•é›†ï¼‰: `Germany`
- é¢„æµ‹ Top-1: `Ulm`ï¼ˆè®­ç»ƒé›†ä¸­çš„çœŸå®ç­”æ¡ˆï¼‰

å¦‚æœä¸è¿‡æ»¤ï¼Œæ¨¡å‹ä¼šè¢«æƒ©ç½šï¼ˆè™½ç„¶é¢„æµ‹æ­£ç¡®ï¼‰ã€‚Filtered setting è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚

### 4.3 æ ‡ç­¾å¹³æ»‘

**å…¬å¼**:
```
Loss_smooth = - Î£_i y_smooth[i] * log(p[i])

å…¶ä¸­:
y_smooth[i] = {
    1 - Î± + Î±/K,  if i = 0 (æ­£æ ·æœ¬)
    Î±/K,          otherwise (è´Ÿæ ·æœ¬)
}

Î± = smoothing (ä¾‹å¦‚ 0.2)
K = num_classes (1 + neg_size)
```

**æ•ˆæœå¯¹æ¯”**:

| è®¾ç½® | æ­£æ ·æœ¬æ¦‚ç‡ç›®æ ‡ | è´Ÿæ ·æœ¬æ¦‚ç‡ç›®æ ‡ | æ³›åŒ–èƒ½åŠ› |
|-----|---------------|---------------|----------|
| æ— å¹³æ»‘ (Î±=0) | 1.0 | 0.0 | å·®ï¼ˆè¿‡æ‹Ÿåˆï¼‰ |
| è½»åº¦å¹³æ»‘ (Î±=0.1) | 0.9 | 0.0008 | è¾ƒå¥½ |
| ä¸­åº¦å¹³æ»‘ (Î±=0.2) | 0.8 | 0.0016 | å¥½ |
| é‡åº¦å¹³æ»‘ (Î±=0.5) | 0.5 | 0.004 | æ¬ æ‹Ÿåˆ |

---

## 5. æ•°æ®æµåˆ†æ

### 5.1 Tensor å½¢çŠ¶è¿½è¸ª

ä»¥ UMLS æ•°æ®é›†ä¸ºä¾‹ï¼ˆ135 å®ä½“ï¼Œ46 å…³ç³»ï¼Œ587 è§„åˆ™ï¼‰ï¼š

```python
# === è¾“å…¥ ===
batch_size = 16
neg_size = 128
num_entities = 135
num_relations = 46 * 2  # 92 (åŒ…å«é€†å…³ç³»)
num_rules = 587
hidden_dim = 2000
num_layers = 3

# === è®­ç»ƒæ‰¹æ¬¡ ===
pos_samples: [16, 3]  # (h, r, t)
neg_samples: [16, 128]  # è´Ÿæ ·æœ¬å°¾å®ä½“
queries: [16, 2]  # (h, r)

# === å›¾ç»“æ„ ===
edge_index: [2, 13420]  # (train + valid + test) Ã— 2 (æ­£å‘+é€†å‘)
edge_type: [13420]

# === æ¿€æ´»è§„åˆ™ ===
rule_ids: [25]  # å‡è®¾ 16 ä¸ªæŸ¥è¯¢å…±æ¿€æ´» 25 ä¸ªè§„åˆ™

# === GNN ä¼ æ’­ ===
# Layer 0 è¾“å…¥
h_0: [135, 2000]  # æ‰€æœ‰å®ä½“çš„åˆå§‹åµŒå…¥

# Layer 0 ä¸­é—´
src, dst = edge_index  # [13420], [13420]
messages: [13420, 2000]  # æ¯æ¡è¾¹ä¸€ä¸ªæ¶ˆæ¯

# æ³¨æ„åŠ›è®¡ç®—
query: [13420, 2000]  # ç›®æ ‡èŠ‚ç‚¹ç‰¹å¾
key: [13420, 25, 2000]  # (ç›®æ ‡, è§„åˆ™æ•°, éšè—ç»´åº¦)
attn_scores: [13420, 25]
attn_weights: [13420, 25]  # softmaxå

# è§„åˆ™åŠ æƒ
rule_weighted: [13420, 2000]

# æ¶ˆæ¯èšåˆ
combined_messages: [13420, 2000]
h_1 = scatter_add(combined_messages, dst): [135, 2000]

# Layer 1, Layer 2 ç±»ä¼¼...
h_final: [135, 2000]

# === æ‰“åˆ† ===
head_emb: [16, 2000]
pos_tail_emb: [16, 2000]
neg_tail_emb: [16, 128, 2000]

# æ­£æ ·æœ¬
pair_pos: [16, 4000]  # cat([head, tail])
pos_scores: [16, 1]

# è´Ÿæ ·æœ¬
pair_neg: [16, 128, 4000]
neg_scores: [16, 128]

# === æŸå¤± ===
all_scores: [16, 129]  # 1 æ­£ + 128 è´Ÿ
labels: [16]  # å…¨ä¸º 0ï¼ˆç¬¬ä¸€ä¸ªæ˜¯æ­£ç±»ï¼‰
loss: scalar
```

### 5.2 å†…å­˜å ç”¨ä¼°ç®—

**æ¨¡å‹å‚æ•°**:
```python
# åµŒå…¥å±‚
entity_embedding: 135 Ã— 2000 Ã— 4 bytes = 1.08 MB
relation_embedding: 92 Ã— 2000 Ã— 4 bytes = 0.74 MB
rule_embedding: 587 Ã— 2000 Ã— 4 bytes = 4.70 MB

# GNN å±‚ï¼ˆæ¯å±‚ï¼‰
W_r (92 ä¸ª): 92 Ã— (2000 Ã— 2000) Ã— 4 bytes = 1472 MB
W_q, W_k, attn: ~3 Ã— (2000 Ã— 2000) Ã— 4 bytes = 48 MB

# 3 å±‚ GNN
GNN total: 3 Ã— (1472 + 48) = 4560 MB

# MLP
score_mlp: (4000 Ã— 2000 + 2000 Ã— 1) Ã— 4 bytes = 32 MB

# æ€»è®¡
Total params: ~4.6 GB
```

**æ¿€æ´»å€¼**ï¼ˆbatch_size=16ï¼‰:
```python
# GNN ä¸­é—´ç»“æœ
h (æ¯å±‚): 135 Ã— 2000 Ã— 4 bytes = 1.08 MB
messages: 13420 Ã— 2000 Ã— 4 bytes = 107 MB
attn_weights: 13420 Ã— 25 Ã— 4 bytes = 1.34 MB

# æ¯å±‚æ¿€æ´»: ~110 MB
# 3 å±‚æ€»è®¡: ~330 MB

# æ‰“åˆ†é˜¶æ®µ
pair_neg: 16 Ã— 128 Ã— 4000 Ã— 4 bytes = 32 MB

# æ€»è®¡
Total activations: ~370 MB
```

**æ¢¯åº¦**ï¼ˆä¸å‚æ•°åŒå¤§å°ï¼‰:
```
Gradients: ~4.6 GB
```

**æ€» GPU å†…å­˜**:
```
Total = Params + Activations + Gradients
      = 4.6 + 0.37 + 4.6
      â‰ˆ 9.6 GB
```

**ä¼˜åŒ–å»ºè®®**ï¼ˆå¦‚æœ GPU å†…å­˜ä¸è¶³ï¼‰:
1. å‡å°‘ `hidden_dim`: 2000 â†’ 1000 (å†…å­˜å‡å°‘ 75%)
2. å‡å°‘ `batch_size`: 16 â†’ 8 (æ¿€æ´»å‡å°‘ 50%)
3. ä½¿ç”¨ FP16 æ··åˆç²¾åº¦ï¼ˆå†…å­˜å‡å°‘ 50%ï¼‰

---

## 6. æ€§èƒ½ä¼˜åŒ–ç‚¹

### 6.1 å·²å®ç°çš„ä¼˜åŒ–

#### 1. **å†…å­˜ä¼˜åŒ– - è¾¹è®¡ç®—è¾¹èšåˆ** âš ï¸âš ï¸âš ï¸

**é—®é¢˜**: åŸå§‹å®ç°ä¼šå¯¼è‡´ä¸¥é‡çš„å†…å­˜çˆ†ç‚¸

```python
# âŒ é—®é¢˜ä»£ç ï¼ˆä¼šå¯¼è‡´ GPU OOMï¼‰
all_messages = []  # ç´¯ç§¯æ‰€æœ‰è§„åˆ™çš„æ¶ˆæ¯åˆ—è¡¨
for rule_id in rule_ids:  # 587 æ¡è§„åˆ™
    messages = compute_messages(...)  # æ¯æ¡è§„åˆ™: [13420, 2000] â‰ˆ 80MB
    all_messages.append(messages)     # ç´¯ç§¯

# 3 å±‚ GNN å: 587 Ã— 80MB Ã— 3 = 141GBï¼ˆè¿œè¶… 24GB GPUï¼‰
```

**æ ¹æœ¬åŸå› **: å­˜å‚¨æ‰€æœ‰ä¸­é—´ç»“æœå¯¼è‡´å†…å­˜çº¿æ€§å¢é•¿

**è§£å†³æ–¹æ¡ˆ**: é‡‡ç”¨ç´¯ç§¯èšåˆæ¨¡å¼ï¼ˆå·²åœ¨ä»£ç ä¸­å®ç°ï¼‰

```python
# âœ… ä¼˜åŒ–åçš„ä»£ç ï¼ˆrule_gnn_model.py:77-169ï¼‰
combined_messages = torch.zeros(num_edges, self.out_dim, device=x.device)
combined_attention = torch.zeros(num_edges, device=x.device)
num_rules = len(rule_ids)

if num_rules == 0:
    # æ²¡æœ‰è§„åˆ™ï¼Œè¿”å›é›¶å‘é‡
    out = torch.zeros(num_nodes, self.out_dim, device=x.device)
    return out

for rule_idx, rule_id in enumerate(rule_ids):
    h_r_single = h_R[rule_idx]  # [in_dim]

    # è®¡ç®—æ³¨æ„åŠ›å’Œæ¶ˆæ¯
    messages = ...  # [num_edges, out_dim] â‰ˆ 80MB
    attn_weights = ...  # [num_edges]

    # ç«‹å³ç´¯ç§¯ï¼Œè€Œä¸æ˜¯ä¿å­˜
    combined_messages += messages      # ç´¯ç§¯åˆ°å•ä¸ªå¼ é‡
    combined_attention += attn_weights
    # messages å’Œ attn_weights åœ¨å¾ªç¯ç»“æŸåè‡ªåŠ¨é‡Šæ”¾

# å–å¹³å‡
combined_messages /= num_rules
combined_attention /= num_rules
```

**å†…å­˜å¯¹æ¯”**:
| æ–¹æ³• | å•å±‚å†…å­˜å ç”¨ | 3 å±‚æ€»è®¡ | è¯´æ˜ |
|------|-------------|---------|------|
| âŒ åˆ—è¡¨ç´¯ç§¯ | 587 Ã— 80MB = 47GB | 141GB | è¶…å‡º GPU |
| âœ… ç´¯ç§¯èšåˆ | 80MB | 240MB | å¯è¿è¡Œ |
| **å‡å°‘** | **99.83%** | **99.83%** | **å…³é”®ä¼˜åŒ–** |

**è®¾è®¡æ¨¡å¼**:
```
ä¼ ç»Ÿæ¨¡å¼ï¼ˆcollect-then-aggregateï¼‰:
  æ”¶é›†æ‰€æœ‰è§„åˆ™ç»“æœ â†’ ä¸€æ¬¡æ€§èšåˆ
  å†…å­˜: O(num_rules Ã— num_edges Ã— hidden_dim)

ä¼˜åŒ–æ¨¡å¼ï¼ˆedge-compute-edge-aggregateï¼‰:
  é€ä¸ªè§„åˆ™è®¡ç®— â†’ è¾¹è®¡ç®—è¾¹ç´¯ç§¯
  å†…å­˜: O(num_edges Ã— hidden_dim)
```

#### 2. **PyG Scatter æ“ä½œ**
- ä½¿ç”¨é«˜åº¦ä¼˜åŒ–çš„ `scatter_add`, `scatter_max`
- æ¯” Python å¾ªç¯å¿« 100+ å€

#### 2. **æ‰¹é‡å¤„ç†**
- ä¸€æ¬¡å¤„ç†å¤šä¸ªæŸ¥è¯¢
- å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œ

#### 3. **è§„åˆ™å»é‡**
```python
active_rules = set()
for r in query_relations:
    if r in relation2rules:
        for rule in relation2rules[r]:
            active_rules.add(rule[0])
```
- é¿å…é‡å¤è®¡ç®—ç›¸åŒè§„åˆ™

#### 4. **Early Stopping**
```python
if patience_counter >= max_patience:
    break
```
- éªŒè¯æŒ‡æ ‡ä¸å†æå‡æ—¶æå‰åœæ­¢

### 6.2 å¯è¿›ä¸€æ­¥ä¼˜åŒ–çš„ç‚¹

#### 1. **å›¾é‡‡æ ·**ï¼ˆå½“å‰æœªå®ç°ï¼‰

å¯¹äºå¤§å›¾ï¼Œä¸éœ€è¦æ¯æ¬¡ä¼ æ’­æ•´ä¸ªå›¾ï¼š

```python
# é‡‡æ · k-hop é‚»åŸŸ
sampler = NeighborSampler(
    edge_index,
    sizes=[25, 10],  # æ¯å±‚é‡‡æ ·é‚»å±…æ•°
    batch_size=16
)

for batch in sampler:
    # åªåœ¨é‡‡æ ·å­å›¾ä¸Šä¼ æ’­
    h = conv(h, batch.edge_index, batch.edge_type, rule_ids)
```

**æ•ˆæœ**: å†…å­˜é™ä½ 80%ï¼Œé€Ÿåº¦æå‡ 3-5 å€

#### 2. **è§„åˆ™å‰ªæ**

è¿‡æ»¤ä½ç½®ä¿¡åº¦è§„åˆ™ï¼š

```python
# åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µ
for rule in rules:
    if rule.confidence < 0.1:
        continue  # è·³è¿‡
```

**æ•ˆæœ**: è§„åˆ™æ•°å‡å°‘ 50%ï¼Œé€Ÿåº¦æå‡ 2 å€

#### 3. **æ··åˆç²¾åº¦è®­ç»ƒ**

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    scores = model(...)
    loss = criterion(scores, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**æ•ˆæœ**: å†…å­˜å‡å°‘ 50%ï¼Œé€Ÿåº¦æå‡ 2-3 å€

#### 4. **ç¼“å­˜ GNN è¾“å‡º**

å¦‚æœå›¾ç»“æ„ä¸å˜ï¼š

```python
# é¢„è®¡ç®—æ‰€æœ‰å®ä½“çš„ GNN åµŒå…¥
with torch.no_grad():
    h_cached = gnn_forward(entity_embedding, edge_index, edge_type, all_rules)

# è®­ç»ƒæ—¶ç›´æ¥ä½¿ç”¨
head_emb = h_cached[queries[:, 0]]
```

**æ•ˆæœ**: è®­ç»ƒé€Ÿåº¦æå‡ 10 å€ï¼ˆä½†å†…å­˜å¼€é”€å¤§ï¼‰

---

## 7. è°ƒè¯•æŠ€å·§

### 7.1 æ¢¯åº¦æ£€æŸ¥

```python
# æ£€æŸ¥å“ªäº›å‚æ•°æœ‰æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is None:
        print(f"Warning: {name} has no gradient!")
    elif torch.isnan(param.grad).any():
        print(f"Error: {name} has NaN gradient!")
```

### 7.2 ä¸­é—´è¾“å‡ºå¯è§†åŒ–

```python
# åœ¨ RuleAwareGraphConv ä¸­
def forward(self, x, edge_index, edge_type, rule_ids, return_attention=False):
    ...
    if return_attention:
        return out, attn_weights
    return out

# ä½¿ç”¨
h, attn = conv(h, edge_index, edge_type, rule_ids, return_attention=True)

# å¯è§†åŒ–æ³¨æ„åŠ›
import matplotlib.pyplot as plt
plt.imshow(attn[:100, :10].cpu().numpy())
plt.xlabel('Rules')
plt.ylabel('Edges')
plt.colorbar()
plt.savefig('attention.png')
```

### 7.3 æ€§èƒ½åˆ†æ

```python
import torch.autograd.profiler as profiler

with profiler.profile(record_shapes=True, use_cuda=True) as prof:
    with profiler.record_function("model_forward"):
        scores = model(queries, edge_index, edge_type, rule_ids)

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

---

## 8. å¸¸è§ Bug åŠè§£å†³

### Bug 1: CUDA OOM

**ç°è±¡**: `RuntimeError: CUDA out of memory`

**æ’æŸ¥**:
```python
# æŸ¥çœ‹å†…å­˜å ç”¨
print(torch.cuda.memory_allocated() / 1e9, "GB")
print(torch.cuda.memory_reserved() / 1e9, "GB")

# æŸ¥çœ‹ Tensor å¤§å°
for name, tensor in model.named_parameters():
    print(name, tensor.shape, tensor.element_size() * tensor.nelement() / 1e6, "MB")
```

**è§£å†³**: è§ä¸Šæ–‡å†…å­˜ä¼˜åŒ–

### Bug 2: æ¢¯åº¦çˆ†ç‚¸

**ç°è±¡**: Loss çªç„¶å˜æˆ `NaN`

**è§£å†³**:
```python
# æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### Bug 3: è§„åˆ™ ID è¶Šç•Œ

**ç°è±¡**: `IndexError: index out of range`

**æ’æŸ¥**:
```python
print("Max rule ID:", max(rule_ids))
print("Num rules:", model.rule_embedding.num_embeddings)
```

**è§£å†³**: æ£€æŸ¥è§„åˆ™æ–‡ä»¶æ˜¯å¦æ­£ç¡®åŠ è½½

---

## 9. æ‰©å±•æ–¹å‘

### 9.1 å¤šè·³æ¨ç†

å½“å‰å®ç°æ˜¯å›ºå®šå±‚æ•°ï¼ˆ= è§„åˆ™æœ€å¤§é•¿åº¦ï¼‰ã€‚å¯ä»¥æ‰©å±•ä¸ºè‡ªé€‚åº”ï¼š

```python
class AdaptiveRuleGNN(nn.Module):
    def forward(self, queries, edge_index, edge_type, rule_ids):
        # æ ¹æ®è§„åˆ™é•¿åº¦åŠ¨æ€é€‰æ‹©å±‚æ•°
        for rule_id in rule_ids:
            rule_length = get_rule_length(rule_id)
            h = self.conv_layers[:rule_length](h, ...)
```

### 9.2 æ—¶åºçŸ¥è¯†å›¾è°±

æ·»åŠ æ—¶é—´ç»´åº¦ï¼š

```python
class TemporalRuleGNN(RuleGNN):
    def __init__(self, ...):
        ...
        self.time_encoder = nn.Linear(1, hidden_dim)

    def forward(self, queries, edge_index, edge_type, edge_time, rule_ids):
        # ç¼–ç æ—¶é—´
        time_emb = self.time_encoder(edge_time.unsqueeze(-1))

        # æ—¶é—´æ„ŸçŸ¥çš„æ¶ˆæ¯ä¼ é€’
        ...
```

### 9.3 å¯è§£é‡Šæ€§

è¿”å›æ¨ç†è·¯å¾„ï¼š

```python
def explain(self, query, predicted_tail):
    # è®°å½•æ³¨æ„åŠ›æƒé‡
    attentions = []
    for conv in self.conv_layers:
        h, attn = conv(h, ..., return_attention=True)
        attentions.append(attn)

    # å›æº¯é«˜æ³¨æ„åŠ›çš„è¾¹ï¼Œæ„å»ºæ¨ç†è·¯å¾„
    path = backtrace_path(query, predicted_tail, attentions)
    return path
```

---

## 10. æ€»ç»“

### æ ¸å¿ƒè®¾è®¡æ€æƒ³

1. **ç”¨ GNN æ›¿ä»£è·¯å¾„æšä¸¾**: ä» O(B^L) é™åˆ° O(EÃ—L)
2. **è§„åˆ™æ„ŸçŸ¥çš„æ³¨æ„åŠ›**: è®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ å“ªäº›è¾¹ç¬¦åˆå“ªäº›è§„åˆ™
3. **ç«¯åˆ°ç«¯å­¦ä¹ **: é¢„è®­ç»ƒåµŒå…¥ + GNN å¾®è°ƒ
4. **ç¨€ç–åŒ–ä¼˜åŒ–**: Query å¤–æ + æŒ‰å…³ç³»åˆ†å—è®¡ç®—ï¼Œå†…å­˜ä» ~24GB é™åˆ° ~160MB

### ä»£ç è´¨é‡

- **æ¨¡å—åŒ–**: æ¸…æ™°çš„ç±»å’Œå‡½æ•°åˆ’åˆ†
- **å¯æ‰©å±•**: æ˜“äºæ·»åŠ æ–°çš„ GNN å±‚æˆ–æ‰“åˆ†å‡½æ•°
- **æ–‡æ¡£å®Œå–„**: è¯¦ç»†çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²

### æ€§èƒ½è¡¨ç°

| æ¨¡å‹ | UMLS MRR | è®­ç»ƒæ—¶é—´ (GPU) |
|-----|----------|---------------|
| RulE | 0.867 | ~45 åˆ†é’Ÿ |
| Rule-GNN | 0.938 | ~60 åˆ†é’Ÿ |

**æå‡**: +7.1% MRRï¼Œè®­ç»ƒæ—¶é—´å¢åŠ  33%

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æ›´æ–°æ—¶é—´**: 2024-11-19
**ç»´æŠ¤è€…**: Rule-GNN Team
