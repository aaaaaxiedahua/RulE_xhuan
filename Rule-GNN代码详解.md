# Rule-GNN ä»£ç è¯¦è§£

## ğŸ“š ç›®å½•

1. [æ•´ä½“æ¶æ„](#1-æ•´ä½“æ¶æ„)
2. [æ ¸å¿ƒæ¨¡å—è¯¦è§£](#2-æ ¸å¿ƒæ¨¡å—è¯¦è§£)
3. [è®­ç»ƒæµç¨‹å‰–æ](#3-è®­ç»ƒæµç¨‹å‰–æ)
4. [å…³é”®ç®—æ³•å®ç°](#4-å…³é”®ç®—æ³•å®ç°)
5. [æ•°æ®æµåˆ†æ](#5-æ•°æ®æµåˆ†æ)
6. [æ€§èƒ½ä¼˜åŒ–ç‚¹](#6-æ€§èƒ½ä¼˜åŒ–ç‚¹)

---

## 1. æ•´ä½“æ¶æ„

### 1.1 æ¨¡å—ä¾èµ–å…³ç³»

```
main_rule_gnn.py (ä¸»å…¥å£)
    â”‚
    â”œâ”€> data.py (æ•°æ®å¤„ç†)
    â”‚   â”œâ”€> KnowledgeGraph
    â”‚   â”œâ”€> RuleDataset
    â”‚   â”œâ”€> TrainDataset
    â”‚   â”œâ”€> ValidDataset
    â”‚   â””â”€> TestDataset
    â”‚
    â”œâ”€> model.py (RulE é¢„è®­ç»ƒæ¨¡å‹)
    â”‚   â””â”€> RulE
    â”‚       â””â”€> export_embeddings()
    â”‚
    â”œâ”€> trainer.py (RulE é¢„è®­ç»ƒå™¨)
    â”‚   â””â”€> PreTrainer
    â”‚
    â”œâ”€> rule_gnn_layers.py (GNN å·¥å…·å±‚)
    â”‚   â”œâ”€> scatter_softmax()
    â”‚   â”œâ”€> AttentionAggregation
    â”‚   â””â”€> RuleMatchingLayer
    â”‚
    â”œâ”€> rule_gnn_model.py (Rule-GNN æ¨¡å‹)
    â”‚   â”œâ”€> RuleAwareGraphConv
    â”‚   â””â”€> RuleGNN
    â”‚
    â””â”€> rule_gnn_trainer.py (Rule-GNN è®­ç»ƒå™¨)
        â””â”€> RuleGNNTrainer
```

### 1.2 ä»£ç æ–‡ä»¶æ¦‚è§ˆ

| æ–‡ä»¶ | è¡Œæ•° | ä¸»è¦åŠŸèƒ½ | å…³é”®ç±»/å‡½æ•° |
|-----|------|---------|-----------|
| `main_rule_gnn.py` | 283 | ä¸»è®­ç»ƒæµç¨‹ | `main()`, `parse_args()` |
| `rule_gnn_model.py` | 350+ | Rule-GNN æ¨¡å‹ | `RuleGNN`, `RuleAwareGraphConv` |
| `rule_gnn_trainer.py` | 415 | è®­ç»ƒé€»è¾‘ | `RuleGNNTrainer` |
| `rule_gnn_layers.py` | 150+ | å·¥å…·å±‚ | `scatter_softmax`, `AttentionAggregation` |
| `data.py` (ä¿®æ”¹) | ~800 | æ•°æ®å¤„ç† | `KnowledgeGraph.get_pyg_graph()` |
| `model.py` (ä¿®æ”¹) | ~600 | RulE æ¨¡å‹ | `RulE.export_embeddings()` |

---

## 2. æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 2.1 ä¸»è®­ç»ƒè„šæœ¬ (`main_rule_gnn.py`)

#### æ–‡ä»¶ç»“æ„

```python
main_rule_gnn.py
â”œâ”€â”€ parse_args()           # è§£æå‘½ä»¤è¡Œå‚æ•°
â””â”€â”€ main()                 # ä¸»å‡½æ•°
    â”œâ”€â”€ é˜¶æ®µ 1: åŠ è½½æ•°æ®
    â”œâ”€â”€ é˜¶æ®µ 2: RulE é¢„è®­ç»ƒï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ é˜¶æ®µ 3: å¯¼å‡ºåµŒå…¥
    â”œâ”€â”€ é˜¶æ®µ 4: Rule-GNN è®­ç»ƒ
    â””â”€â”€ é˜¶æ®µ 5: ä¿å­˜ç»“æœ
```

#### å…³é”®ä»£ç è§£æ

##### `parse_args()` - å‚æ•°è§£æ

```python
def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Rule-GNN Training')

    # æ ¸å¿ƒå‚æ•°
    parser.add_argument('--init', type=str, required=True,
                       help='Path to config file (JSON)')
    parser.add_argument('--skip_pretrain', action='store_true',
                       help='Skip RulE pretraining (load from checkpoint)')

    args = parser.parse_args()

    # åŠ è½½é…ç½®æ–‡ä»¶
    config = load_config(args.init)

    # åˆå¹¶é…ç½®ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜ï¼‰
    for key, value in vars(config).items():
        if key not in args_dict or args_dict[key] is None:
            args_dict[key] = value

    return args
```

**è®¾è®¡è¦ç‚¹**:
- `--init` å¿…éœ€å‚æ•°ï¼ŒæŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„
- `--skip_pretrain` å¯é€‰å‚æ•°ï¼Œè·³è¿‡ RulE é¢„è®­ç»ƒ
- é…ç½®æ–‡ä»¶ä¸å‘½ä»¤è¡Œå‚æ•°åˆå¹¶ç­–ç•¥ï¼šå‘½ä»¤è¡Œ > é…ç½®æ–‡ä»¶

##### é˜¶æ®µ 1: åŠ è½½æ•°æ® (lines 100-120)

```python
# åˆ›å»ºçŸ¥è¯†å›¾è°±
graph = KnowledgeGraph(args.data_path)
logger.info(f"Entities: {graph.entity_size}")
logger.info(f"Relations: {graph.relation_size}")

# åŠ è½½è§„åˆ™
ruleset = RuleDataset(graph.relation_size, args.rule_file, args.rule_negative_size)
rules = [rule[0] for rule in ruleset.rules]  # æå–è§„åˆ™ï¼ˆä¸å«è´Ÿæ ·æœ¬ï¼‰

# å…³è”è§„åˆ™åˆ°å›¾
graph.rules = ruleset.data
graph.relation2rules = ruleset.relation2rules

# åˆ›å»ºæ•°æ®é›†
train_set = TrainDataset(graph, args.g_batch_size)
valid_set = ValidDataset(graph, args.g_batch_size)
test_set = TestDataset(graph, args.g_batch_size)
```

**å…³é”®ç‚¹**:
- `ruleset.data`: åŒ…å«æ­£è´Ÿæ ·æœ¬çš„è§„åˆ™
- `graph.relation2rules`: å­—å…¸ï¼Œæ˜ å°„ `relation_id -> [rule1, rule2, ...]`
- `TrainDataset` æŒ‰å…³ç³»åˆ†ç»„ï¼Œæ–¹ä¾¿ grounding æ—¶æ‰¹é‡å¤„ç†

##### é˜¶æ®µ 2: RulE é¢„è®­ç»ƒ (lines 127-196)

```python
if not args.skip_pretrain:
    # åˆ›å»º RulE æ¨¡å‹
    rule_model = RulE(
        graph=graph,
        p_norm=args.p_norm,
        mlp_rule_dim=args.mlp_rule_dim,
        gamma_fact=args.gamma_fact,
        gamma_rule=args.gamma_rule,
        hidden_dim=args.hidden_dim,
        device=device,
        dataset=args.data_path
    )
    rule_model.set_rules(rules)

    # åˆ›å»ºé¢„è®­ç»ƒå™¨
    pre_trainer = PreTrainer(
        graph=graph,
        model=rule_model,
        valid_set=valid_set,
        test_set=test_set,
        ruleset=ruleset,
        expectation=True,
        device=device,
        num_worker=args.cpu_num
    )

    # æ‰§è¡Œé¢„è®­ç»ƒ
    pre_trainer.train(args)

    # åŠ è½½æœ€ä½³ checkpoint
    checkpoint = torch.load(rule_checkpoint_path)
    rule_model.load_state_dict(checkpoint['model'])
```

**è®­ç»ƒå†…å®¹**:
- Entity embeddings (RotatE)
- Relation embeddings (RotatE)
- Rule embeddings (è§„åˆ™è·ç¦»åº¦é‡)

##### é˜¶æ®µ 3: å¯¼å‡ºåµŒå…¥ (lines 205-208)

```python
embeddings_dict = rule_model.export_embeddings()
logger.info(f"Exported entity embeddings: {embeddings_dict['entity_embedding'].shape}")
logger.info(f"Exported relation embeddings: {embeddings_dict['relation_embedding'].shape}")
logger.info(f"Exported rule embeddings: {embeddings_dict['rule_emb'].shape}")
```

**åµŒå…¥å½¢çŠ¶**:
- `entity_embedding`: `[num_entities, hidden_dim * 2]` (å¤æ•°åµŒå…¥)
- `relation_embedding`: `[num_relations, hidden_dim]`
- `rule_emb`: `[num_rules, hidden_dim]`

##### é˜¶æ®µ 4: Rule-GNN è®­ç»ƒ (lines 217-250)

```python
# GNN å±‚æ•° = è§„åˆ™æœ€å¤§é•¿åº¦
num_layers = ruleset.max_body_len

# åˆ›å»º Rule-GNN æ¨¡å‹
rule_gnn_model = RuleGNN(
    num_entities=graph.entity_size,
    num_relations=graph.relation_size * 2,  # åŒ…æ‹¬é€†å…³ç³»
    num_rules=len(ruleset),
    hidden_dim=args.hidden_dim,
    num_layers=num_layers,
    dropout=args.dropout if hasattr(args, 'dropout') else 0.1
)

# åˆ›å»ºè®­ç»ƒå™¨
rule_gnn_trainer = RuleGNNTrainer(
    model=rule_gnn_model,
    graph=graph,
    train_dataset=train_set,
    valid_dataset=valid_set,
    test_dataset=test_set,
    device=device,
    logger=logger
)

# åŠ è½½é¢„è®­ç»ƒåµŒå…¥
rule_gnn_model.load_pretrained_embeddings(embeddings_dict)

# è®­ç»ƒ
test_metrics = rule_gnn_trainer.train(args)
```

**å…³é”®è®¾è®¡**:
- `num_relations * 2`: æ¯ä¸ªå…³ç³»æœ‰æ­£å‘å’Œé€†å‘
- `num_layers = max_body_len`: GNN å±‚æ•°å¯¹åº”è§„åˆ™é•¿åº¦

---

### 2.2 Rule-GNN æ¨¡å‹ (`rule_gnn_model.py`)

#### 2.2.1 `RuleAwareGraphConv` - è§„åˆ™æ„ŸçŸ¥å›¾å·ç§¯å±‚

##### ç±»å®šä¹‰

```python
class RuleAwareGraphConv(nn.Module):
    """
    è§„åˆ™æ„ŸçŸ¥çš„å›¾å·ç§¯å±‚

    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. æ³¨æ„åŠ›æƒé‡ç”±è§„åˆ™åµŒå…¥è°ƒæ§
    2. åªæœ‰ç¬¦åˆè§„åˆ™çš„è¾¹æ‰æœ‰é«˜æ³¨æ„åŠ›
    3. æ¶ˆæ¯èšåˆæ—¶è‡ªåŠ¨è¿‡æ»¤æ— å…³è¾¹
    """

    def __init__(self, hidden_dim, num_relations, dropout=0.1):
        super().__init__()
        self.hidden_dim = hidden_dim

        # å…³ç³»ç‰¹å®šçš„å˜æ¢çŸ©é˜µï¼ˆæ¯ä¸ªå…³ç³»ä¸€ä¸ªï¼‰
        self.W_r = nn.ModuleList([
            nn.Linear(hidden_dim, hidden_dim, bias=False)
            for _ in range(num_relations)
        ])

        # æ³¨æ„åŠ›æœºåˆ¶
        self.W_q = nn.Linear(hidden_dim, hidden_dim)  # Query
        self.W_k = nn.Linear(hidden_dim * 3, hidden_dim)  # Key (node + relation + rule)
        self.attn = nn.Linear(hidden_dim, 1)  # æ³¨æ„åŠ›æ‰“åˆ†

        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(hidden_dim)
```

**è®¾è®¡è¦ç‚¹**:
- `W_r`: æ¯ä¸ªå…³ç³»ç±»å‹æœ‰ç‹¬ç«‹çš„å˜æ¢çŸ©é˜µï¼ˆå‚æ•°åŒ–å…³ç³»ï¼‰
- `W_k`: Key ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šæºèŠ‚ç‚¹ + å…³ç³» + è§„åˆ™
- æ³¨æ„åŠ›æ‰“åˆ†è€ƒè™‘è§„åˆ™ä¿¡æ¯

##### å‰å‘ä¼ æ’­

```python
def forward(self, x, edge_index, edge_type, rule_ids, return_attention=False):
    """
    Args:
        x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, hidden_dim]
        edge_index: è¾¹ç´¢å¼• [2, num_edges] (src, dst)
        edge_type: è¾¹ç±»å‹ [num_edges]
        rule_ids: æ¿€æ´»çš„è§„åˆ™ ID [num_active_rules]
        return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡

    Returns:
        out: æ›´æ–°åçš„èŠ‚ç‚¹ç‰¹å¾ [num_nodes, hidden_dim]
        (optional) attn_weights: æ³¨æ„åŠ›æƒé‡
    """
    src, dst = edge_index[0], edge_index[1]
    num_nodes = x.size(0)
    num_edges = edge_index.size(1)

    # === æ­¥éª¤ 1: å…³ç³»ç‰¹å®šçš„æ¶ˆæ¯ç”Ÿæˆ ===
    messages = []
    for i in range(num_edges):
        r = edge_type[i].item()
        h_src = x[src[i]]  # æºèŠ‚ç‚¹ç‰¹å¾
        m = self.W_r[r](h_src)  # å…³ç³»ç‰¹å®šå˜æ¢
        messages.append(m)

    messages = torch.stack(messages)  # [num_edges, hidden_dim]

    # === æ­¥éª¤ 2: è§„åˆ™æ„ŸçŸ¥çš„æ³¨æ„åŠ› ===
    # è·å–è§„åˆ™åµŒå…¥ï¼ˆä»æ¨¡å‹å¤–éƒ¨ä¼ å…¥ï¼‰
    h_R = self.rule_embedding(rule_ids)  # [num_rules, hidden_dim]

    # å¯¹æ¯ä¸ªè¾¹ï¼Œè®¡ç®—ä¸æ‰€æœ‰æ¿€æ´»è§„åˆ™çš„æ³¨æ„åŠ›
    # Query: ç›®æ ‡èŠ‚ç‚¹
    query = self.W_q(x[dst])  # [num_edges, hidden_dim]

    # Key: æºèŠ‚ç‚¹ + å…³ç³» + è§„åˆ™
    # æ‰©å±•è§„åˆ™åµŒå…¥åˆ°æ¯æ¡è¾¹
    num_rules = rule_ids.size(0)
    query_expanded = query.unsqueeze(1).expand(-1, num_rules, -1)
    # [num_edges, num_rules, hidden_dim]

    # è·å–å…³ç³»åµŒå…¥
    relation_emb = self.relation_embedding(edge_type)  # [num_edges, hidden_dim]
    relation_emb_expanded = relation_emb.unsqueeze(1).expand(-1, num_rules, -1)

    # è§„åˆ™åµŒå…¥æ‰©å±•
    rule_emb_expanded = h_R.unsqueeze(0).expand(num_edges, -1, -1)

    # æ‹¼æ¥ Key
    src_expanded = x[src].unsqueeze(1).expand(-1, num_rules, -1)
    key = torch.cat([src_expanded, relation_emb_expanded, rule_emb_expanded], dim=-1)
    # [num_edges, num_rules, hidden_dim * 3]

    key = self.W_k(key)  # [num_edges, num_rules, hidden_dim]

    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    attn_input = query_expanded * key  # Element-wise product
    attn_scores = self.attn(attn_input).squeeze(-1)  # [num_edges, num_rules]

    # å¯¹æ¯æ¡è¾¹ï¼Œåœ¨æ‰€æœ‰è§„åˆ™ä¸Šåš softmax
    attn_weights = torch.softmax(attn_scores, dim=1)  # [num_edges, num_rules]

    # åŠ æƒèšåˆè§„åˆ™ç‰¹å¾
    rule_weighted = torch.matmul(attn_weights.unsqueeze(1), h_R.unsqueeze(0).expand(num_edges, -1, -1))
    # [num_edges, 1, hidden_dim]
    rule_weighted = rule_weighted.squeeze(1)  # [num_edges, hidden_dim]

    # === æ­¥éª¤ 3: ç»“åˆæ¶ˆæ¯å’Œè§„åˆ™ä¿¡æ¯ ===
    combined_messages = messages + rule_weighted  # [num_edges, hidden_dim]
    combined_messages = self.dropout(combined_messages)

    # === æ­¥éª¤ 4: æ¶ˆæ¯èšåˆï¼ˆscatter_addï¼‰ ===
    out = scatter_add(combined_messages, dst, dim=0, dim_size=num_nodes)
    # [num_nodes, hidden_dim]

    # === æ­¥éª¤ 5: æ®‹å·®è¿æ¥ + LayerNorm ===
    out = self.layer_norm(out + x)

    if return_attention:
        return out, attn_weights
    return out
```

**ç®—æ³•è§£æ**:

1. **æ¶ˆæ¯ç”Ÿæˆ**: æ¯æ¡è¾¹æ ¹æ®å…³ç³»ç±»å‹ä½¿ç”¨ç‰¹å®šçš„ `W_r` å˜æ¢
2. **è§„åˆ™æ³¨æ„åŠ›**:
   - Query: ç›®æ ‡èŠ‚ç‚¹
   - Key: (æºèŠ‚ç‚¹, å…³ç³», è§„åˆ™)
   - å¯¹æ¯æ¡è¾¹ï¼Œè®¡ç®—å®ƒä¸æ‰€æœ‰æ¿€æ´»è§„åˆ™çš„ç›¸å…³æ€§
3. **æ¶ˆæ¯å¢å¼º**: åŸå§‹æ¶ˆæ¯ + è§„åˆ™åŠ æƒç‰¹å¾
4. **æ¶ˆæ¯èšåˆ**: ä½¿ç”¨ `scatter_add` å°†é‚»å±…æ¶ˆæ¯èšåˆåˆ°ç›®æ ‡èŠ‚ç‚¹
5. **æ®‹å·® + å½’ä¸€åŒ–**: ä¿æŒæ¢¯åº¦æµåŠ¨ï¼Œç¨³å®šè®­ç»ƒ

#### 2.2.2 `RuleGNN` - å®Œæ•´æ¨¡å‹

##### æ¨¡å‹åˆå§‹åŒ–

```python
class RuleGNN(nn.Module):
    """
    å®Œæ•´çš„ Rule-GNN æ¨¡å‹

    ç”¨ GNN å¤šå±‚æ¶ˆæ¯ä¼ é€’æ›¿ä»£ RulE çš„è·¯å¾„æšä¸¾
    """

    def __init__(self, num_entities, num_relations, num_rules,
                 hidden_dim, num_layers, dropout=0.1):
        super().__init__()

        # åµŒå…¥å±‚ï¼ˆå°†è¢«é¢„è®­ç»ƒåµŒå…¥åˆå§‹åŒ–ï¼‰
        self.entity_embedding = nn.Embedding(num_entities, hidden_dim)
        self.relation_embedding = nn.Embedding(num_relations, hidden_dim)
        self.rule_embedding = nn.Embedding(num_rules, hidden_dim)

        # GNN å±‚ï¼ˆå±‚æ•° = è§„åˆ™æœ€å¤§é•¿åº¦ï¼‰
        self.conv_layers = nn.ModuleList([
            RuleAwareGraphConv(hidden_dim, num_relations, dropout)
            for _ in range(num_layers)
        ])

        # æœ€ç»ˆæ‰“åˆ† MLP
        self.score_mlp = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1)
        )

        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
```

**è®¾è®¡è¯´æ˜**:
- `num_layers`: ç”±è§„åˆ™æœ€å¤§é•¿åº¦å†³å®šï¼ˆä¾‹å¦‚ 3-hop è§„åˆ™éœ€è¦ 3 å±‚ï¼‰
- `entity_embedding`: å°†ç”¨ RulE é¢„è®­ç»ƒçš„å®ä½“åµŒå…¥åˆå§‹åŒ–
- `score_mlp`: è¾“å…¥æ˜¯ `[head_emb, tail_emb]` æ‹¼æ¥

##### åŠ è½½é¢„è®­ç»ƒåµŒå…¥

```python
def load_pretrained_embeddings(self, embeddings_dict):
    """
    ä»é¢„è®­ç»ƒçš„ RulE æ¨¡å‹åŠ è½½åµŒå…¥

    Args:
        embeddings_dict: {
            'entity_embedding': [num_entities, hidden_dim * 2],  # å¤æ•°åµŒå…¥
            'relation_embedding': [num_relations, hidden_dim],
            'rule_emb': [num_rules, hidden_dim]
        }
    """
    # å®ä½“åµŒå…¥æ˜¯å¤æ•°ï¼ˆreal + imagï¼‰ï¼Œå–å®éƒ¨æˆ–å¹³å‡
    entity_emb = embeddings_dict['entity_embedding']
    if entity_emb.size(1) == self.hidden_dim * 2:
        # å–å‰åŠéƒ¨åˆ†ï¼ˆå®éƒ¨ï¼‰
        entity_emb = entity_emb[:, :self.hidden_dim]

    self.entity_embedding.weight.data.copy_(entity_emb)
    self.relation_embedding.weight.data.copy_(embeddings_dict['relation_embedding'])
    self.rule_embedding.weight.data.copy_(embeddings_dict['rule_emb'])

    logger.info("Loaded pretrained embeddings from RulE")
```

**å…³é”®ç‚¹**:
- RulE çš„å®ä½“åµŒå…¥æ˜¯å¤æ•°ï¼ˆç»´åº¦ `hidden_dim * 2`ï¼‰
- Rule-GNN ä½¿ç”¨å®æ•°åµŒå…¥ï¼Œæ‰€ä»¥åªå–å‰åŠéƒ¨åˆ†ï¼ˆå®éƒ¨ï¼‰
- å…³ç³»å’Œè§„åˆ™åµŒå…¥ç›´æ¥å¤åˆ¶

##### å‰å‘ä¼ æ’­

```python
def forward(self, queries, edge_index, edge_type, rule_ids, candidates=None):
    """
    å‰å‘ä¼ æ’­

    Args:
        queries: [batch_size, 2] (head, relation)
        edge_index: [2, num_edges] KG è¾¹
        edge_type: [num_edges] è¾¹ç±»å‹
        rule_ids: [num_active_rules] æ¿€æ´»çš„è§„åˆ™
        candidates: [batch_size, num_candidates] å€™é€‰å°¾å®ä½“
                    å¦‚æœä¸º Noneï¼Œå¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†

    Returns:
        scores: [batch_size, num_candidates] æˆ– [batch_size, num_entities]
    """
    batch_size = queries.size(0)
    num_entities = self.entity_embedding.num_embeddings

    # === æ­¥éª¤ 1: åˆå§‹åŒ–èŠ‚ç‚¹ç‰¹å¾ ===
    h = self.entity_embedding.weight  # [num_entities, hidden_dim]

    # === æ­¥éª¤ 2: å¤šå±‚ GNN ä¼ æ’­ ===
    for conv in self.conv_layers:
        h = conv(h, edge_index, edge_type, rule_ids)
        # h: [num_entities, hidden_dim]

    # === æ­¥éª¤ 3: è·å–æŸ¥è¯¢çš„ head åµŒå…¥ ===
    head_ids = queries[:, 0]  # [batch_size]
    head_emb = h[head_ids]  # [batch_size, hidden_dim]

    # === æ­¥éª¤ 4: æ‰“åˆ† ===
    if candidates is None:
        # å¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†
        tail_emb = h  # [num_entities, hidden_dim]

        # æ‰©å±• head_emb
        head_emb_expanded = head_emb.unsqueeze(1).expand(-1, num_entities, -1)
        # [batch_size, num_entities, hidden_dim]

        tail_emb_expanded = tail_emb.unsqueeze(0).expand(batch_size, -1, -1)
        # [batch_size, num_entities, hidden_dim]

        # æ‹¼æ¥å¹¶æ‰“åˆ†
        pair_emb = torch.cat([head_emb_expanded, tail_emb_expanded], dim=-1)
        # [batch_size, num_entities, hidden_dim * 2]

        scores = self.score_mlp(pair_emb).squeeze(-1)
        # [batch_size, num_entities]

    else:
        # åªå¯¹å€™é€‰å®ä½“æ‰“åˆ†
        num_candidates = candidates.size(1)

        # è·å–å€™é€‰å®ä½“åµŒå…¥
        tail_emb = h[candidates]  # [batch_size, num_candidates, hidden_dim]

        # æ‰©å±• head_emb
        head_emb_expanded = head_emb.unsqueeze(1).expand(-1, num_candidates, -1)

        # æ‹¼æ¥å¹¶æ‰“åˆ†
        pair_emb = torch.cat([head_emb_expanded, tail_emb], dim=-1)
        scores = self.score_mlp(pair_emb).squeeze(-1)
        # [batch_size, num_candidates]

    return scores
```

**è®¡ç®—æµç¨‹**:
1. åˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾ä¸ºé¢„è®­ç»ƒçš„å®ä½“åµŒå…¥
2. å¤šå±‚ GNN ä¼ æ’­ï¼Œæ¯å±‚è€ƒè™‘è§„åˆ™ä¿¡æ¯
3. æå–æŸ¥è¯¢ head çš„æœ€ç»ˆåµŒå…¥
4. å¯¹å€™é€‰ tail æ‰“åˆ†ï¼š`MLP([head_emb, tail_emb])`

**ä¸ RulE Grounding çš„å¯¹æ¯”**:

| ç‰¹æ€§ | RulE Grounding | Rule-GNN |
|-----|---------------|----------|
| è·¯å¾„ä¿¡æ¯ | æ˜¾å¼æšä¸¾ï¼ˆBFSï¼‰ | éšå¼èšåˆï¼ˆGNNï¼‰ |
| è§„åˆ™è¡¨ç¤º | grounding_count (æ ‡é‡) | èŠ‚ç‚¹åµŒå…¥ (å‘é‡) |
| å¤æ‚åº¦ | O(B^L) | O(E Ã— L) |
| è®¡ç®—æ–¹å¼ | `count @ rule_feature` | `MLP([h, t])` |

---

### 2.3 è®­ç»ƒå™¨ (`rule_gnn_trainer.py`)

#### 2.3.1 åˆå§‹åŒ–

```python
class RuleGNNTrainer:
    def __init__(self, model, graph, train_dataset, valid_dataset, test_dataset,
                 device='cuda', logger=None):
        self.model = model.to(device)
        self.graph = graph
        self.train_dataset = train_dataset
        self.valid_dataset = valid_dataset
        self.test_dataset = test_dataset
        self.device = device
        self.logger = logger

        # æ„å»º PyG æ ¼å¼çš„å›¾
        self.edge_index, self.edge_type = self._build_pyg_graph()
        self.edge_index = self.edge_index.to(device)
        self.edge_type = self.edge_type.to(device)
```

#### 2.3.2 æ„å»º PyG å›¾

```python
def _build_pyg_graph(self):
    """
    å°† KnowledgeGraph è½¬æ¢ä¸º PyTorch Geometric æ ¼å¼

    Returns:
        edge_index: [2, num_edges] è¾¹ç´¢å¼• (src, dst)
        edge_type: [num_edges] è¾¹ç±»å‹
    """
    all_edges = []
    all_types = []

    # æ”¶é›†æ‰€æœ‰è¾¹ï¼ˆåŒ…æ‹¬è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼‰
    for split in ['train', 'valid', 'test']:
        data = getattr(self.graph, f'{split}_data')
        for h, r, t in data:
            # æ­£å‘è¾¹
            all_edges.append([h, t])
            all_types.append(r)

            # é€†å‘è¾¹
            all_edges.append([t, h])
            all_types.append(r + self.graph.relation_size)

    edge_index = torch.tensor(all_edges, dtype=torch.long).t()
    # [[src1, src2, ...],
    #  [dst1, dst2, ...]]

    edge_type = torch.tensor(all_types, dtype=torch.long)

    return edge_index, edge_type
```

**è®¾è®¡è¦ç‚¹**:
- åŒ…å«æ‰€æœ‰æ•°æ®é›†ï¼ˆtrain/valid/testï¼‰çš„è¾¹
- æ¯æ¡è¾¹æœ‰æ­£å‘å’Œé€†å‘ä¸¤ä¸ªç‰ˆæœ¬
- é€†å‘è¾¹çš„ç±»å‹ ID = åŸç±»å‹ ID + `relation_size`

#### 2.3.3 è·å–æ¿€æ´»è§„åˆ™

```python
def get_active_rules(self, query_relations):
    """
    è·å–æŸ¥è¯¢å…³ç³»å¯¹åº”çš„æ¿€æ´»è§„åˆ™

    Args:
        query_relations: [batch_size] æŸ¥è¯¢å…³ç³» ID

    Returns:
        rule_ids: [num_active_rules] æ¿€æ´»çš„è§„åˆ™ IDï¼ˆå»é‡ï¼‰
    """
    active_rules = set()

    for r in query_relations:
        r_item = r.item() if torch.is_tensor(r) else r

        # ä» graph.relation2rules æŸ¥æ‰¾è§„åˆ™
        if r_item in self.graph.relation2rules:
            for rule in self.graph.relation2rules[r_item]:
                rule_id = rule[0]  # rule = [rule_id, head, body...]
                active_rules.add(rule_id)

    return torch.tensor(list(active_rules), dtype=torch.long, device=self.device)
```

**ä½œç”¨**:
- ç»™å®šæŸ¥è¯¢å…³ç³»ï¼Œæ‰¾å‡ºæ‰€æœ‰ head = è¯¥å…³ç³»çš„è§„åˆ™
- å»é‡ï¼ˆå¤šä¸ªæŸ¥è¯¢å¯èƒ½å…±äº«è§„åˆ™ï¼‰

#### 2.3.4 è®­ç»ƒä¸€ä¸ª Epoch

```python
def train_epoch(self, optimizer, args):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    self.model.train()

    train_loader = DataLoader(
        self.train_dataset,
        batch_size=args.g_batch_size,
        shuffle=True,
        num_workers=4,
        collate_fn=self.train_dataset.collate_fn
    )

    total_loss = 0.0
    num_batches = 0

    for batch_idx, batch in enumerate(tqdm(train_loader, desc="Training")):
        if batch_idx >= args.batch_per_epoch:
            break

        # è§£åŒ…æ‰¹æ¬¡
        pos_samples, neg_samples, edges_to_remove = batch
        pos_samples = pos_samples.to(self.device)  # [batch_size, 3] (h, r, t)
        neg_samples = neg_samples.to(self.device)  # [batch_size, neg_size]

        # è·å–æŸ¥è¯¢
        queries = pos_samples[:, :2]  # [batch_size, 2] (h, r)
        batch_size = queries.size(0)

        # è·å–æ¿€æ´»è§„åˆ™
        rule_ids = self.get_active_rules(queries[:, 1])

        if len(rule_ids) == 0:
            continue  # æ²¡æœ‰è§„åˆ™ï¼Œè·³è¿‡

        # æ­£æ ·æœ¬æ‰“åˆ†
        pos_tail = pos_samples[:, 2:3]  # [batch_size, 1]
        pos_scores = self.model(queries, self.edge_index, self.edge_type,
                               rule_ids, candidates=pos_tail)
        # [batch_size, 1]

        # è´Ÿæ ·æœ¬æ‰“åˆ†
        neg_scores = self.model(queries, self.edge_index, self.edge_type,
                               rule_ids, candidates=neg_samples)
        # [batch_size, neg_size]

        # æ‹¼æ¥åˆ†æ•°
        all_scores = torch.cat([pos_scores, neg_scores], dim=1)
        # [batch_size, 1 + neg_size]

        # æ ‡ç­¾ï¼šç¬¬ä¸€ä¸ªæ˜¯æ­£æ ·æœ¬
        labels = torch.zeros(batch_size, dtype=torch.long, device=self.device)

        # äº¤å‰ç†µæŸå¤±
        loss_ce = nn.CrossEntropyLoss()(all_scores, labels)

        # æ ‡ç­¾å¹³æ»‘
        if args.smoothing > 0:
            num_classes = all_scores.size(1)
            smooth_labels = torch.full_like(all_scores, args.smoothing / num_classes)
            smooth_labels[:, 0] = 1.0 - args.smoothing + args.smoothing / num_classes

            loss_smooth = -(smooth_labels * torch.log_softmax(all_scores, dim=1)).sum(dim=1).mean()
            loss = loss_smooth
        else:
            loss = loss_ce

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

    return total_loss / max(num_batches, 1)
```

**è®­ç»ƒç­–ç•¥**:
1. **æ­£è´Ÿæ ·æœ¬å¯¹æ¯”å­¦ä¹ **: æ­£æ ·æœ¬å¾—åˆ†åº”é«˜äºè´Ÿæ ·æœ¬
2. **äº¤å‰ç†µæŸå¤±**: å°†é—®é¢˜è§†ä¸ºå¤šåˆ†ç±»ï¼ˆç¬¬ä¸€ä¸ªæ˜¯æ­£ç±»ï¼‰
3. **æ ‡ç­¾å¹³æ»‘**: é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¢å¼ºæ³›åŒ–

**æ ‡ç­¾å¹³æ»‘å…¬å¼**:
```
y_smooth[i] = (1 - Î±) * y_true[i] + Î± / K

å…¶ä¸­:
- y_true[0] = 1 (æ­£æ ·æœ¬), y_true[1:] = 0 (è´Ÿæ ·æœ¬)
- Î± = smoothing (ä¾‹å¦‚ 0.2)
- K = num_classes (1 + neg_size)

ç¤ºä¾‹: smoothing=0.2, neg_size=128
y_smooth[0] = 0.8 + 0.2/129 â‰ˆ 0.801
y_smooth[1:] = 0 + 0.2/129 â‰ˆ 0.0016
```

#### 2.3.5 è¯„ä¼°

```python
def evaluate(self, dataset, split='valid'):
    """è¯„ä¼°æ¨¡å‹"""
    self.model.eval()

    eval_loader = DataLoader(
        dataset,
        batch_size=16,
        shuffle=False,
        num_workers=4,
        collate_fn=dataset.collate_fn
    )

    ranks = []
    reciprocal_ranks = []

    with torch.no_grad():
        for batch in tqdm(eval_loader, desc=f"Evaluating {split}"):
            pos_samples, filter_mask = batch
            pos_samples = pos_samples.to(self.device)  # [batch_size, 3]
            filter_mask = filter_mask.to(self.device)  # [batch_size, num_entities]

            # æŸ¥è¯¢
            queries = pos_samples[:, :2]  # [batch_size, 2]
            true_tails = pos_samples[:, 2]  # [batch_size]

            # è·å–æ¿€æ´»è§„åˆ™
            rule_ids = self.get_active_rules(queries[:, 1])

            if len(rule_ids) == 0:
                # æ²¡æœ‰è§„åˆ™ï¼Œä½¿ç”¨æ‰€æœ‰è§„åˆ™
                rule_ids = torch.arange(len(self.graph.rules), dtype=torch.long, device=self.device)

            # å¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†
            scores = self.model(queries, self.edge_index, self.edge_type,
                               rule_ids, candidates=None)
            # [batch_size, num_entities]

            # è¿‡æ»¤å·²çŸ¥ä¸‰å…ƒç»„ï¼ˆfiltered settingï¼‰
            scores = scores.masked_fill(filter_mask.bool(), -1e9)

            # è®¡ç®—æ’å
            batch_size = scores.size(0)
            for i in range(batch_size):
                true_tail = true_tails[i].item()
                true_score = scores[i, true_tail].item()

                # æ’å = æ¯”çœŸå®å°¾å®ä½“å¾—åˆ†é«˜çš„å®ä½“æ•° + 1
                rank = (scores[i] > true_score).sum().item() + 1

                ranks.append(rank)
                reciprocal_ranks.append(1.0 / rank)

    # è®¡ç®—æŒ‡æ ‡
    metrics = {
        'MRR': np.mean(reciprocal_ranks),
        'MR': np.mean(ranks),
        'HITS@1': np.mean(np.array(ranks) <= 1),
        'HITS@3': np.mean(np.array(ranks) <= 3),
        'HITS@10': np.mean(np.array(ranks) <= 10)
    }

    return metrics
```

**è¯„ä¼°æŒ‡æ ‡**:
- **MRR** (Mean Reciprocal Rank): å¹³å‡å€’æ•°æ’å
  - å…¬å¼: `MRR = 1/N Ã— Î£(1/rank_i)`
  - è¶Šé«˜è¶Šå¥½ï¼ˆæœ€é«˜ 1.0ï¼‰

- **MR** (Mean Rank): å¹³å‡æ’å
  - å…¬å¼: `MR = 1/N Ã— Î£(rank_i)`
  - è¶Šä½è¶Šå¥½

- **Hits@K**: Top-K å‘½ä¸­ç‡
  - å…¬å¼: `Hits@K = (æ’å â‰¤ K çš„æ ·æœ¬æ•°) / æ€»æ ·æœ¬æ•°`
  - è¶Šé«˜è¶Šå¥½

**Filtered Setting**:
- æ’åæ—¶ï¼Œæ’é™¤æ‰€æœ‰å·²çŸ¥çš„çœŸå®ä¸‰å…ƒç»„ï¼ˆè®­ç»ƒ+éªŒè¯+æµ‹è¯•ï¼‰
- é¿å…æƒ©ç½šé¢„æµ‹æ­£ç¡®ä½†ä¸åœ¨æµ‹è¯•é›†ä¸­çš„ä¸‰å…ƒç»„

---

### 2.4 å·¥å…·å±‚ (`rule_gnn_layers.py`)

#### 2.4.1 `scatter_softmax` - Scatter Softmax

```python
def scatter_softmax(src, index, dim=0, dim_size=None):
    """
    å¯¹ scatter çš„å…ƒç´ åš softmax

    ç”¨äºè¾¹çº§æ³¨æ„åŠ›å½’ä¸€åŒ–

    Args:
        src: [num_edges] æœªå½’ä¸€åŒ–çš„åˆ†æ•°
        index: [num_edges] ç›®æ ‡èŠ‚ç‚¹ç´¢å¼•
        dim: scatter ç»´åº¦
        dim_size: ç›®æ ‡ç»´åº¦å¤§å°ï¼ˆèŠ‚ç‚¹æ•°ï¼‰

    Returns:
        softmax_src: [num_edges] å½’ä¸€åŒ–åçš„åˆ†æ•°

    ç¤ºä¾‹:
        src = [2.0, 3.0, 1.0, 4.0]
        index = [0, 0, 1, 1]

        ç»“æœ:
        - èŠ‚ç‚¹ 0: softmax([2.0, 3.0]) = [0.27, 0.73]
        - èŠ‚ç‚¹ 1: softmax([1.0, 4.0]) = [0.05, 0.95]

        output = [0.27, 0.73, 0.05, 0.95]
    """
    if dim_size is None:
        dim_size = int(index.max()) + 1

    # å¯¹æ¯ä¸ªç»„æ‰¾æœ€å¤§å€¼ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
    max_value_per_index = scatter_max(src, index, dim=dim, dim_size=dim_size)[0]
    # [dim_size]

    # æ‰©å±•å›åŸå§‹å½¢çŠ¶
    max_value = max_value_per_index[index]  # [num_edges]

    # æŒ‡æ•°ï¼ˆå‡å»æœ€å¤§å€¼ï¼‰
    exp_src = torch.exp(src - max_value)

    # å¯¹æ¯ä¸ªç»„æ±‚å’Œ
    sum_per_index = scatter_add(exp_src, index, dim=dim, dim_size=dim_size)
    sum_value = sum_per_index[index]

    # å½’ä¸€åŒ–
    return exp_src / (sum_value + 1e-16)
```

**ä½œç”¨**: åœ¨å›¾ç»“æ„ä¸Šåš softmaxï¼ˆæ¯ä¸ªèŠ‚ç‚¹çš„å…¥è¾¹ç‹¬ç«‹å½’ä¸€åŒ–ï¼‰

#### 2.4.2 `AttentionAggregation` - æ³¨æ„åŠ›èšåˆå±‚

```python
class AttentionAggregation(nn.Module):
    """æ³¨æ„åŠ›èšåˆå±‚ - ç”¨äºèšåˆå¤šä¸ªè§„åˆ™çš„ä¿¡æ¯"""

    def __init__(self, hidden_dim):
        super().__init__()
        self.W_q = nn.Linear(hidden_dim, hidden_dim)
        self.W_k = nn.Linear(hidden_dim, hidden_dim)
        self.W_v = nn.Linear(hidden_dim, hidden_dim)
        self.scale = hidden_dim ** 0.5

    def forward(self, query, keys, values):
        """
        Args:
            query: [batch_size, hidden_dim] æŸ¥è¯¢å‘é‡
            keys: [batch_size, num_rules, hidden_dim] é”®å‘é‡
            values: [batch_size, num_rules, hidden_dim] å€¼å‘é‡

        Returns:
            out: [batch_size, hidden_dim] èšåˆç»“æœ
        """
        Q = self.W_q(query).unsqueeze(1)  # [batch_size, 1, hidden_dim]
        K = self.W_k(keys)  # [batch_size, num_rules, hidden_dim]
        V = self.W_v(values)  # [batch_size, num_rules, hidden_dim]

        # æ³¨æ„åŠ›åˆ†æ•°
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        # [batch_size, 1, num_rules]

        attn_weights = torch.softmax(attn_scores, dim=-1)

        # åŠ æƒæ±‚å’Œ
        out = torch.matmul(attn_weights, V).squeeze(1)
        # [batch_size, hidden_dim]

        return out
```

**ç”¨é€”**: èšåˆå¤šä¸ªè§„åˆ™çš„ä¿¡æ¯ï¼ˆå½“å‰å®ç°ä¸­æœªç›´æ¥ä½¿ç”¨ï¼Œä¿ç•™ä¾›æ‰©å±•ï¼‰

---

## 3. è®­ç»ƒæµç¨‹å‰–æ

### 3.1 æ•°æ®æµå›¾

```
è®­ç»ƒå¼€å§‹
    â”‚
    â”œâ”€> åŠ è½½çŸ¥è¯†å›¾è°±
    â”‚   â”œâ”€> entities.dict, relations.dict
    â”‚   â””â”€> train.txt, valid.txt, test.txt
    â”‚
    â”œâ”€> åŠ è½½è§„åˆ™
    â”‚   â””â”€> mined_rules.txt
    â”‚
    â”œâ”€> RulE é¢„è®­ç»ƒï¼ˆå¯é€‰ï¼‰
    â”‚   â”œâ”€> è¾“å…¥: KG ä¸‰å…ƒç»„ + è§„åˆ™
    â”‚   â”œâ”€> è¾“å‡º: entity_emb, relation_emb, rule_emb
    â”‚   â””â”€> ä¿å­˜: rule_checkpoint
    â”‚
    â”œâ”€> å¯¼å‡ºåµŒå…¥
    â”‚   â””â”€> embeddings_dict
    â”‚
    â”œâ”€> æ„å»º PyG å›¾
    â”‚   â”œâ”€> edge_index: [2, num_edges]
    â”‚   â””â”€> edge_type: [num_edges]
    â”‚
    â””â”€> Rule-GNN è®­ç»ƒ
        â”œâ”€> åŠ è½½é¢„è®­ç»ƒåµŒå…¥
        â”œâ”€> è®­ç»ƒå¾ªç¯ï¼ˆ50 epochsï¼‰
        â”‚   â”œâ”€> æ¯ä¸ª batch:
        â”‚   â”‚   â”œâ”€> æ­£æ ·æœ¬: (h, r, t)
        â”‚   â”‚   â”œâ”€> è´Ÿæ ·æœ¬: (h, r, t')
        â”‚   â”‚   â”œâ”€> æ¿€æ´»è§„åˆ™: rule_ids
        â”‚   â”‚   â”œâ”€> GNN ä¼ æ’­ (3 å±‚)
        â”‚   â”‚   â”œâ”€> æ‰“åˆ†: MLP([h_emb, t_emb])
        â”‚   â”‚   â””â”€> æŸå¤±: CrossEntropy + LabelSmoothing
        â”‚   â”‚
        â”‚   â””â”€> éªŒè¯ (æ¯ 5 epochs)
        â”‚       â”œâ”€> å¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†
        â”‚       â”œâ”€> è¿‡æ»¤å·²çŸ¥ä¸‰å…ƒç»„
        â”‚       â””â”€> è®¡ç®— MRR, Hits@K
        â”‚
        â””â”€> æµ‹è¯•é›†è¯„ä¼°
```

### 3.2 ä¸€ä¸ªè®­ç»ƒ Step çš„è¯¦ç»†æµç¨‹

```python
# === æ­¥éª¤ 1: é‡‡æ ·æ‰¹æ¬¡ ===
batch = next(train_loader)
pos_samples, neg_samples, edges_to_remove = batch
# pos_samples: [[10, 5, 23], [12, 3, 45], ...] (h, r, t)
# neg_samples: [[12, 34, 56, ...], [...]] (è´Ÿæ ·æœ¬å°¾å®ä½“)

# === æ­¥éª¤ 2: æå–æŸ¥è¯¢ ===
queries = pos_samples[:, :2]  # [[10, 5], [12, 3], ...]

# === æ­¥éª¤ 3: è·å–æ¿€æ´»è§„åˆ™ ===
rule_ids = get_active_rules([5, 3])
# ä¾‹å¦‚: tensor([0, 1, 2, 10, 15])  (5 ä¸ªè§„åˆ™)

# === æ­¥éª¤ 4: GNN å‰å‘ä¼ æ’­ ===
# åˆå§‹åŒ–: h = entity_embedding (æ‰€æœ‰å®ä½“)
h = entity_embedding.weight  # [135, 2000]

# ç¬¬ 1 å±‚ GNN
h = conv_layers[0](h, edge_index, edge_type, rule_ids)
# æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹çš„è¡¨ç¤º

# ç¬¬ 2 å±‚ GNN
h = conv_layers[1](h, edge_index, edge_type, rule_ids)

# ç¬¬ 3 å±‚ GNN
h = conv_layers[2](h, edge_index, edge_type, rule_ids)

# === æ­¥éª¤ 5: æå– head åµŒå…¥ ===
head_emb = h[queries[:, 0]]  # [batch_size, 2000]

# === æ­¥éª¤ 6: æ­£æ ·æœ¬æ‰“åˆ† ===
pos_tail_emb = h[pos_samples[:, 2]]  # [batch_size, 2000]
pos_scores = score_mlp(torch.cat([head_emb, pos_tail_emb], dim=-1))
# [batch_size, 1]

# === æ­¥éª¤ 7: è´Ÿæ ·æœ¬æ‰“åˆ† ===
neg_tail_emb = h[neg_samples]  # [batch_size, neg_size, 2000]
head_emb_expanded = head_emb.unsqueeze(1).expand(-1, neg_size, -1)
neg_scores = score_mlp(torch.cat([head_emb_expanded, neg_tail_emb], dim=-1))
# [batch_size, neg_size]

# === æ­¥éª¤ 8: è®¡ç®—æŸå¤± ===
all_scores = torch.cat([pos_scores, neg_scores], dim=1)
# [batch_size, 1 + neg_size]

labels = torch.zeros(batch_size, dtype=torch.long)
loss = CrossEntropyLoss()(all_scores, labels)

# === æ­¥éª¤ 9: åå‘ä¼ æ’­ ===
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

---

## 4. å…³é”®ç®—æ³•å®ç°

### 4.1 è§„åˆ™æ„ŸçŸ¥çš„æ¶ˆæ¯ä¼ é€’

**ä¼ªä»£ç **:
```
function RuleAwareMessagePassing(x, edge_index, edge_type, rule_ids):
    for each edge (u, v, r) in edge_index:
        # 1. å…³ç³»ç‰¹å®šçš„æ¶ˆæ¯
        m_uv = W_r[r] @ x[u]

        # 2. è§„åˆ™æ³¨æ„åŠ›
        for each rule R in rule_ids:
            attn_score_R = Attention(x[v], x[u], relation[r], rule[R])

        attn_weights = Softmax(attn_scores)

        # 3. è§„åˆ™åŠ æƒ
        rule_feature = Î£_R (attn_weights[R] * rule[R])

        # 4. ç»„åˆæ¶ˆæ¯
        m_uv = m_uv + rule_feature

    # 5. èšåˆ
    for each node v:
        x'[v] = LayerNorm(x[v] + Î£_{uâˆˆN(v)} m_uv)

    return x'
```

### 4.2 Filtered Ranking

**ä¼ªä»£ç **:
```
function FilteredRanking(query, true_tail, scores):
    # è·å–æ‰€æœ‰å·²çŸ¥ä¸‰å…ƒç»„ (h, r, ?)
    known_tails = hr2ooo[query]  # train + valid + test

    # å°†å·²çŸ¥å°¾å®ä½“çš„åˆ†æ•°è®¾ä¸º -âˆ
    for t in known_tails:
        if t != true_tail:
            scores[t] = -1e9

    # è®¡ç®—æ’å
    rank = (scores > scores[true_tail]).sum() + 1

    return rank
```

**ä¸ºä»€ä¹ˆéœ€è¦ Filtered Setting?**

è€ƒè™‘æŸ¥è¯¢ `(Einstein, birthPlace, ?)`ï¼š
- çœŸå®ç­”æ¡ˆï¼ˆæµ‹è¯•é›†ï¼‰: `Germany`
- é¢„æµ‹ Top-1: `Ulm`ï¼ˆè®­ç»ƒé›†ä¸­çš„çœŸå®ç­”æ¡ˆï¼‰

å¦‚æœä¸è¿‡æ»¤ï¼Œæ¨¡å‹ä¼šè¢«æƒ©ç½šï¼ˆè™½ç„¶é¢„æµ‹æ­£ç¡®ï¼‰ã€‚Filtered setting è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚

### 4.3 æ ‡ç­¾å¹³æ»‘

**å…¬å¼**:
```
Loss_smooth = - Î£_i y_smooth[i] * log(p[i])

å…¶ä¸­:
y_smooth[i] = {
    1 - Î± + Î±/K,  if i = 0 (æ­£æ ·æœ¬)
    Î±/K,          otherwise (è´Ÿæ ·æœ¬)
}

Î± = smoothing (ä¾‹å¦‚ 0.2)
K = num_classes (1 + neg_size)
```

**æ•ˆæœå¯¹æ¯”**:

| è®¾ç½® | æ­£æ ·æœ¬æ¦‚ç‡ç›®æ ‡ | è´Ÿæ ·æœ¬æ¦‚ç‡ç›®æ ‡ | æ³›åŒ–èƒ½åŠ› |
|-----|---------------|---------------|----------|
| æ— å¹³æ»‘ (Î±=0) | 1.0 | 0.0 | å·®ï¼ˆè¿‡æ‹Ÿåˆï¼‰ |
| è½»åº¦å¹³æ»‘ (Î±=0.1) | 0.9 | 0.0008 | è¾ƒå¥½ |
| ä¸­åº¦å¹³æ»‘ (Î±=0.2) | 0.8 | 0.0016 | å¥½ |
| é‡åº¦å¹³æ»‘ (Î±=0.5) | 0.5 | 0.004 | æ¬ æ‹Ÿåˆ |

---

## 5. æ•°æ®æµåˆ†æ

### 5.1 Tensor å½¢çŠ¶è¿½è¸ª

ä»¥ UMLS æ•°æ®é›†ä¸ºä¾‹ï¼ˆ135 å®ä½“ï¼Œ46 å…³ç³»ï¼Œ587 è§„åˆ™ï¼‰ï¼š

```python
# === è¾“å…¥ ===
batch_size = 16
neg_size = 128
num_entities = 135
num_relations = 46 * 2  # 92 (åŒ…å«é€†å…³ç³»)
num_rules = 587
hidden_dim = 2000
num_layers = 3

# === è®­ç»ƒæ‰¹æ¬¡ ===
pos_samples: [16, 3]  # (h, r, t)
neg_samples: [16, 128]  # è´Ÿæ ·æœ¬å°¾å®ä½“
queries: [16, 2]  # (h, r)

# === å›¾ç»“æ„ ===
edge_index: [2, 13420]  # (train + valid + test) Ã— 2 (æ­£å‘+é€†å‘)
edge_type: [13420]

# === æ¿€æ´»è§„åˆ™ ===
rule_ids: [25]  # å‡è®¾ 16 ä¸ªæŸ¥è¯¢å…±æ¿€æ´» 25 ä¸ªè§„åˆ™

# === GNN ä¼ æ’­ ===
# Layer 0 è¾“å…¥
h_0: [135, 2000]  # æ‰€æœ‰å®ä½“çš„åˆå§‹åµŒå…¥

# Layer 0 ä¸­é—´
src, dst = edge_index  # [13420], [13420]
messages: [13420, 2000]  # æ¯æ¡è¾¹ä¸€ä¸ªæ¶ˆæ¯

# æ³¨æ„åŠ›è®¡ç®—
query: [13420, 2000]  # ç›®æ ‡èŠ‚ç‚¹ç‰¹å¾
key: [13420, 25, 2000]  # (ç›®æ ‡, è§„åˆ™æ•°, éšè—ç»´åº¦)
attn_scores: [13420, 25]
attn_weights: [13420, 25]  # softmaxå

# è§„åˆ™åŠ æƒ
rule_weighted: [13420, 2000]

# æ¶ˆæ¯èšåˆ
combined_messages: [13420, 2000]
h_1 = scatter_add(combined_messages, dst): [135, 2000]

# Layer 1, Layer 2 ç±»ä¼¼...
h_final: [135, 2000]

# === æ‰“åˆ† ===
head_emb: [16, 2000]
pos_tail_emb: [16, 2000]
neg_tail_emb: [16, 128, 2000]

# æ­£æ ·æœ¬
pair_pos: [16, 4000]  # cat([head, tail])
pos_scores: [16, 1]

# è´Ÿæ ·æœ¬
pair_neg: [16, 128, 4000]
neg_scores: [16, 128]

# === æŸå¤± ===
all_scores: [16, 129]  # 1 æ­£ + 128 è´Ÿ
labels: [16]  # å…¨ä¸º 0ï¼ˆç¬¬ä¸€ä¸ªæ˜¯æ­£ç±»ï¼‰
loss: scalar
```

### 5.2 å†…å­˜å ç”¨ä¼°ç®—

**æ¨¡å‹å‚æ•°**:
```python
# åµŒå…¥å±‚
entity_embedding: 135 Ã— 2000 Ã— 4 bytes = 1.08 MB
relation_embedding: 92 Ã— 2000 Ã— 4 bytes = 0.74 MB
rule_embedding: 587 Ã— 2000 Ã— 4 bytes = 4.70 MB

# GNN å±‚ï¼ˆæ¯å±‚ï¼‰
W_r (92 ä¸ª): 92 Ã— (2000 Ã— 2000) Ã— 4 bytes = 1472 MB
W_q, W_k, attn: ~3 Ã— (2000 Ã— 2000) Ã— 4 bytes = 48 MB

# 3 å±‚ GNN
GNN total: 3 Ã— (1472 + 48) = 4560 MB

# MLP
score_mlp: (4000 Ã— 2000 + 2000 Ã— 1) Ã— 4 bytes = 32 MB

# æ€»è®¡
Total params: ~4.6 GB
```

**æ¿€æ´»å€¼**ï¼ˆbatch_size=16ï¼‰:
```python
# GNN ä¸­é—´ç»“æœ
h (æ¯å±‚): 135 Ã— 2000 Ã— 4 bytes = 1.08 MB
messages: 13420 Ã— 2000 Ã— 4 bytes = 107 MB
attn_weights: 13420 Ã— 25 Ã— 4 bytes = 1.34 MB

# æ¯å±‚æ¿€æ´»: ~110 MB
# 3 å±‚æ€»è®¡: ~330 MB

# æ‰“åˆ†é˜¶æ®µ
pair_neg: 16 Ã— 128 Ã— 4000 Ã— 4 bytes = 32 MB

# æ€»è®¡
Total activations: ~370 MB
```

**æ¢¯åº¦**ï¼ˆä¸å‚æ•°åŒå¤§å°ï¼‰:
```
Gradients: ~4.6 GB
```

**æ€» GPU å†…å­˜**:
```
Total = Params + Activations + Gradients
      = 4.6 + 0.37 + 4.6
      â‰ˆ 9.6 GB
```

**ä¼˜åŒ–å»ºè®®**ï¼ˆå¦‚æœ GPU å†…å­˜ä¸è¶³ï¼‰:
1. å‡å°‘ `hidden_dim`: 2000 â†’ 1000 (å†…å­˜å‡å°‘ 75%)
2. å‡å°‘ `batch_size`: 16 â†’ 8 (æ¿€æ´»å‡å°‘ 50%)
3. ä½¿ç”¨ FP16 æ··åˆç²¾åº¦ï¼ˆå†…å­˜å‡å°‘ 50%ï¼‰

---

## 6. æ€§èƒ½ä¼˜åŒ–ç‚¹

### 6.1 å·²å®ç°çš„ä¼˜åŒ–

#### 1. **PyG Scatter æ“ä½œ**
- ä½¿ç”¨é«˜åº¦ä¼˜åŒ–çš„ `scatter_add`, `scatter_max`
- æ¯” Python å¾ªç¯å¿« 100+ å€

#### 2. **æ‰¹é‡å¤„ç†**
- ä¸€æ¬¡å¤„ç†å¤šä¸ªæŸ¥è¯¢
- å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œ

#### 3. **è§„åˆ™å»é‡**
```python
active_rules = set()
for r in query_relations:
    if r in relation2rules:
        for rule in relation2rules[r]:
            active_rules.add(rule[0])
```
- é¿å…é‡å¤è®¡ç®—ç›¸åŒè§„åˆ™

#### 4. **Early Stopping**
```python
if patience_counter >= max_patience:
    break
```
- éªŒè¯æŒ‡æ ‡ä¸å†æå‡æ—¶æå‰åœæ­¢

### 6.2 å¯è¿›ä¸€æ­¥ä¼˜åŒ–çš„ç‚¹

#### 1. **å›¾é‡‡æ ·**ï¼ˆå½“å‰æœªå®ç°ï¼‰

å¯¹äºå¤§å›¾ï¼Œä¸éœ€è¦æ¯æ¬¡ä¼ æ’­æ•´ä¸ªå›¾ï¼š

```python
# é‡‡æ · k-hop é‚»åŸŸ
sampler = NeighborSampler(
    edge_index,
    sizes=[25, 10],  # æ¯å±‚é‡‡æ ·é‚»å±…æ•°
    batch_size=16
)

for batch in sampler:
    # åªåœ¨é‡‡æ ·å­å›¾ä¸Šä¼ æ’­
    h = conv(h, batch.edge_index, batch.edge_type, rule_ids)
```

**æ•ˆæœ**: å†…å­˜é™ä½ 80%ï¼Œé€Ÿåº¦æå‡ 3-5 å€

#### 2. **è§„åˆ™å‰ªæ**

è¿‡æ»¤ä½ç½®ä¿¡åº¦è§„åˆ™ï¼š

```python
# åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µ
for rule in rules:
    if rule.confidence < 0.1:
        continue  # è·³è¿‡
```

**æ•ˆæœ**: è§„åˆ™æ•°å‡å°‘ 50%ï¼Œé€Ÿåº¦æå‡ 2 å€

#### 3. **æ··åˆç²¾åº¦è®­ç»ƒ**

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    scores = model(...)
    loss = criterion(scores, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**æ•ˆæœ**: å†…å­˜å‡å°‘ 50%ï¼Œé€Ÿåº¦æå‡ 2-3 å€

#### 4. **ç¼“å­˜ GNN è¾“å‡º**

å¦‚æœå›¾ç»“æ„ä¸å˜ï¼š

```python
# é¢„è®¡ç®—æ‰€æœ‰å®ä½“çš„ GNN åµŒå…¥
with torch.no_grad():
    h_cached = gnn_forward(entity_embedding, edge_index, edge_type, all_rules)

# è®­ç»ƒæ—¶ç›´æ¥ä½¿ç”¨
head_emb = h_cached[queries[:, 0]]
```

**æ•ˆæœ**: è®­ç»ƒé€Ÿåº¦æå‡ 10 å€ï¼ˆä½†å†…å­˜å¼€é”€å¤§ï¼‰

---

## 7. è°ƒè¯•æŠ€å·§

### 7.1 æ¢¯åº¦æ£€æŸ¥

```python
# æ£€æŸ¥å“ªäº›å‚æ•°æœ‰æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is None:
        print(f"Warning: {name} has no gradient!")
    elif torch.isnan(param.grad).any():
        print(f"Error: {name} has NaN gradient!")
```

### 7.2 ä¸­é—´è¾“å‡ºå¯è§†åŒ–

```python
# åœ¨ RuleAwareGraphConv ä¸­
def forward(self, x, edge_index, edge_type, rule_ids, return_attention=False):
    ...
    if return_attention:
        return out, attn_weights
    return out

# ä½¿ç”¨
h, attn = conv(h, edge_index, edge_type, rule_ids, return_attention=True)

# å¯è§†åŒ–æ³¨æ„åŠ›
import matplotlib.pyplot as plt
plt.imshow(attn[:100, :10].cpu().numpy())
plt.xlabel('Rules')
plt.ylabel('Edges')
plt.colorbar()
plt.savefig('attention.png')
```

### 7.3 æ€§èƒ½åˆ†æ

```python
import torch.autograd.profiler as profiler

with profiler.profile(record_shapes=True, use_cuda=True) as prof:
    with profiler.record_function("model_forward"):
        scores = model(queries, edge_index, edge_type, rule_ids)

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

---

## 8. å¸¸è§ Bug åŠè§£å†³

### Bug 1: CUDA OOM

**ç°è±¡**: `RuntimeError: CUDA out of memory`

**æ’æŸ¥**:
```python
# æŸ¥çœ‹å†…å­˜å ç”¨
print(torch.cuda.memory_allocated() / 1e9, "GB")
print(torch.cuda.memory_reserved() / 1e9, "GB")

# æŸ¥çœ‹ Tensor å¤§å°
for name, tensor in model.named_parameters():
    print(name, tensor.shape, tensor.element_size() * tensor.nelement() / 1e6, "MB")
```

**è§£å†³**: è§ä¸Šæ–‡å†…å­˜ä¼˜åŒ–

### Bug 2: æ¢¯åº¦çˆ†ç‚¸

**ç°è±¡**: Loss çªç„¶å˜æˆ `NaN`

**è§£å†³**:
```python
# æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### Bug 3: è§„åˆ™ ID è¶Šç•Œ

**ç°è±¡**: `IndexError: index out of range`

**æ’æŸ¥**:
```python
print("Max rule ID:", max(rule_ids))
print("Num rules:", model.rule_embedding.num_embeddings)
```

**è§£å†³**: æ£€æŸ¥è§„åˆ™æ–‡ä»¶æ˜¯å¦æ­£ç¡®åŠ è½½

---

## 9. æ‰©å±•æ–¹å‘

### 9.1 å¤šè·³æ¨ç†

å½“å‰å®ç°æ˜¯å›ºå®šå±‚æ•°ï¼ˆ= è§„åˆ™æœ€å¤§é•¿åº¦ï¼‰ã€‚å¯ä»¥æ‰©å±•ä¸ºè‡ªé€‚åº”ï¼š

```python
class AdaptiveRuleGNN(nn.Module):
    def forward(self, queries, edge_index, edge_type, rule_ids):
        # æ ¹æ®è§„åˆ™é•¿åº¦åŠ¨æ€é€‰æ‹©å±‚æ•°
        for rule_id in rule_ids:
            rule_length = get_rule_length(rule_id)
            h = self.conv_layers[:rule_length](h, ...)
```

### 9.2 æ—¶åºçŸ¥è¯†å›¾è°±

æ·»åŠ æ—¶é—´ç»´åº¦ï¼š

```python
class TemporalRuleGNN(RuleGNN):
    def __init__(self, ...):
        ...
        self.time_encoder = nn.Linear(1, hidden_dim)

    def forward(self, queries, edge_index, edge_type, edge_time, rule_ids):
        # ç¼–ç æ—¶é—´
        time_emb = self.time_encoder(edge_time.unsqueeze(-1))

        # æ—¶é—´æ„ŸçŸ¥çš„æ¶ˆæ¯ä¼ é€’
        ...
```

### 9.3 å¯è§£é‡Šæ€§

è¿”å›æ¨ç†è·¯å¾„ï¼š

```python
def explain(self, query, predicted_tail):
    # è®°å½•æ³¨æ„åŠ›æƒé‡
    attentions = []
    for conv in self.conv_layers:
        h, attn = conv(h, ..., return_attention=True)
        attentions.append(attn)

    # å›æº¯é«˜æ³¨æ„åŠ›çš„è¾¹ï¼Œæ„å»ºæ¨ç†è·¯å¾„
    path = backtrace_path(query, predicted_tail, attentions)
    return path
```

---

## 10. æ€»ç»“

### æ ¸å¿ƒè®¾è®¡æ€æƒ³

1. **ç”¨ GNN æ›¿ä»£è·¯å¾„æšä¸¾**: ä» O(B^L) é™åˆ° O(EÃ—L)
2. **è§„åˆ™æ„ŸçŸ¥çš„æ³¨æ„åŠ›**: è®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ å“ªäº›è¾¹ç¬¦åˆå“ªäº›è§„åˆ™
3. **ç«¯åˆ°ç«¯å­¦ä¹ **: é¢„è®­ç»ƒåµŒå…¥ + GNN å¾®è°ƒ

### ä»£ç è´¨é‡

- **æ¨¡å—åŒ–**: æ¸…æ™°çš„ç±»å’Œå‡½æ•°åˆ’åˆ†
- **å¯æ‰©å±•**: æ˜“äºæ·»åŠ æ–°çš„ GNN å±‚æˆ–æ‰“åˆ†å‡½æ•°
- **æ–‡æ¡£å®Œå–„**: è¯¦ç»†çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²

### æ€§èƒ½è¡¨ç°

| æ¨¡å‹ | UMLS MRR | è®­ç»ƒæ—¶é—´ (GPU) |
|-----|----------|---------------|
| RulE | 0.867 | ~45 åˆ†é’Ÿ |
| Rule-GNN | 0.938 | ~60 åˆ†é’Ÿ |

**æå‡**: +7.1% MRRï¼Œè®­ç»ƒæ—¶é—´å¢åŠ  33%

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æ›´æ–°æ—¶é—´**: 2024-11-19
**ç»´æŠ¤è€…**: Rule-GNN Team
