# RulE模型术语解释与概念说明

## 目录
1. [核心概念](#核心概念)
2. [专有名词详解](#专有名词详解)
3. [技术术语](#技术术语)
4. [示例说明](#示例说明)

---

## 核心概念

### 1. Grounding (规则落地/规则实例化)

#### 什么是Grounding？

**Grounding**是将**抽象逻辑规则**映射到**具体知识图谱实例**的过程。

#### 直观理解

想象一下：
- **抽象规则**：像是一个"模板"或"公式"
- **Grounding**：把这个模板应用到实际数据上，找到所有满足条件的具体例子

#### 详细解释

在知识图谱中，逻辑规则是抽象的模式，例如：

**抽象规则**：
```
"如果A是B的父亲，B是C的父亲，那么A是C的祖父"
```

用关系表示：
```
fatherOf(X, Y) ∧ fatherOf(Y, Z) → grandfatherOf(X, Z)
```

**Grounding过程**就是：
给定一个具体的人X（比如"张三"），在知识图谱中找到所有满足这个规则的实例：

```
1. 张三 --fatherOf--> 李四 --fatherOf--> 王五
   ✓ Ground成功！推断：张三是王五的祖父

2. 张三 --fatherOf--> 李四 --fatherOf--> 赵六
   ✓ Ground成功！推断：张三是赵六的祖父

3. 张三 --fatherOf--> 李四 (但李四没有孩子)
   ✗ Ground失败！无法应用规则
```

#### 技术定义

给定：
- **查询**：$(h, r, ?)$ - 寻找头实体为$h$、关系为$r$的尾实体
- **规则**：$r_1 \land r_2 \land \cdots \land r_n \rightarrow r$

**Grounding过程**：
1. 从头实体$h$出发
2. 沿着关系$r_1$走一步，到达中间实体集合$M_1$
3. 从$M_1$沿着关系$r_2$走一步，到达$M_2$
4. 重复直到走完所有规则体中的关系
5. 最终到达的实体集合就是**被ground的实例**

#### 数学表示

设知识图谱为$\mathcal{G} = (E, R, T)$，其中：
- $E$：实体集合
- $R$：关系集合
- $T$：三元组集合

对于规则 $r_1 \land r_2 \rightarrow r_h$，从实体$h$出发的grounding：

$$
\begin{aligned}
M_0 &= \{h\} \\
M_1 &= \{e_1 : (h, r_1, e_1) \in T\} \\
M_2 &= \{e_2 : \exists e_1 \in M_1, (e_1, r_2, e_2) \in T\}
\end{aligned}
$$

$M_2$中的每个实体$e_2$都是规则在实体$h$上的一个**ground实例**。

#### RulE中的实现

在代码中，grounding通过图传播实现：

```python
def grounding(self, h, r, rule, edges_to_remove):
    # 初始化：从头实体h开始，用one-hot表示
    x = one_hot(h, num_entities)  # [num_entities]

    # 依次沿着规则体中的每个关系传播
    for r_body in rule:
        x = propagate(x, r_body)  # 沿着关系r_body传播

    # 最终x[i]表示从h出发、遵循规则体到达实体i的路径数量
    return x
```

**Grounding计数**：
- `x[i] = 0`：实体$i$无法通过该规则从$h$到达
- `x[i] = 1`：有1条路径通过该规则从$h$到达$i$
- `x[i] = 5`：有5条不同的路径通过该规则从$h$到达$i$

---

### 2. Rule (逻辑规则)

#### 什么是逻辑规则？

逻辑规则是知识图谱中的**模式**或**推理规则**，描述了实体之间关系的规律。

#### 规则的形式

**Horn子句形式**：
```
Body → Head
前提 → 结论
```

**具体例子**：

1. **家庭关系**：
   ```
   fatherOf(X,Y) ∧ fatherOf(Y,Z) → grandfatherOf(X,Z)
   "X是Y的父亲" ∧ "Y是Z的父亲" → "X是Z的祖父"
   ```

2. **地理关系**：
   ```
   locatedIn(X,Y) ∧ locatedIn(Y,Z) → locatedIn(X,Z)
   "X位于Y" ∧ "Y位于Z" → "X位于Z"
   例：北京位于中国，中国位于亚洲 → 北京位于亚洲
   ```

3. **社交网络**：
   ```
   friendOf(X,Y) ∧ friendOf(Y,Z) → mayKnow(X,Z)
   "X是Y的朋友" ∧ "Y是Z的朋友" → "X可能认识Z"
   ```

#### 规则的组成部分

- **规则头 (Head)**：结论部分的关系
- **规则体 (Body)**：前提部分的关系序列
- **规则长度**：规则体中关系的数量

例如规则 `r1 ∧ r2 ∧ r3 → r_head`：
- 头：`r_head`
- 体：`[r1, r2, r3]`
- 长度：3

#### RulE中的规则表示

规则存储为整数列表：
```
[rule_id, rule_head, r_body_1, r_body_2, ..., r_body_n]
```

**示例**：
```python
规则：关系5 ∧ 关系12 → 关系3
表示为：[0, 3, 5, 12]
       ↑  ↑  ↑   ↑
       ID 头 体1 体2
```

#### 规则挖掘

规则不是手工编写的，而是从知识图谱中**自动挖掘**的。

RulE使用**RNNLogic**等工具从训练数据中挖掘规则，存储在`mined_rules.txt`中：

```
# mined_rules.txt 格式
3 5 12        # 规则：5 ∧ 12 → 3
7 2 8 15      # 规则：2 ∧ 8 ∧ 15 → 7
```

---

### 3. Pre-training (预训练)

#### 什么是预训练？

预训练是模型的**第一阶段**，目标是学习知识图谱中实体、关系和规则的向量表示（嵌入）。

#### 预训练的目标

学习三类嵌入：
1. **实体嵌入**：每个实体映射到一个向量
2. **关系嵌入**：每个关系映射到一个向量
3. **规则嵌入**：每个规则映射到一个向量

#### 为什么需要预训练？

**问题**：原始的知识图谱是离散符号：
```
(张三, fatherOf, 李四)
(李四, fatherOf, 王五)
```

计算机无法直接计算"张三"和"李四"的相似度。

**解决方案**：将符号映射到连续向量空间：
```
张三 → [0.2, -0.5, 0.8, ...]
李四 → [0.3, -0.4, 0.7, ...]
fatherOf → [0.1, 0.9, -0.2, ...]
```

现在可以计算向量之间的距离、相似度等。

#### 预训练做什么？

1. **学习实体和关系的语义**：
   - 相似的实体应该有相似的向量
   - 满足相同模式的三元组应该有相似的分数

2. **学习规则的表示**：
   - 规则的向量应该接近其规则体关系的组合
   - 好的规则应该有高的评分

#### 训练方式

使用两个损失函数：

1. **三元组损失**（基于RotatE）：
   ```
   最大化：真实三元组的分数
   最小化：假的三元组的分数
   ```

2. **规则损失**：
   ```
   最大化：真实规则的分数
   最小化：破坏规则的分数
   ```

---

### 4. Knowledge Graph Embedding (KGE, 知识图谱嵌入)

#### 什么是知识图谱嵌入？

将知识图谱中的**实体**和**关系**映射到**低维连续向量空间**的技术。

#### 为什么需要嵌入？

**原始知识图谱**：
```
实体：离散符号 {e1, e2, e3, ...}
关系：离散符号 {r1, r2, r3, ...}
三元组：(头实体, 关系, 尾实体)
```

**问题**：
- 无法计算相似度
- 无法进行数值优化
- 难以泛化到未见过的实体

**嵌入后**：
```
实体：向量 e1 → [0.2, -0.5, 0.8, ...]
关系：向量 r1 → [0.1, 0.9, -0.2, ...]
```

**优势**：
- 可以计算语义相似度
- 支持梯度下降优化
- 向量空间中相近的实体语义相似

#### 常见的KGE方法

1. **TransE**：关系是平移
   ```
   h + r ≈ t
   ```

2. **DistMult**：双线性模型
   ```
   score = h^T · diag(r) · t
   ```

3. **RotatE**（RulE使用）：关系是旋转
   ```
   h ∘ r ≈ t  (复数空间的旋转)
   ```

---

### 5. RotatE

#### 什么是RotatE？

**RotatE** (Rotate in Complex Space) 是一种知识图谱嵌入方法，将关系建模为**复数空间中的旋转**。

#### 核心思想

在二维平面上旋转一个点：
```
        ↑ t (尾实体)
        |
    r (旋转)
       ↗
      h (头实体)
```

将关系$r$看作旋转操作，使头实体$h$旋转后得到尾实体$t$。

#### 数学表示

实体用**复数向量**表示：
```
h = [h1_real + i·h1_imag, h2_real + i·h2_imag, ...]
```

关系用**相位角**表示：
```
r = [θ1, θ2, ..., θd]
每个θ ∈ [0, 2π)
```

**旋转操作**：
```
t = h ∘ r
其中 ∘ 表示逐元素的复数乘法
```

复数乘法就是旋转：
```
(a + bi) · e^(iθ) = (a + bi) · (cos θ + i·sin θ)
```

#### 为什么使用RotatE？

**优势**：
1. 可以建模**对称关系**：$r(h,t) = r(t,h)$ → $\theta_r = 0$
2. 可以建模**反对称关系**：$r(h,t) \neq r(t,h)$ → $\theta_r \neq 0$
3. 可以建模**逆关系**：$r^{-1}$ → $-\theta_r$
4. 可以建模**组合关系**：$r_1 \circ r_2$ → $\theta_{r_1} + \theta_{r_2}$

#### 实现细节

实体嵌入维度加倍（存储实部和虚部）：
```python
entity_embedding: [num_entities, hidden_dim * 2]
                                ↑           ↑
                              实部        虚部
```

关系嵌入转换为相位角：
```python
relation_embedding: [num_relations, hidden_dim]
                                    ↓
phase = relation / (gamma / π) * π  # 映射到 [-π, π]
```

---

### 6. Negative Sampling (负采样)

#### 什么是负采样？

在训练知识图谱嵌入时，需要区分**真实三元组**和**虚假三元组**。

**正样本**（真实三元组）：
```
(北京, 首都, 中国) ✓ 存在于知识图谱中
```

**负样本**（虚假三元组）：
```
(北京, 首都, 美国) ✗ 不存在于知识图谱中
```

#### 为什么需要负采样？

**训练目标**：
- 提高正样本的分数
- 降低负样本的分数

如果只有正样本，模型会给所有三元组都打高分（过拟合）。

#### 负采样策略

**随机负采样**：
随机替换头实体或尾实体
```
正样本：(北京, 首都, 中国)
↓ 替换尾实体
负样本：(北京, 首都, 美国)
负样本：(北京, 首都, 日本)
负样本：(北京, 首都, 法国)
```

**过滤负采样**：
确保负样本确实不在知识图谱中
```python
while negative_sample in knowledge_graph:
    negative_sample = random_sample()
```

#### 自对抗负采样 (Self-Adversarial Negative Sampling)

RulE使用的**高级负采样策略**。

**思想**：不是均匀采样负样本，而是更多地采样**难区分的负样本**。

**权重计算**：
```
p(t'|h,r) = exp(α · score(h,r,t')) / Σ exp(α · score(h,r,t''))
```

- 分数高的负样本（更像真的）获得更高权重
- 分数低的负样本（明显是假的）权重较低

**效果**：
- 加速训练：不浪费时间在简单样本上
- 提高性能：模型学会区分困难样本

---

### 7. Margin-based Loss (基于边界的损失)

#### 什么是Margin？

**Margin（边界）**是正样本和负样本分数之间的**安全距离**。

#### 直观理解

想象一条分界线：
```
正样本分数：    8  9  10  11  12
                        ↑
                     margin = 6
                        ↓
负样本分数：    1  2  3  4  5
```

我们希望正样本分数 > 负样本分数 + margin

#### 数学表示

**Margin Loss**：
```
L = max(0, γ - score_positive + score_negative)
```

其中 $\gamma$ 是margin参数。

**含义**：
- 如果 `score_positive > score_negative + γ`：损失为0（满足要求）
- 否则：有损失，需要调整参数

#### RulE中的应用

RulE使用**两个margin**：

1. **$\gamma_{fact}$**：三元组的margin（通常为6）
   ```
   score_fact = γ_fact - distance(h, r, t)
   ```

2. **$\gamma_{rule}$**：规则的margin（通常为5-8）
   ```
   score_rule = γ_rule - distance(rule_body, rule_head)
   ```

---

### 8. Adversarial Temperature (对抗温度)

#### 什么是对抗温度？

控制负采样中**难样本**和**易样本**权重分布的参数。

#### 数学表示

负样本权重：
```
p(t'|h,r,t) = exp(α · score(t')) / Σ exp(α · score(t''))
```

其中 $\alpha$ 是**对抗温度**。

#### 温度的影响

**高温度（α很大，如α=1.0）**：
```
分数: [1, 2, 3, 4, 5]
权重: [0.01, 0.02, 0.09, 0.25, 0.67]  ← 差异很大，聚焦于高分样本
```

**低温度（α很小，如α=0.1）**：
```
分数: [1, 2, 3, 4, 5]
权重: [0.16, 0.18, 0.19, 0.21, 0.24]  ← 差异较小，接近均匀分布
```

**零温度（α=0）**：
```
权重: [0.20, 0.20, 0.20, 0.20, 0.20]  ← 完全均匀分布
```

#### RulE中的使用

```python
adversarial_temperature = 0.5  # 初始温度

# Warm-up阶段后
if args.disable_adv:
    adversarial_temperature = 0  # 关闭对抗采样
```

**训练策略**：
- 早期：使用中等温度（0.25-0.5），帮助快速学习
- 后期：降低或关闭，使训练更稳定

---

### 9. Label Smoothing (标签平滑)

#### 什么是标签平滑？

**标签平滑**是一种正则化技术，防止模型过度自信。

#### 问题

**硬标签**（传统方法）：
```
查询：(北京, 首都, ?)
标签：[0, 0, 0, ..., 1, ..., 0]
                    ↑
                  中国=1
```

模型会极度自信地预测"中国"，对其他实体的概率接近0。

**问题**：
- 过拟合：对训练数据过度自信
- 泛化差：对相似实体（如"中华人民共和国"）给分很低

#### 解决方案

**软标签**（标签平滑）：
```
原始标签：        [0, 0, 0, ..., 1, ..., 0]
平滑后(ε=0.2)：   [0.04, 0.04, 0.04, ..., 0.84, ..., 0.04]
```

#### 数学公式

```
ỹ = (1 - ε) · y + ε · y_current
```

其中：
- $y$：所有可能的正确答案（one-hot或multi-hot）
- $y_{current}$：当前查询的答案
- $\epsilon$：平滑系数（0.1-0.5）

#### RulE中的应用

```python
# 原始标签：所有真实尾实体
target[k][all_correct_tails] = 1

# 当前查询的尾实体
target_t = one_hot(current_tail)

# 标签平滑
target = target * smoothing + target_t * (1 - smoothing)
```

**效果**：
- 鼓励模型给多个合理答案较高分数
- 减少过拟合
- 提高泛化能力

---

### 10. Filtered Evaluation (过滤评估)

#### 什么是过滤评估？

在评估时，排除**所有已知的真实三元组**，只对真正的负样本排序。

#### 问题场景

**查询**：`(张三, fatherOf, ?)`

**知识图谱中的真实答案**：
```
(张三, fatherOf, 李四) ✓
(张三, fatherOf, 王五) ✓
```

**未过滤评估**：
```
排序：
1. 李四  (score: 0.9) ✓
2. 赵六  (score: 0.8) ✗
3. 王五  (score: 0.7) ✓  ← 排名=3，但这是正确答案！
```

王五虽然是正确答案，但因为李四分数更高，排名是3。这不公平！

#### 过滤评估

**排除已知的其他正确答案**：
```
候选集：移除李四（因为也是正确答案）

排序：
1. 赵六  (score: 0.8) ✗
2. 王五  (score: 0.7) ✓  ← 排名=2
```

现在王五的排名是2，更合理地反映了模型性能。

#### 实现

```python
# 标记所有真实三元组
mask = torch.ones(num_entities).bool()
for (h, r, t) in all_true_triples:
    mask[t] = False  # 排除所有真实尾实体

# 只对非真实的实体计算排名
rank = (scores[mask] > score_target).sum() + 1
```

---

### 11. Expectation Ranking (期望排名)

#### 什么是期望排名？

当多个候选实体有**相同分数**时，使用它们排名的**期望值**而不是最好或最坏排名。

#### 问题场景

**查询结果**：
```
实体    分数
e1      0.9
e2      0.8
e3      0.8  ← 目标实体
e4      0.8
e5      0.7
```

目标实体e3与e2、e4分数相同（都是0.8）。

**三种排名方式**：

1. **最好排名**（Optimistic）：
   ```
   排名 = 2  (假设e3排在所有0.8分的最前面)
   ```

2. **最坏排名**（Pessimistic）：
   ```
   排名 = 4  (假设e3排在所有0.8分的最后面)
   ```

3. **期望排名**（Expectation）：
   ```
   排名 = (2 + 3 + 4) / 3 = 3
   ```

#### RulE的实现

定义两个变量：
- $L$：比目标分数**严格更高**的实体数 + 1
- $H$：比目标分数**大于等于**的实体数 + 2

```python
L = (scores > target_score).sum() + 1  # 严格大于
H = (scores >= target_score).sum() + 2 # 大于等于
```

**期望排名计算**：
```python
for rank in range(L, H):
    if rank <= 1:
        hit1 += 1.0 / (H - L)
    if rank <= 3:
        hit3 += 1.0 / (H - L)
    if rank <= 10:
        hit10 += 1.0 / (H - L)
    mr += rank / (H - L)
    mrr += 1.0 / rank / (H - L)
```

**示例**：
```
L = 2, H = 5  (有3个实体分数相同，可能排名为2、3、4)

MRR = 1/(H-L) * (1/2 + 1/3 + 1/4)
    = 1/3 * (0.5 + 0.333 + 0.25)
    = 0.361
```

---

### 12. Hits@k

#### 什么是Hits@k？

**Hits@k** 衡量正确答案是否出现在**前k个预测**中。

#### 定义

```
Hits@k = (正确答案排名 ≤ k 的查询数量) / (总查询数量)
```

#### 常用的k值

- **Hits@1**：正确答案排名第1的比例（最严格）
- **Hits@3**：正确答案排名前3的比例
- **Hits@10**：正确答案排名前10的比例（最宽松）

#### 示例

100个查询的结果：
```
查询1：正确答案排名 1  ✓ 计入Hits@1, @3, @10
查询2：正确答案排名 2  ✓ 计入Hits@3, @10
查询3：正确答案排名 5  ✓ 计入Hits@10
查询4：正确答案排名 15 ✗ 都不计入
...
```

假设统计结果：
```
排名≤1的查询：30个
排名≤3的查询：55个
排名≤10的查询：75个

Hits@1  = 30 / 100 = 0.30
Hits@3  = 55 / 100 = 0.55
Hits@10 = 75 / 100 = 0.75
```

#### 解释

- **Hits@1 = 0.30**：30%的情况下，模型的第一个预测就是正确的
- **Hits@10 = 0.75**：75%的情况下，正确答案在前10个预测中

---

### 13. MRR (Mean Reciprocal Rank, 平均倒数排名)

#### 什么是MRR？

**MRR** 是所有查询的**倒数排名**的平均值。

#### 公式

```
MRR = (1/N) * Σ (1 / rank_i)
```

其中：
- $N$：查询总数
- $rank_i$：第$i$个查询的正确答案排名

#### 直观理解

**倒数排名的意义**：
- 排名1：倒数 = 1/1 = 1.0
- 排名2：倒数 = 1/2 = 0.5
- 排名3：倒数 = 1/3 = 0.33
- 排名10：倒数 = 1/10 = 0.1
- 排名100：倒数 = 1/100 = 0.01

排名越靠前，倒数越大，对MRR贡献越大。

#### 示例

5个查询的排名：
```
查询1：排名 1  → 倒数 = 1.0
查询2：排名 2  → 倒数 = 0.5
查询3：排名 5  → 倒数 = 0.2
查询4：排名 10 → 倒数 = 0.1
查询5：排名 100 → 倒数 = 0.01

MRR = (1.0 + 0.5 + 0.2 + 0.1 + 0.01) / 5 = 0.362
```

#### MRR vs Hits@k

**区别**：
- **Hits@k**：只看排名是否在前k，二元指标（是/否）
- **MRR**：考虑具体排名，排名1比排名3更好

**示例**：
```
模型A：所有查询排名都是1
→ MRR = 1.0, Hits@3 = 1.0

模型B：所有查询排名都是3
→ MRR = 0.33, Hits@3 = 1.0

两个模型Hits@3相同，但MRR显示模型A更好
```

---

## 示例说明

### 完整的知识图谱推理例子

#### 场景设置

**知识图谱**：
```
(张三, fatherOf, 李四)
(李四, fatherOf, 王五)
(李四, fatherOf, 赵六)
(张三, fatherOf, 孙七)
```

**挖掘的规则**：
```
规则1: fatherOf(X,Y) ∧ fatherOf(Y,Z) → grandfatherOf(X,Z)
```

#### 查询：张三的孙子是谁？

**查询形式**：`(张三, grandfatherOf, ?)`

---

### 步骤1：规则落地 (Grounding)

**目标**：找到所有满足规则的路径

**初始化**：
```
从张三出发：x = [0, 0, 1, 0, 0, 0, 0]
                     ↑
                   张三=1
```

**第一跳**（沿着fatherOf）：
```
张三 --fatherOf--> 李四
张三 --fatherOf--> 孙七

x = [0, 0, 0, 1, 0, 0, 1]
              ↑        ↑
            李四=1   孙七=1
```

**第二跳**（再次沿着fatherOf）：
```
李四 --fatherOf--> 王五
李四 --fatherOf--> 赵六
孙七 --fatherOf--> (无)

x = [0, 0, 0, 0, 1, 1, 0]
                 ↑  ↑
              王五=1 赵六=1
```

**Grounding结果**：
```
候选实体：王五、赵六
路径计数：
  - 王五：1条路径 (张三→李四→王五)
  - 赵六：1条路径 (张三→李四→赵六)
```

---

### 步骤2：规则聚合

**规则特征**：
```
规则1的MLP特征：f_rule1 = [0.2, -0.5, 0.8, ...]
```

**聚合计算**：
```
对于王五：
  h_王五 = 1 * f_rule1 = [0.2, -0.5, 0.8, ...]

对于赵六：
  h_赵六 = 1 * f_rule1 = [0.2, -0.5, 0.8, ...]
```

---

### 步骤3：MLP评分

```
s_王五 = MLP(h_王五) + bias_王五 = 0.85
s_赵六 = MLP(h_赵六) + bias_赵六 = 0.82
```

---

### 步骤4：混合推理（可选）

结合KGE分数：
```
KGE_score_王五 = RotatE(张三, grandfatherOf, 王五) = 0.3
KGE_score_赵六 = RotatE(张三, grandfatherOf, 赵六) = 0.25

最终分数：
s_王五 = 0.85 + α * 0.3  = 0.85 + 2.0 * 0.3 = 1.45
s_赵六 = 0.82 + α * 0.25 = 0.82 + 2.0 * 0.25 = 1.32
```

---

### 步骤5：排名与评估

**排序结果**：
```
1. 王五  (分数: 1.45)
2. 赵六  (分数: 1.32)
3. 李四  (分数: 0.10)
4. 孙七  (分数: 0.05)
...
```

**假设真实答案是王五和赵六**：

**评估指标**：
```
对于王五：
  排名 = 1
  MRR += 1/1 = 1.0
  Hits@1 += 1
  Hits@3 += 1
  Hits@10 += 1

对于赵六：
  排名 = 2
  MRR += 1/2 = 0.5
  Hits@1 += 0
  Hits@3 += 1
  Hits@10 += 1
```

---

### 对比：有规则 vs 无规则

**无规则（纯KGE）**：
```
如果知识图谱中没有(张三, grandfatherOf, ?)的训练样本
→ RotatE只能根据嵌入猜测
→ 分数可能很低，排名靠后
```

**有规则（RulE）**：
```
即使没见过(张三, grandfatherOf, ?)
→ 通过规则推理：张三→李四→王五
→ 成功找到正确答案
→ 泛化能力强
```

---

## 术语速查表

| 术语 | 中文 | 简要说明 |
|-----|------|---------|
| Grounding | 规则落地/实例化 | 将抽象规则应用到具体知识图谱实例 |
| Rule | 逻辑规则 | 知识图谱中的推理模式，如 A∧B→C |
| Pre-training | 预训练 | 学习实体、关系、规则的向量表示 |
| KGE | 知识图谱嵌入 | 将实体和关系映射到向量空间 |
| RotatE | 旋转嵌入 | 在复数空间中用旋转建模关系 |
| Negative Sampling | 负采样 | 生成虚假三元组作为负样本 |
| Self-Adversarial | 自对抗采样 | 优先采样难区分的负样本 |
| Margin | 边界 | 正负样本分数之间的安全距离 |
| Adversarial Temperature | 对抗温度 | 控制负采样难度的参数 |
| Label Smoothing | 标签平滑 | 软化标签防止过拟合 |
| Filtered Evaluation | 过滤评估 | 排除已知正样本后评估 |
| Expectation Ranking | 期望排名 | 处理分数相同时的平均排名 |
| Hits@k | 命中率@k | 正确答案在前k的比例 |
| MRR | 平均倒数排名 | 倒数排名的平均值 |
| MR | 平均排名 | 排名的平均值 |
| Entity Embedding | 实体嵌入 | 实体的向量表示 |
| Relation Embedding | 关系嵌入 | 关系的向量表示 |
| Rule Embedding | 规则嵌入 | 规则的向量表示 |
| Triplet | 三元组 | (头实体, 关系, 尾实体) |
| Head Entity | 头实体 | 三元组中的起始实体 |
| Tail Entity | 尾实体 | 三元组中的目标实体 |
| Rule Head | 规则头 | 规则的结论部分 |
| Rule Body | 规则体 | 规则的前提部分 |
| Message Passing | 消息传递 | 图神经网络中的信息传播 |
| Propagation | 传播 | 沿着关系边传递信息 |
| Aggregation | 聚合 | 汇总多个信息源 |

---

## 关键概念关系图

```
知识图谱推理
    │
    ├─── 嵌入学习 (Pre-training)
    │       │
    │       ├─── KGE (RotatE)
    │       │      └─── 实体嵌入 + 关系嵌入
    │       │
    │       └─── 规则嵌入
    │              └─── 规则体聚合
    │
    ├─── 规则落地 (Grounding)
    │       │
    │       ├─── 图传播 (Message Passing)
    │       │      └─── 多跳推理
    │       │
    │       └─── 实例计数
    │              └─── 路径统计
    │
    └─── 推理与评估
            │
            ├─── 分数计算
            │      ├─── 纯规则推理
            │      └─── 混合推理 (Rule + KGE)
            │
            └─── 评估指标
                   ├─── MRR
                   ├─── Hits@k
                   └─── MR
```

---

## 总结

本文档解释了RulE模型中的核心概念：

1. **Grounding（规则落地）**：将抽象规则应用到具体知识图谱，通过多跳图传播找到满足规则的实例

2. **Pre-training（预训练）**：学习实体、关系、规则的向量表示，为后续推理奠定基础

3. **各种训练技巧**：负采样、标签平滑、对抗训练等提升模型性能

4. **评估方法**：Filtered评估、期望排名、MRR、Hits@k等指标衡量模型效果

这些概念共同构成了RulE模型的理论基础和实现框架。
