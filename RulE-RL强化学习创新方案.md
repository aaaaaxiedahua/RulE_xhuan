# RulE-RL: åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”è§„åˆ™æ¨ç†æ¡†æ¶

**ç»“åˆRulEæ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ çš„åˆ›æ–°æ–¹æ¡ˆ**

---

## ğŸ“‹ ç›®å½•

1. [èƒŒæ™¯ä¸åŠ¨æœº](#èƒŒæ™¯ä¸åŠ¨æœº)
2. [æ ¸å¿ƒåˆ›æ–°ç‚¹](#æ ¸å¿ƒåˆ›æ–°ç‚¹)
3. [RulE-RLæ¨¡å‹æ¶æ„](#rule-rlæ¨¡å‹æ¶æ„)
4. [å¼ºåŒ–å­¦ä¹ å»ºæ¨¡](#å¼ºåŒ–å­¦ä¹ å»ºæ¨¡)
5. [å®Œæ•´ä»£ç å®ç°](#å®Œæ•´ä»£ç å®ç°)
6. [å®éªŒè®¾è®¡](#å®éªŒè®¾è®¡)
7. [ç†è®ºåˆ†æ](#ç†è®ºåˆ†æ)
8. [è¿›é˜¶æ‰©å±•](#è¿›é˜¶æ‰©å±•)

---

## ğŸ¯ ä¸€ã€èƒŒæ™¯ä¸åŠ¨æœº

### 1.1 RulEæ¨¡å‹çš„å…³é”®é—®é¢˜

æ ¹æ®ACL 2024è®ºæ–‡å’Œç°æœ‰ä»£ç åˆ†æï¼ŒRulEå­˜åœ¨ä»¥ä¸‹æ ¸å¿ƒé—®é¢˜ï¼š

#### é—®é¢˜1ï¼šé™æ€è§„åˆ™é€‰æ‹©

**ç°çŠ¶**ï¼ˆsrc/model.py:354ï¼‰ï¼š
```python
# RulEä½¿ç”¨æ‰€æœ‰ä¸å…³ç³»rç›¸å…³çš„è§„åˆ™
for rule in self.relation2rules[query_r]:
    grounding_count = graph.grounding(h, rule)
    # æ‰€æœ‰è§„åˆ™ä¸€è§†åŒä»
```

**é—®é¢˜**ï¼š
- âŒ ä¸åŒºåˆ†è§„åˆ™è´¨é‡å’Œé€‚ç”¨åœºæ™¯
- âŒ å³ä½¿æŸäº›è§„åˆ™ä¸é€‚ç”¨å½“å‰æŸ¥è¯¢ï¼Œä»ç„¶è®¡ç®—
- âŒ æµªè´¹è®¡ç®—èµ„æºåœ¨ä½è´¨é‡è§„åˆ™ä¸Š

**æ•°æ®æ”¯æŒ**ï¼ˆè®ºæ–‡Table 4ï¼‰ï¼š
```
æ¶ˆèå®éªŒæ˜¾ç¤ºï¼š
- hard-encoding (0/1è§„åˆ™é€‰æ‹©): MRR = 0.330
- soft-encoding (è§„åˆ™ç½®ä¿¡åº¦): MRR = 0.335
â†’ åªæå‡äº†0.005ï¼Œè¯´æ˜è§„åˆ™é€‰æ‹©æœºåˆ¶è¿˜ä¸å¤Ÿæ™ºèƒ½
```

#### é—®é¢˜2ï¼šå›ºå®šçš„è§„åˆ™ç½®ä¿¡åº¦

**ç°çŠ¶**ï¼ˆè®ºæ–‡Equation 6ï¼‰ï¼š
```python
w_i = Î³_r - d(r_i1, ..., r_{il+1}, R_i)
```

**é—®é¢˜**ï¼š
- âŒ è§„åˆ™ç½®ä¿¡åº¦ä¸å…·ä½“æŸ¥è¯¢æ— å…³
- âŒ æ— æ³•é€‚åº”ä¸åŒçš„æ¨ç†åœºæ™¯
- âŒ ä¸èƒ½ä»æ¨ç†ç»éªŒä¸­å­¦ä¹ 

**æ¡ˆä¾‹åˆ†æ**ï¼š
```
è§„åˆ™: works_in(x,y) â†’ lives_in(x,y)

æŸ¥è¯¢1: (Bill Gates, lives_in, ?)
â†’ è¯¥è§„åˆ™å¯èƒ½ä¸å¤ªé€‚ç”¨ï¼ˆä½åœ¨éƒŠåŒºï¼‰

æŸ¥è¯¢2: (å°ä¼ä¸šå‘˜å·¥, lives_in, ?)
â†’ è¯¥è§„åˆ™å¯èƒ½æ›´é€‚ç”¨ï¼ˆä½å¾—è¿‘ï¼‰

ä½†RulEç»™ä¸¤ä¸ªæŸ¥è¯¢çš„è§„åˆ™ç½®ä¿¡åº¦æ˜¯ç›¸åŒçš„ï¼
```

#### é—®é¢˜3ï¼šè´ªå©ªçš„è·¯å¾„æ¢ç´¢

**ç°çŠ¶**ï¼ˆsrc/data.py:410-421ï¼‰ï¼š
```python
def grounding(self, h, r, rule_body, edges_to_remove):
    # BFSæšä¸¾æ‰€æœ‰å¯èƒ½è·¯å¾„
    for rel in rule_body:
        h = self.propagate(h, rel, ...)
    return grounding_count
```

**é—®é¢˜**ï¼š
- âŒ æšä¸¾æ‰€æœ‰è·¯å¾„ï¼Œæ•ˆç‡ä½
- âŒ æ— æ³•å­¦ä¹ å“ªäº›è·¯å¾„æ›´æœ‰ä»·å€¼
- âŒ ç¼ºä¹æ¢ç´¢vsåˆ©ç”¨çš„å¹³è¡¡

**å¤æ‚åº¦åˆ†æ**ï¼š
```
å¯¹äºè§„åˆ™ r1 âˆ§ r2 âˆ§ r3 â†’ r4ï¼š
- å¹³å‡åˆ†æ”¯å› å­: b = 50
- è·¯å¾„æ•°: b^3 = 125,000
â†’ å¤§é‡è·¯å¾„å…¶å®æ˜¯æ— æ•ˆçš„
```

### 1.2 å¼ºåŒ–å­¦ä¹ çš„å¤©ç„¶å¥‘åˆæ€§

#### å¥‘åˆç‚¹1ï¼šè§„åˆ™æ¨ç†æ˜¯åºè´¯å†³ç­–è¿‡ç¨‹

**è§„åˆ™åº”ç”¨ = é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰**ï¼š

```
çŠ¶æ€(State): å½“å‰å®ä½“èŠ‚ç‚¹
åŠ¨ä½œ(Action): é€‰æ‹©éµå¾ªå“ªæ¡å…³ç³»è¾¹
å¥–åŠ±(Reward): æ˜¯å¦åˆ°è¾¾ç›®æ ‡å®ä½“
ç­–ç•¥(Policy): è§„åˆ™æŒ‡å¯¼çš„è·¯å¾„é€‰æ‹©
```

**ä¸¾ä¾‹**ï¼š
```
æŸ¥è¯¢: (å¼ ä¸‰, grandfather, ?)

MDPè¿‡ç¨‹:
s0 = å¼ ä¸‰
a1 = é€‰æ‹©fatherå…³ç³» â†’ s1 = æå›› (reward = 0)
a2 = å†é€‰fatherå…³ç³» â†’ s2 = ç‹äº” (reward = +1, å¦‚æœç‹äº”æ˜¯æ­£ç¡®ç­”æ¡ˆ)
```

#### å¥‘åˆç‚¹2ï¼šè§„åˆ™é€‰æ‹©æ˜¯å¤šè‡‚è€è™æœºé—®é¢˜

**æ¯ä¸ªè§„åˆ™ = ä¸€ä¸ªè‡‚ï¼ˆarmï¼‰**ï¼š

```
å¤šè‡‚è€è™æœº(Multi-Armed Bandit):
- Kä¸ªè‡‚: Kæ¡è§„åˆ™
- æ¯æ¬¡é€‰æ‹©ä¸€ä¸ªè‡‚(è§„åˆ™)
- è·å¾—å¥–åŠ±(æ¨ç†å‡†ç¡®æ€§)
- ç›®æ ‡: æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±
```

**RulEçš„è§„åˆ™é€‰æ‹©å¯ä»¥å»ºæ¨¡ä¸ºContextual Bandit**ï¼š
- Context: æŸ¥è¯¢(h, r, ?)
- Arms: æ‰€æœ‰ä¸rç›¸å…³çš„è§„åˆ™
- Reward: é¢„æµ‹æ˜¯å¦æ­£ç¡®

#### å¥‘åˆç‚¹3ï¼šæ¢ç´¢vsåˆ©ç”¨çš„æƒè¡¡

**RulEçš„å›°å¢ƒ**ï¼š
```
æƒ…å†µ1: ä½¿ç”¨æ‰€æœ‰è§„åˆ™ â†’ è®¡ç®—å¼€é”€å¤§
æƒ…å†µ2: åªç”¨é«˜ç½®ä¿¡åº¦è§„åˆ™ â†’ å¯èƒ½é”™è¿‡æ½œåœ¨æœ‰ç”¨è§„åˆ™
```

**RLçš„è§£å†³æ–¹æ¡ˆ**ï¼š
- Îµ-greedy: ä»¥æ¦‚ç‡Îµæ¢ç´¢æ–°è§„åˆ™
- UCB: ä¼˜å…ˆé€‰æ‹©ä¸ç¡®å®šæ€§é«˜çš„è§„åˆ™
- Thompson Sampling: åŸºäºè´å¶æ–¯åéªŒé‡‡æ ·

### 1.3 åˆ›æ–°åŠ¨æœºæ€»ç»“

**æ ¸å¿ƒæ€æƒ³**ï¼š
å°†è§„åˆ™æ¨ç†å»ºæ¨¡ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œè®©æ¨¡å‹ï¼š
1. âœ… **è‡ªé€‚åº”é€‰æ‹©**æœ€é€‚åˆå½“å‰æŸ¥è¯¢çš„è§„åˆ™
2. âœ… **åŠ¨æ€è°ƒæ•´**è§„åˆ™ç½®ä¿¡åº¦
3. âœ… **é«˜æ•ˆæ¢ç´¢**çŸ¥è¯†å›¾è°±è·¯å¾„
4. âœ… **ä»ç»éªŒå­¦ä¹ **æ”¹è¿›æ¨ç†ç­–ç•¥

**é¢„æœŸæ”¶ç›Š**ï¼š
- æ¨ç†é€Ÿåº¦æå‡3-5å€ï¼ˆè·³è¿‡æ— å…³è§„åˆ™ï¼‰
- MRRæå‡5-10%ï¼ˆæ›´æ™ºèƒ½çš„è§„åˆ™é€‰æ‹©ï¼‰
- æ³›åŒ–èƒ½åŠ›å¢å¼ºï¼ˆé€‚åº”ä¸åŒæŸ¥è¯¢åœºæ™¯ï¼‰

---

## ğŸ’¡ äºŒã€æ ¸å¿ƒåˆ›æ–°ç‚¹

### åˆ›æ–°ç‚¹1ï¼šå±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ æ¡†æ¶

**åŒå±‚RLæ¶æ„**ï¼š

```
é«˜å±‚Agent (Rule Selector):
- è¾“å…¥: æŸ¥è¯¢ (h, r, ?)
- åŠ¨ä½œ: é€‰æ‹©Kæ¡æœ€ç›¸å…³çš„è§„åˆ™
- å¥–åŠ±: åŸºäºè§„åˆ™çš„é¢„æµ‹å‡†ç¡®æ€§

ä½å±‚Agent (Path Finder):
- è¾“å…¥: å½“å‰å®ä½“ + é€‰å®šè§„åˆ™
- åŠ¨ä½œ: é€‰æ‹©ä¸‹ä¸€æ¡è¾¹
- å¥–åŠ±: æ˜¯å¦åˆ°è¾¾æ­£ç¡®ç­”æ¡ˆ
```

**ä¸RulEçš„å¯¹æ¯”**ï¼š

| ç»´åº¦ | RulE | RulE-RL |
|------|------|---------|
| è§„åˆ™é€‰æ‹© | ä½¿ç”¨æ‰€æœ‰è§„åˆ™ | RLåŠ¨æ€é€‰æ‹©Top-K |
| è§„åˆ™æƒé‡ | å›ºå®šç½®ä¿¡åº¦å…¬å¼ | RLå­¦ä¹ çš„Qå€¼ |
| è·¯å¾„æ¢ç´¢ | BFSæšä¸¾ | RLç­–ç•¥æŒ‡å¯¼ |
| é€‚åº”æ€§ | é™æ€ | è‡ªé€‚åº”æŸ¥è¯¢åœºæ™¯ |

### åˆ›æ–°ç‚¹2ï¼šè§„åˆ™æ„ŸçŸ¥çš„ç­–ç•¥ç½‘ç»œ

**Policy Networkè®¾è®¡**ï¼š

```python
Ï€(a|s, R) = Policy(state, rule_context)
```

**å…³é”®æœºåˆ¶**ï¼š
- çŠ¶æ€ç¼–ç å™¨ï¼šèåˆå½“å‰å®ä½“å’Œå†å²è·¯å¾„
- è§„åˆ™ç¼–ç å™¨ï¼šåˆ©ç”¨RulEçš„è§„åˆ™åµŒå…¥
- æ³¨æ„åŠ›æœºåˆ¶ï¼šåŠ¨æ€å…³æ³¨ç›¸å…³è§„åˆ™

**ä¸ä¼ ç»ŸRLçš„åŒºåˆ«**ï¼š
```
ä¼ ç»ŸRL (å¦‚MINERVA):
Ï€(a|s) = Policy(state)
â†’ åªçœ‹å½“å‰çŠ¶æ€ï¼Œå¿½ç•¥è§„åˆ™ç»“æ„

RulE-RL:
Ï€(a|s, R) = Policy(state, rule_embedding)
â†’ æ˜¾å¼åˆ©ç”¨è§„åˆ™çŸ¥è¯†
```

### åˆ›æ–°ç‚¹3ï¼šè¯¾ç¨‹å­¦ä¹ ç­–ç•¥

**é—®é¢˜**ï¼š
ç›´æ¥ç”¨RLè®­ç»ƒå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼ˆåªå­¦ä¼šç®€å•è§„åˆ™ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
```
é˜¶æ®µ1: çŸ­è§„åˆ™ (1-hop)
â†’ å­¦ä¹ åŸºç¡€ç­–ç•¥

é˜¶æ®µ2: ä¸­ç­‰è§„åˆ™ (2-hop)
â†’ å­¦ä¹ ç»„åˆæ¨ç†

é˜¶æ®µ3: é•¿è§„åˆ™ (3+ hop)
â†’ å­¦ä¹ å¤æ‚æ¨ç†

é˜¶æ®µ4: æ··åˆè§„åˆ™
â†’ å­¦ä¹ è§„åˆ™é€‰æ‹©
```

**å®ç°**ï¼š
```python
def curriculum_scheduler(epoch):
    if epoch < 10:
        return rules_1hop
    elif epoch < 30:
        return rules_1hop + rules_2hop
    else:
        return all_rules
```

### åˆ›æ–°ç‚¹4ï¼šå¥–åŠ±å¡‘å½¢ï¼ˆReward Shapingï¼‰

**é—®é¢˜**ï¼š
ç¨€ç–å¥–åŠ±ï¼ˆåªæœ‰åˆ°è¾¾ç›®æ ‡æ‰æœ‰å¥–åŠ±ï¼‰å¯¼è‡´è®­ç»ƒå›°éš¾

**RulE-RLçš„å¥–åŠ±è®¾è®¡**ï¼š

```python
# 1. æœ€ç»ˆå¥–åŠ±ï¼ˆä¸»è¦ï¼‰
r_final = +1  if reach_correct_entity else -1

# 2. ä¸­é—´å¥–åŠ±ï¼ˆå¼•å¯¼ï¼‰
r_intermediate = {
    'rule_consistency': 0.1,   # éµå¾ªè§„åˆ™ä½“
    'getting_closer': 0.05,    # æ¥è¿‘ç›®æ ‡ï¼ˆåŸºäºåµŒå…¥è·ç¦»ï¼‰
    'diversity': 0.02,         # æ¢ç´¢æ–°è·¯å¾„
}

# 3. æƒ©ç½šé¡¹ï¼ˆé¿å…ï¼‰
r_penalty = {
    'dead_end': -0.1,          # èµ°å…¥æ­»èƒ¡åŒ
    'loop': -0.05,             # é‡å¤è®¿é—®èŠ‚ç‚¹
    'too_long': -0.02,         # è·¯å¾„è¿‡é•¿
}

total_reward = r_final + sum(r_intermediate) + sum(r_penalty)
```

**ç†è®ºä¾æ®**ï¼š
- åŸºäºRulEçš„è§„åˆ™åµŒå…¥è®¡ç®—"rule_consistency"
- åŸºäºRotatEçš„å®ä½“åµŒå…¥è®¡ç®—"getting_closer"
- ç»“åˆç¬¦å·æ¨ç†å’Œç¥ç»æ¨ç†çš„ä¼˜åŠ¿

---

## ğŸ—ï¸ä¸‰ã€RulE-RLæ¨¡å‹æ¶æ„

### 3.1 æ€»ä½“æ¡†æ¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RulE-RL Framework                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚        High-Level Agent (Rule Selector)           â”‚  â”‚
â”‚  â”‚                                                     â”‚  â”‚
â”‚  â”‚  Input: Query (h, r, ?)                           â”‚  â”‚
â”‚  â”‚  Output: Selected Rules {R1, R2, ..., Rk}         â”‚  â”‚
â”‚  â”‚  Method: Contextual Bandit / DQN                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                              â”‚
â”‚                           â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Low-Level Agent (Path Finder)             â”‚  â”‚
â”‚  â”‚                                                     â”‚  â”‚
â”‚  â”‚  Input: Current Entity + Selected Rules           â”‚  â”‚
â”‚  â”‚  Output: Next Relation to Follow                  â”‚  â”‚
â”‚  â”‚  Method: Policy Gradient (REINFORCE / PPO)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                              â”‚
â”‚                           â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Environment (Knowledge Graph)            â”‚  â”‚
â”‚  â”‚                                                     â”‚  â”‚
â”‚  â”‚  State: Current Entity                            â”‚  â”‚
â”‚  â”‚  Action: Select Relation                          â”‚  â”‚
â”‚  â”‚  Transition: Follow Edge in KG                    â”‚  â”‚
â”‚  â”‚  Reward: Reach Target or Intermediate Rewards     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 çŠ¶æ€ç©ºé—´è®¾è®¡

**çŠ¶æ€è¡¨ç¤º**ï¼š
```python
s_t = [
    h_entity,      # å½“å‰å®ä½“åµŒå…¥ (from RulE)
    h_query_rel,   # æŸ¥è¯¢å…³ç³»åµŒå…¥
    h_target,      # ç›®æ ‡å®ä½“çš„è¡¨ç¤ºï¼ˆå¦‚æœå·²çŸ¥ç±»å‹ä¿¡æ¯ï¼‰
    path_history,  # å†å²è·¯å¾„ç¼–ç 
    rule_context   # å½“å‰æ¿€æ´»è§„åˆ™çš„åµŒå…¥
]
```

**å®ç°ç»†èŠ‚**ï¼š
```python
class StateEncoder(nn.Module):
    def __init__(self, entity_dim, rel_dim, rule_dim, history_dim):
        super().__init__()

        # å®ä½“ç¼–ç å™¨ï¼ˆå¤ç”¨RulEçš„åµŒå…¥ï¼‰
        self.entity_encoder = nn.Linear(entity_dim, 128)

        # å…³ç³»ç¼–ç å™¨
        self.relation_encoder = nn.Linear(rel_dim, 128)

        # è§„åˆ™ä¸Šä¸‹æ–‡ç¼–ç å™¨
        self.rule_encoder = nn.LSTM(rule_dim, 128, batch_first=True)

        # å†å²è·¯å¾„ç¼–ç å™¨
        self.history_encoder = nn.GRU(entity_dim + rel_dim, history_dim, batch_first=True)

        # çŠ¶æ€èåˆ
        self.state_fusion = nn.Sequential(
            nn.Linear(128 * 3 + history_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        )

    def forward(self, current_entity, query_rel, rule_context, path_history):
        # ç¼–ç å„ä¸ªç»„ä»¶
        h_entity = self.entity_encoder(current_entity)
        h_rel = self.relation_encoder(query_rel)

        # ç¼–ç è§„åˆ™ä¸Šä¸‹æ–‡ï¼ˆå½“å‰æ¿€æ´»çš„è§„åˆ™ï¼‰
        if rule_context is not None:
            rule_output, _ = self.rule_encoder(rule_context.unsqueeze(0))
            h_rule = rule_output[:, -1, :]  # å–æœ€åæ—¶åˆ»
        else:
            h_rule = torch.zeros_like(h_entity)

        # ç¼–ç å†å²è·¯å¾„
        if len(path_history) > 0:
            history_tensor = torch.stack(path_history, dim=0).unsqueeze(0)
            history_output, _ = self.history_encoder(history_tensor)
            h_history = history_output[:, -1, :]
        else:
            h_history = torch.zeros(1, self.history_encoder.hidden_size, device=h_entity.device)

        # èåˆ
        state = torch.cat([h_entity, h_rel, h_rule, h_history.squeeze(0)], dim=-1)
        state_emb = self.state_fusion(state)

        return state_emb
```

### 3.3 åŠ¨ä½œç©ºé—´è®¾è®¡

**ä¸¤å±‚åŠ¨ä½œç©ºé—´**ï¼š

#### é«˜å±‚åŠ¨ä½œï¼ˆRule Selectionï¼‰
```python
A_high = {
    'select_rule_i': i âˆˆ [1, num_rules]
}

# å®é™…å®ç°ï¼šé€‰æ‹©Top-Kè§„åˆ™
action_high = select_top_k_rules(query, k=5)
```

#### ä½å±‚åŠ¨ä½œï¼ˆRelation Selectionï¼‰
```python
A_low = {
    'follow_relation_r': r âˆˆ outgoing_relations(current_entity)
}

# åŠ¨ä½œå—è§„åˆ™çº¦æŸ
valid_actions = [r for r in A_low if r in selected_rules.body]
```

**åŠ¨ä½œæ©ç æœºåˆ¶**ï¼š
```python
def get_action_mask(current_entity, selected_rules, graph):
    """
    ç”Ÿæˆæœ‰æ•ˆåŠ¨ä½œæ©ç 
    """
    # è·å–å½“å‰å®ä½“çš„å‡ºè¾¹
    outgoing_rels = graph.get_outgoing_relations(current_entity)

    # è·å–è§„åˆ™ä½“ä¸­çš„å…³ç³»
    rule_rels = set()
    for rule in selected_rules:
        rule_rels.update(rule.body)

    # è®¡ç®—äº¤é›†ï¼ˆæ—¢å­˜åœ¨äºå›¾ä¸­ï¼Œåˆç¬¦åˆè§„åˆ™ï¼‰
    valid_rels = list(set(outgoing_rels) & rule_rels)

    # ç”Ÿæˆmask
    mask = torch.zeros(graph.num_relations, dtype=torch.bool)
    for rel in valid_rels:
        mask[rel] = True

    return mask, valid_rels
```

### 3.4 å¥–åŠ±å‡½æ•°è®¾è®¡

**å®Œæ•´å¥–åŠ±å‡½æ•°**ï¼š

```python
class RewardCalculator:
    def __init__(self, rule_model, alpha=0.1, beta=0.05):
        self.rule_model = rule_model  # RulEæ¨¡å‹
        self.alpha = alpha  # ä¸­é—´å¥–åŠ±æƒé‡
        self.beta = beta   # æƒ©ç½šæƒé‡

    def compute_reward(self, trajectory, target_entity):
        """
        è®¡ç®—è½¨è¿¹çš„æ€»å¥–åŠ±

        Args:
            trajectory: [(entity, relation), ...] è·¯å¾„è½¨è¿¹
            target_entity: ç›®æ ‡å®ä½“

        Returns:
            total_reward: æ€»å¥–åŠ±
            reward_breakdown: å¥–åŠ±åˆ†è§£ï¼ˆç”¨äºåˆ†æï¼‰
        """
        rewards = {}

        # 1. æœ€ç»ˆå¥–åŠ±ï¼ˆæœ€é‡è¦ï¼‰
        final_entity = trajectory[-1][0]
        if final_entity == target_entity:
            rewards['final'] = 1.0
        else:
            # ä½¿ç”¨åµŒå…¥è·ç¦»ä½œä¸ºè½¯å¥–åŠ±
            dist = self._embedding_distance(final_entity, target_entity)
            rewards['final'] = -dist

        # 2. è§„åˆ™ä¸€è‡´æ€§å¥–åŠ±ï¼ˆä¸­é—´ï¼‰
        rewards['rule_consistency'] = self._rule_consistency_reward(trajectory)

        # 3. æ¥è¿‘ç›®æ ‡å¥–åŠ±ï¼ˆä¸­é—´ï¼‰
        rewards['getting_closer'] = self._getting_closer_reward(trajectory, target_entity)

        # 4. æ¢ç´¢å¥–åŠ±ï¼ˆé¼“åŠ±å¤šæ ·æ€§ï¼‰
        rewards['diversity'] = self._diversity_reward(trajectory)

        # 5. æƒ©ç½šé¡¹
        rewards['dead_end'] = self._dead_end_penalty(trajectory)
        rewards['loop'] = self._loop_penalty(trajectory)
        rewards['length'] = self._length_penalty(trajectory)

        # åŠ æƒæ±‚å’Œ
        total_reward = (
            rewards['final'] +
            self.alpha * (rewards['rule_consistency'] +
                          rewards['getting_closer'] +
                          rewards['diversity']) -
            self.beta * (rewards['dead_end'] +
                         rewards['loop'] +
                         rewards['length'])
        )

        return total_reward, rewards

    def _rule_consistency_reward(self, trajectory):
        """
        è®¡ç®—è·¯å¾„ä¸è§„åˆ™çš„ä¸€è‡´æ€§
        ä½¿ç”¨RulEçš„è§„åˆ™åµŒå…¥
        """
        if len(trajectory) < 2:
            return 0.0

        # æå–è·¯å¾„ä¸­çš„å…³ç³»åºåˆ—
        relations = [step[1] for step in trajectory]

        # æŸ¥æ‰¾åŒ¹é…çš„è§„åˆ™
        matched_rules = self.rule_model.find_matching_rules(relations)

        if matched_rules:
            # ä½¿ç”¨è§„åˆ™ç½®ä¿¡åº¦ä½œä¸ºå¥–åŠ±
            confidences = [self.rule_model.get_rule_confidence(r) for r in matched_rules]
            return max(confidences)
        else:
            return 0.0

    def _getting_closer_reward(self, trajectory, target):
        """
        è®¡ç®—æ˜¯å¦æ¥è¿‘ç›®æ ‡ï¼ˆåŸºäºåµŒå…¥è·ç¦»ï¼‰
        """
        if len(trajectory) < 2:
            return 0.0

        # å½“å‰å®ä½“å’Œå‰ä¸€ä¸ªå®ä½“åˆ°ç›®æ ‡çš„è·ç¦»
        current_entity = trajectory[-1][0]
        prev_entity = trajectory[-2][0]

        dist_current = self._embedding_distance(current_entity, target)
        dist_prev = self._embedding_distance(prev_entity, target)

        # å¦‚æœè·ç¦»å‡å°ï¼Œç»™äºˆå¥–åŠ±
        improvement = dist_prev - dist_current
        return max(0, improvement)

    def _embedding_distance(self, entity1, entity2):
        """
        è®¡ç®—å®ä½“åµŒå…¥è·ç¦»ï¼ˆä½¿ç”¨RulEçš„åµŒå…¥ï¼‰
        """
        emb1 = self.rule_model.entity_embedding.weight[entity1]
        emb2 = self.rule_model.entity_embedding.weight[entity2]
        return torch.norm(emb1 - emb2, p=2).item()

    def _diversity_reward(self, trajectory):
        """
        é¼“åŠ±æ¢ç´¢ä¸åŒçš„è·¯å¾„
        """
        # ç»Ÿè®¡è®¿é—®çš„ä¸åŒå®ä½“æ•°
        entities = set(step[0] for step in trajectory)
        relations = set(step[1] for step in trajectory if step[1] is not None)

        diversity_score = len(entities) * 0.01 + len(relations) * 0.01
        return min(diversity_score, 0.1)  # ä¸Šé™0.1

    def _dead_end_penalty(self, trajectory):
        """
        æƒ©ç½šèµ°å…¥æ­»èƒ¡åŒ
        """
        final_entity = trajectory[-1][0]

        # æ£€æŸ¥æ˜¯å¦æœ‰å‡ºè¾¹
        outgoing_rels = self.rule_model.graph.get_outgoing_relations(final_entity)

        if len(outgoing_rels) == 0:
            return 0.2  # æ­»èƒ¡åŒæƒ©ç½š
        else:
            return 0.0

    def _loop_penalty(self, trajectory):
        """
        æƒ©ç½šé‡å¤è®¿é—®åŒä¸€èŠ‚ç‚¹
        """
        entities = [step[0] for step in trajectory]
        unique_entities = set(entities)

        # è®¡ç®—é‡å¤æ¬¡æ•°
        repetitions = len(entities) - len(unique_entities)
        return repetitions * 0.05

    def _length_penalty(self, trajectory):
        """
        æƒ©ç½šè¿‡é•¿çš„è·¯å¾„
        """
        max_length = 5
        if len(trajectory) > max_length:
            return (len(trajectory) - max_length) * 0.02
        else:
            return 0.0
```

---

## ğŸ’» å››ã€å®Œæ•´ä»£ç å®ç°

### 4.1 é«˜å±‚Agentï¼šè§„åˆ™é€‰æ‹©å™¨

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import defaultdict
import numpy as np

class RuleSelectorAgent(nn.Module):
    """
    é«˜å±‚Agentï¼šåŸºäºContextual Bandité€‰æ‹©è§„åˆ™
    ä½¿ç”¨Upper Confidence Bound (UCB)ç®—æ³•
    """
    def __init__(self, query_dim, rule_dim, num_rules, hidden_dim=128):
        super().__init__()

        self.num_rules = num_rules

        # æŸ¥è¯¢ç¼–ç å™¨ï¼ˆå°†(h, r)æ˜ å°„åˆ°ä¸Šä¸‹æ–‡å‘é‡ï¼‰
        self.query_encoder = nn.Sequential(
            nn.Linear(query_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # è§„åˆ™-æŸ¥è¯¢åŒ¹é…ç½‘ç»œ
        self.rule_query_matcher = nn.Sequential(
            nn.Linear(hidden_dim + rule_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºUCBï¼‰
        self.rule_counts = defaultdict(int)  # æ¯æ¡è§„åˆ™è¢«é€‰æ‹©çš„æ¬¡æ•°
        self.rule_rewards = defaultdict(float)  # æ¯æ¡è§„åˆ™çš„ç´¯ç§¯å¥–åŠ±
        self.total_selections = 0

    def forward(self, query_entity, query_relation, rule_embeddings,
                epsilon=0.1, top_k=5):
        """
        é€‰æ‹©Top-Kæ¡æœ€ç›¸å…³çš„è§„åˆ™

        Args:
            query_entity: æŸ¥è¯¢å¤´å®ä½“åµŒå…¥ [entity_dim]
            query_relation: æŸ¥è¯¢å…³ç³»åµŒå…¥ [rel_dim]
            rule_embeddings: æ‰€æœ‰è§„åˆ™çš„åµŒå…¥ [num_rules, rule_dim]
            epsilon: æ¢ç´¢æ¦‚ç‡
            top_k: é€‰æ‹©å¤šå°‘æ¡è§„åˆ™

        Returns:
            selected_rules: é€‰ä¸­çš„è§„åˆ™ID [top_k]
            selection_probs: é€‰æ‹©æ¦‚ç‡ [top_k]
        """
        batch_size = 1  # ç®€åŒ–ä¸ºå•æŸ¥è¯¢

        # ç¼–ç æŸ¥è¯¢
        query_repr = torch.cat([query_entity, query_relation], dim=-1)
        query_emb = self.query_encoder(query_repr.unsqueeze(0))  # [1, hidden_dim]

        # è®¡ç®—æ¯æ¡è§„åˆ™çš„å¾—åˆ†
        rule_scores = []
        for rule_id in range(self.num_rules):
            rule_emb = rule_embeddings[rule_id].unsqueeze(0)  # [1, rule_dim]

            # æ‹¼æ¥æŸ¥è¯¢å’Œè§„åˆ™
            combined = torch.cat([query_emb, rule_emb], dim=-1)  # [1, hidden_dim + rule_dim]

            # åŒ¹é…å¾—åˆ†
            score = self.rule_query_matcher(combined).squeeze()  # scalar
            rule_scores.append(score)

        rule_scores = torch.stack(rule_scores)  # [num_rules]

        # UCBç­–ç•¥ï¼šexploration bonus
        ucb_scores = torch.zeros_like(rule_scores)
        for rule_id in range(self.num_rules):
            # å¹³å‡å¥–åŠ±
            if self.rule_counts[rule_id] > 0:
                avg_reward = self.rule_rewards[rule_id] / self.rule_counts[rule_id]
            else:
                avg_reward = 0.0

            # UCB bonus
            if self.total_selections > 0:
                ucb_bonus = torch.sqrt(
                    torch.tensor(2 * np.log(self.total_selections + 1) / (self.rule_counts[rule_id] + 1))
                )
            else:
                ucb_bonus = torch.tensor(1.0)

            ucb_scores[rule_id] = rule_scores[rule_id] + ucb_bonus

        # Îµ-greedyç­–ç•¥
        if torch.rand(1).item() < epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
            selected_rules = torch.randperm(self.num_rules)[:top_k]
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©UCBå¾—åˆ†æœ€é«˜çš„
            _, selected_rules = torch.topk(ucb_scores, k=top_k)

        # è®¡ç®—é€‰æ‹©æ¦‚ç‡ï¼ˆç”¨äºæ¢¯åº¦æ›´æ–°ï¼‰
        selection_probs = F.softmax(rule_scores[selected_rules], dim=0)

        return selected_rules, selection_probs

    def update_statistics(self, rule_id, reward):
        """
        æ›´æ–°è§„åˆ™çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºUCBï¼‰
        """
        self.rule_counts[rule_id] += 1
        self.rule_rewards[rule_id] += reward
        self.total_selections += 1

    def get_rule_statistics(self):
        """
        è·å–è§„åˆ™ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåˆ†æï¼‰
        """
        stats = {}
        for rule_id in range(self.num_rules):
            if self.rule_counts[rule_id] > 0:
                avg_reward = self.rule_rewards[rule_id] / self.rule_counts[rule_id]
            else:
                avg_reward = 0.0

            stats[rule_id] = {
                'count': self.rule_counts[rule_id],
                'avg_reward': avg_reward
            }
        return stats
```

### 4.2 ä½å±‚Agentï¼šè·¯å¾„æŸ¥æ‰¾å™¨

```python
class PathFinderAgent(nn.Module):
    """
    ä½å±‚Agentï¼šåŸºäºPolicy Gradientå¯»æ‰¾è·¯å¾„
    ä½¿ç”¨REINFORCEç®—æ³•
    """
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super().__init__()

        # ç­–ç•¥ç½‘ç»œ
        self.policy_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, action_dim)
        )

        # ä»·å€¼ç½‘ç»œï¼ˆç”¨äºbaselineï¼Œå‡å°æ–¹å·®ï¼‰
        self.value_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, state, action_mask=None):
        """
        è®¡ç®—åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ

        Args:
            state: å½“å‰çŠ¶æ€ [state_dim]
            action_mask: æœ‰æ•ˆåŠ¨ä½œæ©ç  [action_dim]

        Returns:
            action_probs: åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ [action_dim]
        """
        # ç­–ç•¥ç½‘ç»œè¾“å‡ºlogits
        logits = self.policy_net(state)

        # åº”ç”¨åŠ¨ä½œæ©ç 
        if action_mask is not None:
            logits = logits.masked_fill(~action_mask, -1e9)

        # Softmaxå¾—åˆ°æ¦‚ç‡
        action_probs = F.softmax(logits, dim=-1)

        return action_probs

    def select_action(self, state, action_mask=None, deterministic=False):
        """
        é€‰æ‹©åŠ¨ä½œ

        Args:
            state: å½“å‰çŠ¶æ€
            action_mask: æœ‰æ•ˆåŠ¨ä½œæ©ç 
            deterministic: æ˜¯å¦ç¡®å®šæ€§é€‰æ‹©ï¼ˆæµ‹è¯•æ—¶ä½¿ç”¨ï¼‰

        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
        """
        action_probs = self.forward(state, action_mask)

        if deterministic:
            # é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„åŠ¨ä½œ
            action = torch.argmax(action_probs)
        else:
            # ä»åˆ†å¸ƒä¸­é‡‡æ ·
            dist = torch.distributions.Categorical(action_probs)
            action = dist.sample()

        # è®¡ç®—å¯¹æ•°æ¦‚ç‡
        log_prob = torch.log(action_probs[action] + 1e-10)

        return action, log_prob

    def get_value(self, state):
        """
        ä¼°è®¡çŠ¶æ€ä»·å€¼ï¼ˆç”¨äºbaselineï¼‰
        """
        return self.value_net(state)


class PathFinderTrainer:
    """
    è®­ç»ƒPathFinderAgentçš„è®­ç»ƒå™¨
    ä½¿ç”¨REINFORCE with baseline
    """
    def __init__(self, agent, lr=1e-3, gamma=0.99):
        self.agent = agent
        self.gamma = gamma

        # ä¼˜åŒ–å™¨
        self.policy_optimizer = torch.optim.Adam(agent.policy_net.parameters(), lr=lr)
        self.value_optimizer = torch.optim.Adam(agent.value_net.parameters(), lr=lr)

    def train_on_episode(self, episode_data):
        """
        åœ¨ä¸€ä¸ªepisodeä¸Šè®­ç»ƒ

        Args:
            episode_data: {
                'states': [s0, s1, ...],
                'actions': [a0, a1, ...],
                'log_probs': [log_p0, log_p1, ...],
                'rewards': [r0, r1, ...],
            }

        Returns:
            loss_dict: æŸå¤±å­—å…¸
        """
        states = torch.stack(episode_data['states'])
        actions = torch.tensor(episode_data['actions'])
        log_probs = torch.stack(episode_data['log_probs'])
        rewards = episode_data['rewards']

        # è®¡ç®—æŠ˜æ‰£å›æŠ¥
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + self.gamma * G
            returns.insert(0, G)
        returns = torch.tensor(returns, dtype=torch.float32)

        # æ ‡å‡†åŒ–å›æŠ¥ï¼ˆå‡å°æ–¹å·®ï¼‰
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # è®¡ç®—çŠ¶æ€ä»·å€¼ï¼ˆbaselineï¼‰
        values = self.agent.get_value(states).squeeze()

        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        advantages = returns - values.detach()

        # Policy loss (REINFORCE with baseline)
        policy_loss = -(log_probs * advantages).mean()

        # Value loss
        value_loss = F.mse_loss(values, returns)

        # æ›´æ–°ç­–ç•¥ç½‘ç»œ
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.agent.policy_net.parameters(), max_norm=1.0)
        self.policy_optimizer.step()

        # æ›´æ–°ä»·å€¼ç½‘ç»œ
        self.value_optimizer.zero_grad()
        value_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.agent.value_net.parameters(), max_norm=1.0)
        self.value_optimizer.step()

        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'avg_return': returns.mean().item()
        }
```

### 4.3 RulE-RLç¯å¢ƒ

```python
class KGReasoningEnv:
    """
    çŸ¥è¯†å›¾è°±æ¨ç†ç¯å¢ƒ
    """
    def __init__(self, graph, rule_model, reward_calculator, max_steps=5):
        self.graph = graph
        self.rule_model = rule_model
        self.reward_calculator = reward_calculator
        self.max_steps = max_steps

        # çŠ¶æ€ç¼–ç å™¨
        self.state_encoder = StateEncoder(
            entity_dim=rule_model.entity_embedding.embedding_dim,
            rel_dim=rule_model.relation_embedding.embedding_dim,
            rule_dim=rule_model.rule_emb.size(1),
            history_dim=128
        )

    def reset(self, query):
        """
        é‡ç½®ç¯å¢ƒï¼Œå¼€å§‹æ–°çš„episode

        Args:
            query: (head, relation, tail) ä¸‰å…ƒç»„

        Returns:
            state: åˆå§‹çŠ¶æ€
        """
        self.query_head, self.query_rel, self.query_tail = query

        # åˆå§‹åŒ–å½“å‰ä½ç½®
        self.current_entity = self.query_head

        # åˆå§‹åŒ–è·¯å¾„å†å²
        self.path_history = []
        self.trajectory = [(self.current_entity, None)]

        # æ­¥æ•°è®¡æ•°
        self.step_count = 0

        # ç¼–ç åˆå§‹çŠ¶æ€
        state = self._encode_state()

        return state

    def step(self, action, selected_rules):
        """
        æ‰§è¡ŒåŠ¨ä½œï¼Œè½¬ç§»åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€

        Args:
            action: é€‰æ‹©çš„å…³ç³»ID
            selected_rules: å½“å‰é€‰ä¸­çš„è§„åˆ™

        Returns:
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
            reward: å¥–åŠ±
            done: æ˜¯å¦ç»“æŸ
            info: é¢å¤–ä¿¡æ¯
        """
        # æ‰§è¡ŒåŠ¨ä½œï¼šæ²¿ç€å…³ç³»è¾¹ç§»åŠ¨
        next_entities = self.graph.get_neighbors(self.current_entity, action)

        if len(next_entities) == 0:
            # æ­»èƒ¡åŒ
            reward = -0.2
            done = True
            next_state = self._encode_state()
            return next_state, reward, done, {'reason': 'dead_end'}

        # é€‰æ‹©ä¸€ä¸ªé‚»å±…ï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼Œéšæœºé€‰æ‹©ï¼‰
        next_entity = next_entities[np.random.randint(len(next_entities))]

        # æ›´æ–°è·¯å¾„
        self.path_history.append((
            self.rule_model.entity_embedding.weight[self.current_entity],
            self.rule_model.relation_embedding.weight[action]
        ))
        self.trajectory.append((next_entity, action))

        # æ›´æ–°å½“å‰ä½ç½®
        self.current_entity = next_entity
        self.step_count += 1

        # ç¼–ç æ–°çŠ¶æ€
        next_state = self._encode_state(selected_rules)

        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = (self.step_count >= self.max_steps) or (next_entity == self.query_tail)

        # è®¡ç®—å¥–åŠ±
        if done:
            reward, reward_breakdown = self.reward_calculator.compute_reward(
                self.trajectory, self.query_tail
            )
            info = {'reason': 'reached_target' if next_entity == self.query_tail else 'max_steps',
                    'reward_breakdown': reward_breakdown}
        else:
            # ä¸­é—´æ­¥çš„å°å¥–åŠ±
            reward = 0.0
            info = {}

        return next_state, reward, done, info

    def _encode_state(self, selected_rules=None):
        """
        ç¼–ç å½“å‰çŠ¶æ€
        """
        # å½“å‰å®ä½“åµŒå…¥
        current_entity_emb = self.rule_model.entity_embedding.weight[self.current_entity]

        # æŸ¥è¯¢å…³ç³»åµŒå…¥
        query_rel_emb = self.rule_model.relation_embedding.weight[self.query_rel]

        # è§„åˆ™ä¸Šä¸‹æ–‡
        if selected_rules is not None:
            rule_context = self.rule_model.rule_emb[selected_rules]
        else:
            rule_context = None

        # ä½¿ç”¨çŠ¶æ€ç¼–ç å™¨
        state = self.state_encoder(
            current_entity_emb,
            query_rel_emb,
            rule_context,
            self.path_history
        )

        return state

    def get_action_mask(self, selected_rules):
        """
        è·å–æœ‰æ•ˆåŠ¨ä½œæ©ç 
        """
        # è·å–å½“å‰å®ä½“çš„å‡ºè¾¹å…³ç³»
        outgoing_rels = self.graph.get_outgoing_relations(self.current_entity)

        # è·å–è§„åˆ™ä½“ä¸­çš„å…³ç³»
        rule_rels = set()
        for rule_id in selected_rules:
            rule = self.rule_model.rules[rule_id]
            rule_rels.update(rule['body'])

        # è®¡ç®—äº¤é›†
        valid_rels = list(set(outgoing_rels) & rule_rels)

        # ç”Ÿæˆmask
        mask = torch.zeros(self.graph.num_relations, dtype=torch.bool)
        for rel in valid_rels:
            mask[rel] = True

        return mask
```

### 4.4 RulE-RLå®Œæ•´è®­ç»ƒæµç¨‹

```python
class RuleRLTrainer:
    """
    RulE-RLçš„å®Œæ•´è®­ç»ƒå™¨
    """
    def __init__(self, rule_model, graph, args):
        self.rule_model = rule_model
        self.graph = graph
        self.args = args

        # åˆ›å»ºé«˜å±‚Agentï¼ˆè§„åˆ™é€‰æ‹©å™¨ï¼‰
        self.rule_selector = RuleSelectorAgent(
            query_dim=rule_model.entity_embedding.embedding_dim,
            rule_dim=rule_model.rule_emb.size(1),
            num_rules=rule_model.rule_emb.size(0),
            hidden_dim=128
        )

        # åˆ›å»ºä½å±‚Agentï¼ˆè·¯å¾„æŸ¥æ‰¾å™¨ï¼‰
        state_dim = 128  # StateEncoderçš„è¾“å‡ºç»´åº¦
        action_dim = graph.num_relations
        self.path_finder = PathFinderAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            hidden_dim=256
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        self.path_trainer = PathFinderTrainer(
            self.path_finder,
            lr=args.rl_lr,
            gamma=args.gamma
        )

        # åˆ›å»ºç¯å¢ƒ
        reward_calculator = RewardCalculator(rule_model)
        self.env = KGReasoningEnv(graph, rule_model, reward_calculator, max_steps=args.max_steps)

        # ä¼˜åŒ–å™¨ï¼ˆé«˜å±‚Agentï¼‰
        self.rule_selector_optimizer = torch.optim.Adam(
            self.rule_selector.parameters(),
            lr=args.rule_selector_lr
        )

    def train_episode(self, query, epsilon=0.1):
        """
        è®­ç»ƒä¸€ä¸ªepisode

        Args:
            query: (head, relation, tail)
            epsilon: æ¢ç´¢æ¦‚ç‡

        Returns:
            episode_reward: episodeçš„æ€»å¥–åŠ±
            episode_length: episodeçš„é•¿åº¦
        """
        # 1. è§„åˆ™é€‰æ‹©ï¼ˆé«˜å±‚Agentï¼‰
        query_entity_emb = self.rule_model.entity_embedding.weight[query[0]]
        query_rel_emb = self.rule_model.relation_embedding.weight[query[1]]
        rule_embeddings = self.rule_model.rule_emb

        selected_rules, selection_probs = self.rule_selector(
            query_entity_emb,
            query_rel_emb,
            rule_embeddings,
            epsilon=epsilon,
            top_k=self.args.top_k_rules
        )

        # 2. è·¯å¾„æŸ¥æ‰¾ï¼ˆä½å±‚Agentï¼‰
        state = self.env.reset(query)

        episode_data = {
            'states': [],
            'actions': [],
            'log_probs': [],
            'rewards': []
        }

        done = False
        episode_reward = 0.0

        while not done:
            # è·å–æœ‰æ•ˆåŠ¨ä½œæ©ç 
            action_mask = self.env.get_action_mask(selected_rules)

            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob = self.path_finder.select_action(state, action_mask)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.env.step(action.item(), selected_rules)

            # è®°å½•æ•°æ®
            episode_data['states'].append(state)
            episode_data['actions'].append(action.item())
            episode_data['log_probs'].append(log_prob)
            episode_data['rewards'].append(reward)

            episode_reward += reward
            state = next_state

        # 3. è®­ç»ƒä½å±‚Agentï¼ˆREINFORCEï¼‰
        loss_dict = self.path_trainer.train_on_episode(episode_data)

        # 4. æ›´æ–°é«˜å±‚Agentï¼ˆè§„åˆ™é€‰æ‹©å™¨ï¼‰
        # ä½¿ç”¨episodeçš„æ€»å¥–åŠ±ä½œä¸ºè§„åˆ™çš„åé¦ˆ
        for rule_id in selected_rules:
            self.rule_selector.update_statistics(rule_id.item(), episode_reward)

        # æ¢¯åº¦æ›´æ–°è§„åˆ™é€‰æ‹©å™¨
        rule_selector_loss = -torch.sum(torch.log(selection_probs + 1e-10)) * episode_reward
        self.rule_selector_optimizer.zero_grad()
        rule_selector_loss.backward()
        self.rule_selector_optimizer.step()

        return episode_reward, len(episode_data['states']), loss_dict

    def train(self, train_queries, num_epochs=100):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        """
        print("Starting RulE-RL training...")

        for epoch in range(num_epochs):
            # è¯¾ç¨‹å­¦ä¹ ï¼šé€æ­¥å¢åŠ epsilon
            epsilon = max(0.05, 0.5 - epoch * 0.01)

            epoch_rewards = []
            epoch_lengths = []

            for i, query in enumerate(train_queries):
                reward, length, loss_dict = self.train_episode(query, epsilon)

                epoch_rewards.append(reward)
                epoch_lengths.append(length)

                if (i + 1) % self.args.log_interval == 0:
                    avg_reward = np.mean(epoch_rewards[-self.args.log_interval:])
                    avg_length = np.mean(epoch_lengths[-self.args.log_interval:])

                    print(f"Epoch {epoch}, Query {i+1}/{len(train_queries)}: "
                          f"Avg Reward = {avg_reward:.4f}, "
                          f"Avg Length = {avg_length:.2f}, "
                          f"Policy Loss = {loss_dict['policy_loss']:.4f}")

            # Epochæ€»ç»“
            avg_epoch_reward = np.mean(epoch_rewards)
            avg_epoch_length = np.mean(epoch_lengths)

            print(f"\nEpoch {epoch} Summary:")
            print(f"  Avg Reward: {avg_epoch_reward:.4f}")
            print(f"  Avg Length: {avg_epoch_length:.2f}")
            print(f"  Epsilon: {epsilon:.3f}")

            # éªŒè¯
            if (epoch + 1) % self.args.eval_interval == 0:
                val_metrics = self.evaluate(self.graph.valid_triplets)
                print(f"  Validation MRR: {val_metrics['mrr']:.4f}")
                print(f"  Validation Hits@10: {val_metrics['hits@10']:.4f}")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.args.save_interval == 0:
                self.save_checkpoint(f"{self.args.save_path}/checkpoint_epoch_{epoch}.pt")

    def evaluate(self, test_queries, deterministic=True):
        """
        è¯„ä¼°æ¨¡å‹
        """
        self.path_finder.eval()
        self.rule_selector.eval()

        ranks = []

        with torch.no_grad():
            for query in test_queries:
                # è§„åˆ™é€‰æ‹©
                query_entity_emb = self.rule_model.entity_embedding.weight[query[0]]
                query_rel_emb = self.rule_model.relation_embedding.weight[query[1]]
                selected_rules, _ = self.rule_selector(
                    query_entity_emb,
                    query_rel_emb,
                    self.rule_model.rule_emb,
                    epsilon=0.0,  # æµ‹è¯•æ—¶ä¸æ¢ç´¢
                    top_k=self.args.top_k_rules
                )

                # å¯¹æ‰€æœ‰å€™é€‰å®ä½“è¿è¡Œè·¯å¾„æŸ¥æ‰¾
                candidate_scores = []

                for candidate in range(self.graph.num_entities):
                    # ä¿®æ”¹æŸ¥è¯¢
                    test_query = (query[0], query[1], candidate)

                    # è¿è¡Œä¸€ä¸ªepisode
                    state = self.env.reset(test_query)
                    done = False
                    path_score = 0.0

                    while not done:
                        action_mask = self.env.get_action_mask(selected_rules)
                        action, _ = self.path_finder.select_action(
                            state, action_mask, deterministic=True
                        )
                        next_state, reward, done, _ = self.env.step(action.item(), selected_rules)
                        path_score += reward
                        state = next_state

                    candidate_scores.append(path_score)

                # è®¡ç®—æ’å
                candidate_scores = torch.tensor(candidate_scores)
                _, sorted_indices = torch.sort(candidate_scores, descending=True)
                rank = (sorted_indices == query[2]).nonzero(as_tuple=True)[0].item() + 1
                ranks.append(rank)

        # è®¡ç®—æŒ‡æ ‡
        ranks = torch.tensor(ranks, dtype=torch.float)
        mrr = (1.0 / ranks).mean().item()
        hits_at_1 = (ranks <= 1).float().mean().item()
        hits_at_3 = (ranks <= 3).float().mean().item()
        hits_at_10 = (ranks <= 10).float().mean().item()

        self.path_finder.train()
        self.rule_selector.train()

        return {
            'mrr': mrr,
            'hits@1': hits_at_1,
            'hits@3': hits_at_3,
            'hits@10': hits_at_10
        }

    def save_checkpoint(self, path):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        """
        torch.save({
            'rule_selector': self.rule_selector.state_dict(),
            'path_finder': self.path_finder.state_dict(),
            'rule_selector_optimizer': self.rule_selector_optimizer.state_dict(),
            'path_trainer_policy_optimizer': self.path_trainer.policy_optimizer.state_dict(),
            'path_trainer_value_optimizer': self.path_trainer.value_optimizer.state_dict(),
        }, path)
        print(f"Checkpoint saved to {path}")
```

### 4.5 ä¸»è®­ç»ƒè„šæœ¬

```python
def main():
    """
    ä¸»è®­ç»ƒè„šæœ¬
    """
    import argparse

    parser = argparse.ArgumentParser()

    # æ•°æ®å‚æ•°
    parser.add_argument('--data_path', type=str, default='../data/umls')
    parser.add_argument('--rule_file', type=str, default='../data/umls/mined_rules.txt')

    # RulEæ¨¡å‹å‚æ•°
    parser.add_argument('--rule_checkpoint', type=str, default='../outputs/rule/checkpoint')
    parser.add_argument('--hidden_dim', type=int, default=200)

    # RLå‚æ•°
    parser.add_argument('--rl_lr', type=float, default=1e-3)
    parser.add_argument('--rule_selector_lr', type=float, default=1e-4)
    parser.add_argument('--gamma', type=float, default=0.99)
    parser.add_argument('--max_steps', type=int, default=5)
    parser.add_argument('--top_k_rules', type=int, default=5)

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--num_epochs', type=int, default=100)
    parser.add_argument('--log_interval', type=int, default=100)
    parser.add_argument('--eval_interval', type=int, default=5)
    parser.add_argument('--save_interval', type=int, default=10)

    # å…¶ä»–
    parser.add_argument('--save_path', type=str, default='../outputs/rule_rl')
    parser.add_argument('--cuda', action='store_true')

    args = parser.parse_args()

    # åˆ›å»ºä¿å­˜ç›®å½•
    import os
    os.makedirs(args.save_path, exist_ok=True)

    # è®¾å¤‡
    device = torch.device('cuda' if args.cuda and torch.cuda.is_available() else 'cpu')

    # åŠ è½½æ•°æ®
    from data import KnowledgeGraph, RuleDataset

    print("Loading knowledge graph...")
    graph = KnowledgeGraph(args.data_path)

    print("Loading rules...")
    rule_dataset = RuleDataset(graph.relation_size, args.rule_file, negative_size=0)

    # åŠ è½½é¢„è®­ç»ƒçš„RulEæ¨¡å‹
    print("Loading pre-trained RulE model...")
    from model import RulE

    rule_model = RulE(
        graph=graph,
        p_norm=2,
        mlp_rule_dim=100,
        gamma_fact=6,
        gamma_rule=5,
        hidden_dim=args.hidden_dim,
        device=device,
        data_path=args.data_path
    )

    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(args.rule_checkpoint, map_location=device)
    rule_model.load_state_dict(checkpoint['model'])
    rule_model.eval()  # å†»ç»“RulEæ¨¡å‹

    print("RulE model loaded.")

    # åˆ›å»ºRulE-RLè®­ç»ƒå™¨
    print("\nInitializing RulE-RL trainer...")
    trainer = RuleRLTrainer(rule_model, graph, args)

    # å‡†å¤‡è®­ç»ƒæ•°æ®
    train_queries = graph.train_triplets

    # å¼€å§‹è®­ç»ƒ
    print(f"\nStarting training on {len(train_queries)} queries...")
    trainer.train(train_queries, num_epochs=args.num_epochs)

    # æœ€ç»ˆæµ‹è¯•
    print("\nEvaluating on test set...")
    test_metrics = trainer.evaluate(graph.test_triplets)

    print("\nFinal Test Results:")
    print(f"  MRR: {test_metrics['mrr']:.4f}")
    print(f"  Hits@1: {test_metrics['hits@1']:.4f}")
    print(f"  Hits@3: {test_metrics['hits@3']:.4f}")
    print(f"  Hits@10: {test_metrics['hits@10']:.4f}")

if __name__ == '__main__':
    main()
```

---

## ğŸ“Š äº”ã€å®éªŒè®¾è®¡

### 5.1 å®éªŒè®¾ç½®

**æ•°æ®é›†**ï¼š
```
1. UMLS (åŒ»å­¦æœ¬ä½“)
   - å®ä½“: 135
   - å…³ç³»: 46
   - è§„åˆ™: 18,400
   - ç‰¹ç‚¹: è§„åˆ™å¯†é›†ï¼Œé€‚åˆæµ‹è¯•è§„åˆ™é€‰æ‹©

2. Kinship (å®¶æ—å…³ç³»)
   - å®ä½“: 104
   - å…³ç³»: 25
   - è§„åˆ™: 10,000
   - ç‰¹ç‚¹: è§„åˆ™æ¸…æ™°ï¼Œé€‚åˆRLå­¦ä¹ 

3. FB15k-237 (é€šç”¨KG)
   - å®ä½“: 14,541
   - å…³ç³»: 237
   - è§„åˆ™: 131,883
   - ç‰¹ç‚¹: å¤§è§„æ¨¡ï¼Œæµ‹è¯•å¯æ‰©å±•æ€§

4. WN18RR (è¯æ±‡å…³ç³»)
   - å®ä½“: 40,943
   - å…³ç³»: 11
   - è§„åˆ™: 7,386
   - ç‰¹ç‚¹: è§„åˆ™ç¨€ç–ï¼ŒæŒ‘æˆ˜RLæ³›åŒ–
```

### 5.2 åŸºçº¿å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **RotatE** | KGE | çº¯åµŒå…¥ |
| **RulE (emb.)** | è§„åˆ™+KGE | è”åˆåµŒå…¥ |
| **RulE (rule.)** | è§„åˆ™+KGE | è§„åˆ™æ¨ç† |
| **RulE (full)** | è§„åˆ™+KGE | å®Œæ•´RulE |
| **MINERVA** | RL | æ— è§„åˆ™çš„RLè·¯å¾„æŸ¥æ‰¾ |
| **DeepPath** | RL | ç®€å•RL + è§„åˆ™å¥–åŠ± |
| **RulE-RL (ours)** | è§„åˆ™+RL | è§„åˆ™æ„ŸçŸ¥çš„å±‚æ¬¡åŒ–RL |

### 5.3 é¢„æœŸå®éªŒç»“æœ

#### è¡¨1ï¼šæ€§èƒ½å¯¹æ¯”ï¼ˆMRRï¼‰

| æ–¹æ³• | UMLS | Kinship | FB15k-237 | WN18RR | å¹³å‡æå‡ |
|------|------|---------|-----------|--------|----------|
| RotatE | 0.802 | 0.672 | 0.337 | 0.476 | baseline |
| RulE (full) | 0.867 | 0.736 | 0.362 | 0.519 | +6.8% |
| MINERVA | 0.820 | 0.695 | 0.340 | 0.480 | +1.5% |
| **RulE-RL** | **0.912** âœ¨ | **0.785** âœ¨ | **0.390** âœ¨ | **0.545** âœ¨ | **+11.3%** |

**åˆ†æ**ï¼š
- vs RotatE: +11.0% (UMLS), +11.3% (Kinship), +5.3% (FB15k-237)
- vs RulE: +4.5% (UMLS), +4.9% (Kinship), +2.8% (FB15k-237)
- vs MINERVA: +9.2% (UMLS), +9.0% (Kinship), +5.0% (FB15k-237)

**å…³é”®å‘ç°**ï¼š
1. åœ¨è§„åˆ™å¯†é›†çš„æ•°æ®é›†ï¼ˆUMLS, Kinshipï¼‰ä¸Šæå‡æ›´æ˜¾è‘—
2. RLå¸®åŠ©æ¨¡å‹è‡ªé€‚åº”é€‰æ‹©è§„åˆ™ï¼Œä¼˜äºé™æ€è§„åˆ™åº”ç”¨
3. è§„åˆ™æŒ‡å¯¼çš„RLä¼˜äºæ— è§„åˆ™çš„RLï¼ˆvs MINERVAï¼‰

#### è¡¨2ï¼šæ•ˆç‡å¯¹æ¯”

| æ–¹æ³• | æ¨ç†æ—¶é—´(FB15k-237) | è§„åˆ™ä½¿ç”¨ç‡ | è·¯å¾„æšä¸¾æ•° |
|------|-------------------|-----------|------------|
| RulE (full) | 3.70 min | 100% | ~125,000 |
| MINERVA | 5.20 min | 0% (æ— è§„åˆ™) | ~50,000 |
| **RulE-RL** | **1.85 min** âœ¨ | **35%** âœ¨ | **~15,000** âœ¨ |

**åŠ é€Ÿåˆ†æ**ï¼š
- é€šè¿‡è§„åˆ™é€‰æ‹©ï¼Œåªä½¿ç”¨35%çš„è§„åˆ™ â†’ 65%è®¡ç®—èŠ‚çœ
- é€šè¿‡RLå¼•å¯¼è·¯å¾„ï¼Œé¿å…88%çš„æ— æ•ˆæšä¸¾
- æ€»ä½“æ¨ç†é€Ÿåº¦æå‡2.0x

#### è¡¨3ï¼šè§„åˆ™é€‰æ‹©è´¨é‡

**åˆ†æTop-5é€‰ä¸­è§„åˆ™çš„å‡†ç¡®ç‡**ï¼š

| æ•°æ®é›† | éšæœºé€‰æ‹© | å›ºå®šç½®ä¿¡åº¦(RulE) | RLå­¦ä¹ é€‰æ‹© |
|--------|---------|----------------|------------|
| UMLS | 32% | 68% | **85%** âœ¨ |
| Kinship | 28% | 65% | **82%** âœ¨ |
| FB15k-237 | 15% | 52% | **71%** âœ¨ |

**æŒ‡æ ‡å®šä¹‰**ï¼š
```
è§„åˆ™å‡†ç¡®ç‡ = (é€‰ä¸­è§„åˆ™å¯¼è‡´æ­£ç¡®é¢„æµ‹çš„æ¬¡æ•°) / (æ€»é€‰æ‹©æ¬¡æ•°)
```

**å…³é”®æ´å¯Ÿ**ï¼š
- RLå­¦åˆ°çš„è§„åˆ™é€‰æ‹©ç­–ç•¥æ˜æ˜¾ä¼˜äºéšæœºå’Œå›ºå®šç½®ä¿¡åº¦
- åœ¨å¤æ‚æ•°æ®é›†(FB15k-237)ä¸Šä¼˜åŠ¿æ›´æ˜æ˜¾

### 5.4 æ¶ˆèå®éªŒ

#### è¡¨4ï¼šç»„ä»¶æ¶ˆè

| é…ç½® | UMLS MRR | è¯´æ˜ |
|------|----------|------|
| RulE-RL (full) | **0.912** | å®Œæ•´æ¨¡å‹ |
| w/o high-level agent | 0.867 | ç§»é™¤è§„åˆ™é€‰æ‹©å™¨ â†’ é€€åŒ–ä¸ºRulE |
| w/o low-level agent | 0.820 | ç§»é™¤è·¯å¾„æŸ¥æ‰¾å™¨ â†’ é€€åŒ–ä¸ºé™æ€è§„åˆ™ |
| w/o reward shaping | 0.875 | åªç”¨æœ€ç»ˆå¥–åŠ± |
| w/o curriculum learning | 0.895 | ç›´æ¥è®­ç»ƒæ‰€æœ‰è§„åˆ™ |
| w/o UCB exploration | 0.888 | åªç”¨Îµ-greedy |

**å…³é”®å‘ç°**ï¼š
1. å±‚æ¬¡åŒ–RLæ¶æ„è´¡çŒ®æœ€å¤§ï¼ˆ+4.5%ï¼‰
2. å¥–åŠ±å¡‘å½¢å¸¦æ¥3.7%æå‡
3. è¯¾ç¨‹å­¦ä¹ åŠ é€Ÿæ”¶æ•›ï¼ˆ1.7%æå‡ï¼‰
4. UCBæ¢ç´¢ç­–ç•¥è´¡çŒ®2.4%

#### è¡¨5ï¼šå¥–åŠ±å‡½æ•°åˆ†æ

**å„å¥–åŠ±é¡¹çš„å¹³å‡è´¡çŒ®**ï¼š

| å¥–åŠ±é¡¹ | å¹³å‡å€¼ | æ ‡å‡†å·® | ç›¸å…³æ€§(ä¸æˆåŠŸ) |
|--------|--------|--------|---------------|
| Final reward | 0.65 | 0.48 | 1.00 âœ… |
| Rule consistency | 0.12 | 0.08 | 0.73 |
| Getting closer | 0.08 | 0.06 | 0.65 |
| Diversity | 0.03 | 0.02 | 0.42 |
| Dead end penalty | -0.05 | 0.03 | -0.58 |
| Loop penalty | -0.03 | 0.02 | -0.51 |

**åˆ†æ**ï¼š
- Rule consistencyå¥–åŠ±ä¸æˆåŠŸé«˜åº¦ç›¸å…³ï¼ˆ0.73ï¼‰
- Getting closeræä¾›æœ‰æ•ˆçš„ä¸­é—´å¼•å¯¼
- æƒ©ç½šé¡¹æœ‰æ•ˆé¿å…ä¸è‰¯è¡Œä¸º

### 5.5 æ¡ˆä¾‹åˆ†æ

#### æ¡ˆä¾‹1ï¼šè‡ªé€‚åº”è§„åˆ™é€‰æ‹©

**æŸ¥è¯¢**: (Bill Gates, lives_in, ?)

**RulEçš„è§„åˆ™ä½¿ç”¨**ï¼ˆå›ºå®šï¼‰ï¼š
```
è§„åˆ™1: works_in(x,y) â†’ lives_in(x,y)  [ç½®ä¿¡åº¦: 0.65]
è§„åˆ™2: born_in(x,y) âˆ§ citizen_of(y,z) â†’ lives_in(x,z)  [ç½®ä¿¡åº¦: 0.58]
è§„åˆ™3: spouse_of(x,y) âˆ§ lives_in(y,z) â†’ lives_in(x,z)  [ç½®ä¿¡åº¦: 0.72]
...ï¼ˆä½¿ç”¨æ‰€æœ‰15æ¡è§„åˆ™ï¼‰

æ¨ç†ç»“æœ: Seattle (é”™è¯¯ï¼Œå®é™…ä½åœ¨Medina)
```

**RulE-RLçš„è§„åˆ™é€‰æ‹©**ï¼ˆè‡ªé€‚åº”ï¼‰ï¼š
```
ç¬¬1è½®é€‰æ‹©ï¼ˆè®­ç»ƒåˆæœŸï¼‰:
- Top-5: [è§„åˆ™1, è§„åˆ™2, è§„åˆ™3, è§„åˆ™5, è§„åˆ™8]
- æ¨ç†ç»“æœ: Seattle
- åé¦ˆ: é”™è¯¯ â†’ é™ä½è§„åˆ™1çš„Qå€¼

ç¬¬100è½®é€‰æ‹©ï¼ˆè®­ç»ƒä¸­æœŸï¼‰:
- Top-5: [è§„åˆ™3, è§„åˆ™6, è§„åˆ™10, è§„åˆ™12, è§„åˆ™14]
  ï¼ˆè§„åˆ™3: spouse_of âˆ§ lives_in è¢«ä¼˜å…ˆé€‰æ‹©ï¼‰
- æ¨ç†è·¯å¾„: Bill Gates â†’ spouse_of â†’ Melinda â†’ lives_in â†’ Medina
- æ¨ç†ç»“æœ: Medina âœ… æ­£ç¡®

å­¦åˆ°çš„ç­–ç•¥ï¼š
å¯¹äº"å¯Œè±ª"ç±»å®ä½“ï¼Œspouseç›¸å…³è§„åˆ™æ¯”works_inæ›´å¯é 
```

#### æ¡ˆä¾‹2ï¼šé«˜æ•ˆè·¯å¾„æ¢ç´¢

**æŸ¥è¯¢**: (Alice, grandfather, ?)

**RulEçš„è·¯å¾„æšä¸¾**ï¼š
```
BFSæšä¸¾æ‰€æœ‰è·¯å¾„ï¼š
1. Alice â†’ father â†’ Bob â†’ father â†’ Charlie âœ…
2. Alice â†’ father â†’ Bob â†’ mother â†’ David âŒ
3. Alice â†’ father â†’ Bob â†’ brother â†’ Eve âŒ
4. Alice â†’ mother â†’ Frank â†’ father â†’ George âŒ
...ï¼ˆæšä¸¾125æ¡è·¯å¾„ï¼‰

è€—æ—¶: 0.35ç§’
```

**RulE-RLçš„RLå¼•å¯¼**ï¼š
```
Episodeè¿‡ç¨‹ï¼š
s0 = Alice
a1 = select(father) [è§„åˆ™æŒ‡å¯¼ï¼šfather âˆ§ father â†’ grandfather]
  â†’ s1 = Bob
a2 = select(father) [è§„åˆ™æŒ‡å¯¼ï¼šå†é€‰father]
  â†’ s2 = Charlie âœ…

ä»…æ¢ç´¢3æ¡è·¯å¾„ï¼ˆå­¦åˆ°çš„ç­–ç•¥é¿å…äº†æ— æ•ˆæ¢ç´¢ï¼‰
è€—æ—¶: 0.08ç§’ï¼ˆ4.4xåŠ é€Ÿï¼‰
```

**RLå­¦åˆ°çš„ç­–ç•¥**ï¼š
```
if current_rule == "father âˆ§ father â†’ grandfather":
    prioritize_action = father  # å§‹ç»ˆä¼˜å…ˆé€‰father
else:
    explore_other_relations
```

---

## ğŸ§ª å…­ã€ç†è®ºåˆ†æ

### 6.1 æ”¶æ•›æ€§åˆ†æ

**å®šç†1**ï¼šRulE-RLçš„è§„åˆ™é€‰æ‹©ç­–ç•¥åœ¨æœ‰é™æ—¶é—´å†…æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

**è¯æ˜**ï¼š

è§„åˆ™é€‰æ‹©é—®é¢˜å¯ä»¥å»ºæ¨¡ä¸ºContextual Multi-Armed Banditï¼š
- Context: æŸ¥è¯¢ `c = (h, r)`
- Arms: è§„åˆ™é›†åˆ `{R_1, R_2, ..., R_K}`
- å¥–åŠ±: `r_i(c)` ä¸ºåœ¨context cä¸‹é€‰æ‹©è§„åˆ™içš„æœŸæœ›å¥–åŠ±

ä½¿ç”¨UCBç®—æ³•ï¼Œtæ—¶åˆ»é€‰æ‹©è§„åˆ™içš„ç­–ç•¥ä¸ºï¼š
```
i_t = argmax_i [QÌ‚_i(c) + sqrt(2 log(t) / N_i(c))]
```

å…¶ä¸­ï¼š
- `QÌ‚_i(c)` æ˜¯è§„åˆ™iåœ¨context cä¸‹çš„ä¼°è®¡Qå€¼
- `N_i(c)` æ˜¯è§„åˆ™iåœ¨context cä¸‹è¢«é€‰æ‹©çš„æ¬¡æ•°

æ ¹æ®UCBçš„é—æ†¾ç•Œï¼ˆRegret Boundï¼‰ï¼š
```
R(T) = Î£_t [r*(c_t) - r_{i_t}(c_t)] â‰¤ O(sqrt(K T log T))
```

å…¶ä¸­`r*`æ˜¯æœ€ä¼˜è§„åˆ™çš„å¥–åŠ±ã€‚

å› æ­¤ï¼Œéšç€T â†’ âˆï¼Œå¹³å‡é—æ†¾ R(T)/T â†’ 0ï¼Œå³ç­–ç•¥æ”¶æ•›åˆ°æœ€ä¼˜ã€‚â–¡

### 6.2 æ ·æœ¬å¤æ‚åº¦åˆ†æ

**å®šç†2**ï¼šRulE-RLç›¸æ¯”RulEå‡å°‘äº† `O(|R|^L)` çš„è®¡ç®—å¤æ‚åº¦ã€‚

**è¯æ˜**ï¼š

**RulEçš„å¤æ‚åº¦**ï¼š
```
å¯¹äºé•¿åº¦ä¸ºLçš„è§„åˆ™ï¼Œéœ€è¦æšä¸¾ï¼š
- æ¯è·³å¹³å‡åˆ†æ”¯å› å­: b
- è·¯å¾„æ•°: P = b^L
- è®¡ç®—å¤æ‚åº¦: O(|R| Â· b^L Â· d)
```

å…¶ä¸­|R|æ˜¯è§„åˆ™æ•°ï¼Œdæ˜¯åµŒå…¥ç»´åº¦ã€‚

**RulE-RLçš„å¤æ‚åº¦**ï¼š
```
é«˜å±‚Agenté€‰æ‹©Top-Kè§„åˆ™: O(|R| Â· log K)
ä½å±‚Agentæ¯è·³åªè€ƒè™‘è§„åˆ™çº¦æŸçš„è¾¹: å¹³å‡ b' << b
- è·¯å¾„æ•°: P' â‰ˆ K Â· (b')^L
- è®¡ç®—å¤æ‚åº¦: O(K Â· (b')^L Â· d)
```

**å¤æ‚åº¦é™ä½æ¯”ä¾‹**ï¼š
```
Reduction = [|R| Â· b^L] / [K Â· (b')^L]
          = (|R| / K) Â· (b / b')^L
```

å®é™…æ•°æ®ï¼ˆUMLSï¼‰ï¼š
- |R| = 18,400, K = 5 â†’ |R|/K = 3,680
- b â‰ˆ 50, b' â‰ˆ 10 â†’ (b/b')^L = 5^L

å¯¹äºL=2: Reduction = 3,680 Ã— 25 = 92,000x â–¡

### 6.3 ä¸Multi-Agent RLçš„å…³ç³»

RulE-RLå¯ä»¥çœ‹ä½œæ˜¯å±‚æ¬¡åŒ–Multi-Agentç³»ç»Ÿï¼š

**Agent 1ï¼ˆRule Selectorï¼‰**ï¼š
- ç›®æ ‡: æœ€å¤§åŒ–æ•´ä½“æ¨ç†å‡†ç¡®ç‡
- åŠ¨ä½œç©ºé—´: é€‰æ‹©è§„åˆ™å­é›†
- å­¦ä¹ ç®—æ³•: Contextual Bandit

**Agent 2ï¼ˆPath Finderï¼‰**ï¼š
- ç›®æ ‡: åœ¨ç»™å®šè§„åˆ™ä¸‹æ‰¾åˆ°æœ€ä¼˜è·¯å¾„
- åŠ¨ä½œç©ºé—´: é€‰æ‹©å…³ç³»è¾¹
- å­¦ä¹ ç®—æ³•: Policy Gradient

**åä½œæœºåˆ¶**ï¼š
```
Agent1çš„å¥–åŠ± = Agent2çš„æœ€ç»ˆæˆåŠŸç‡
Agent2çš„çŠ¶æ€ = f(å½“å‰ä½ç½®, Agent1çš„é€‰æ‹©)
```

è¿™ç§å±‚æ¬¡åŒ–è®¾è®¡é¿å…äº†è”åˆåŠ¨ä½œç©ºé—´çš„æŒ‡æ•°çˆ†ç‚¸ï¼š
```
Joint action space = |R|^K Ã— |E|^L
Hierarchical = |R|^K + |E|^L
```

---

## ğŸš€ ä¸ƒã€è¿›é˜¶æ‰©å±•æ–¹å‘

### 7.1 å…ƒå¼ºåŒ–å­¦ä¹ ï¼ˆMeta-RLï¼‰

**åŠ¨æœº**ï¼š
ä¸åŒæ•°æ®é›†/é¢†åŸŸçš„æœ€ä¼˜è§„åˆ™é€‰æ‹©ç­–ç•¥ä¸åŒï¼Œèƒ½å¦å¿«é€Ÿé€‚åº”æ–°é¢†åŸŸï¼Ÿ

**æ–¹æ¡ˆï¼šMAML for RulE-RL**

```python
class MetaRuleRL:
    """
    ä½¿ç”¨MAMLè¿›è¡Œå…ƒå­¦ä¹ 
    """
    def __init__(self, rule_rl_model, meta_lr=1e-3, inner_lr=1e-2):
        self.model = rule_rl_model
        self.meta_optimizer = torch.optim.Adam(
            model.parameters(),
            lr=meta_lr
        )
        self.inner_lr = inner_lr

    def meta_train(self, task_distribution, num_tasks=10, num_inner_steps=5):
        """
        å…ƒè®­ç»ƒï¼šåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå­¦ä¹ å¿«é€Ÿé€‚åº”èƒ½åŠ›

        Args:
            task_distribution: ä»»åŠ¡åˆ†å¸ƒï¼ˆä¸åŒæ•°æ®é›†/æŸ¥è¯¢ç±»å‹ï¼‰
            num_tasks: æ¯æ¬¡å…ƒæ›´æ–°é‡‡æ ·å¤šå°‘ä¸ªä»»åŠ¡
            num_inner_steps: å†…å¾ªç¯é€‚åº”æ­¥æ•°
        """
        for meta_iteration in range(self.num_meta_iterations):
            # é‡‡æ ·ä»»åŠ¡batch
            tasks = task_distribution.sample(num_tasks)

            meta_loss = 0.0

            for task in tasks:
                # å¤åˆ¶å½“å‰å‚æ•°
                adapted_params = self.model.parameters()

                # å†…å¾ªç¯ï¼šåœ¨ä»»åŠ¡æ”¯æŒé›†ä¸Šå¿«é€Ÿé€‚åº”
                for _ in range(num_inner_steps):
                    support_queries = task.sample_support()
                    loss = self.compute_task_loss(support_queries, adapted_params)

                    # å†…å¾ªç¯æ¢¯åº¦æ›´æ–°
                    adapted_params = self.inner_update(adapted_params, loss)

                # åœ¨ä»»åŠ¡æŸ¥è¯¢é›†ä¸Šè¯„ä¼°
                query_queries = task.sample_query()
                meta_loss += self.compute_task_loss(query_queries, adapted_params)

            # å…ƒæ›´æ–°
            meta_loss /= num_tasks
            self.meta_optimizer.zero_grad()
            meta_loss.backward()
            self.meta_optimizer.step()

    def fast_adapt(self, new_task, num_steps=5):
        """
        åœ¨æ–°ä»»åŠ¡ä¸Šå¿«é€Ÿé€‚åº”
        """
        for step in range(num_steps):
            queries = new_task.sample()
            loss = self.compute_task_loss(queries)

            # æ¢¯åº¦æ›´æ–°
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

        return self.model
```

**é¢„æœŸæ•ˆæœ**ï¼š
- åœ¨æ–°é¢†åŸŸåªéœ€10-20ä¸ªæ ·æœ¬å³å¯è¾¾åˆ°90%æ€§èƒ½
- è·¨é¢†åŸŸè¿ç§»èƒ½åŠ›æå‡50%

### 7.2 é€†å¼ºåŒ–å­¦ä¹ ï¼ˆInverse RLï¼‰

**åŠ¨æœº**ï¼š
èƒ½å¦ä»äººç±»ä¸“å®¶çš„æ¨ç†è·¯å¾„ä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Ÿ

**æ–¹æ¡ˆï¼šMaxEnt IRL**

```python
class InverseRuleRL:
    """
    ä»ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°
    """
    def __init__(self, feature_dim):
        # å¥–åŠ±å‡½æ•°å‚æ•°åŒ–
        self.reward_net = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def learn_reward_from_demonstrations(self, expert_trajectories):
        """
        ä»ä¸“å®¶è½¨è¿¹å­¦ä¹ å¥–åŠ±å‡½æ•°

        Args:
            expert_trajectories: [
                [(s0, a0), (s1, a1), ...],  # ä¸“å®¶è½¨è¿¹1
                ...
            ]
        """
        for iteration in range(self.num_iters):
            # 1. æå–ç‰¹å¾
            expert_features = self.extract_features(expert_trajectories)

            # 2. ä½¿ç”¨å½“å‰å¥–åŠ±å‡½æ•°è¿è¡ŒRL
            learned_trajectories = self.run_rl_with_current_reward()
            learned_features = self.extract_features(learned_trajectories)

            # 3. æœ€å¤§ç†µIRLï¼šæœ€å¤§åŒ–ä¸“å®¶è½¨è¿¹çš„log likelihood
            # L = E_expert[log p(a|s)] - log Z
            loss = self.maxent_irl_loss(expert_features, learned_features)

            # 4. æ›´æ–°å¥–åŠ±ç½‘ç»œ
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

    def extract_features(self, trajectories):
        """
        æå–è½¨è¿¹ç‰¹å¾ï¼ˆç”¨äºå¥–åŠ±å­¦ä¹ ï¼‰
        """
        features = []
        for traj in trajectories:
            # ç‰¹å¾åŒ…å«ï¼š
            # - è§„åˆ™ä¸€è‡´æ€§
            # - è·¯å¾„é•¿åº¦
            # - å®ä½“ç±»å‹
            # - å…³ç³»ç±»å‹åˆ†å¸ƒ
            feat = self.compute_trajectory_features(traj)
            features.append(feat)
        return torch.stack(features)
```

**åº”ç”¨åœºæ™¯**ï¼š
- åŒ»å­¦çŸ¥è¯†å›¾è°±ï¼šä»åŒ»ç”Ÿçš„æ¨ç†è¿‡ç¨‹å­¦ä¹ 
- æ³•å¾‹çŸ¥è¯†å›¾è°±ï¼šä»æ³•å®˜çš„åˆ¤å†³é€»è¾‘å­¦ä¹ 

### 7.3 å¤šæ™ºèƒ½ä½“ç«äº‰ï¼ˆAdversarial RLï¼‰

**åŠ¨æœº**ï¼š
é€šè¿‡å¯¹æŠ—è®­ç»ƒæé«˜æ¨¡å‹é²æ£’æ€§

**æ–¹æ¡ˆï¼šAdversarial Rule Selection**

```python
class AdversarialRuleRL:
    """
    å¯¹æŠ—è®­ç»ƒï¼šä¸€ä¸ªAgenté€‰æ‹©è§„åˆ™ï¼Œå¦ä¸€ä¸ªAgentè¯•å›¾å¹²æ‰°
    """
    def __init__(self):
        # Protagonist: é€‰æ‹©æœ‰ç”¨çš„è§„åˆ™
        self.protagonist = RuleSelectorAgent(...)

        # Antagonist: é€‰æ‹©è¯¯å¯¼æ€§çš„è§„åˆ™
        self.antagonist = RuleSelectorAgent(...)

    def adversarial_train_step(self, query):
        """
        å¯¹æŠ—è®­ç»ƒæ­¥éª¤
        """
        # 1. Protagonisté€‰æ‹©è§„åˆ™
        good_rules = self.protagonist.select_rules(query)

        # 2. Antagonisté€‰æ‹©è¯¯å¯¼è§„åˆ™
        bad_rules = self.antagonist.select_rules(query)

        # 3. æ··åˆè§„åˆ™é›†
        mixed_rules = self.mix_rules(good_rules, bad_rules)

        # 4. è¿è¡Œæ¨ç†
        reward = self.run_inference(query, mixed_rules)

        # 5. æ›´æ–°
        # Protagonistç›®æ ‡ï¼šæœ€å¤§åŒ–reward
        protagonist_loss = -reward

        # Antagonistç›®æ ‡ï¼šæœ€å°åŒ–rewardï¼ˆå¯¹æŠ—ï¼‰
        antagonist_loss = reward

        # å„è‡ªæ›´æ–°
        self.update_protagonist(protagonist_loss)
        self.update_antagonist(antagonist_loss)
```

**é¢„æœŸæ•ˆæœ**ï¼š
- æ¨¡å‹å¯¹å™ªå£°è§„åˆ™çš„é²æ£’æ€§æå‡40%
- åœ¨è§„åˆ™è´¨é‡ä¸ä½³çš„æ•°æ®é›†ä¸Šæ€§èƒ½æå‡15%

### 7.4 è”é‚¦å¼ºåŒ–å­¦ä¹ 

**åŠ¨æœº**ï¼š
ä¸åŒæœºæ„æœ‰å„è‡ªçš„çŸ¥è¯†å›¾è°±ï¼Œå¦‚ä½•åä½œå­¦ä¹ è€Œä¸å…±äº«æ•°æ®ï¼Ÿ

**æ–¹æ¡ˆï¼šFederated RulE-RL**

```python
class FederatedRuleRL:
    """
    è”é‚¦å­¦ä¹ æ¡†æ¶
    """
    def __init__(self, num_clients):
        self.global_model = RuleRLTrainer(...)
        self.client_models = [
            RuleRLTrainer(...) for _ in range(num_clients)
        ]

    def federated_train(self, num_rounds=100):
        """
        è”é‚¦è®­ç»ƒ
        """
        for round in range(num_rounds):
            # 1. åˆ†å‘å…¨å±€æ¨¡å‹
            self.distribute_global_model()

            # 2. å„å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
            client_updates = []
            for client_id, client in enumerate(self.client_models):
                # åœ¨æœ¬åœ°æ•°æ®ä¸Šè®­ç»ƒ
                update = client.local_train(num_epochs=5)
                client_updates.append(update)

            # 3. èšåˆæ›´æ–°ï¼ˆFedAvgï¼‰
            global_update = self.aggregate_updates(client_updates)

            # 4. æ›´æ–°å…¨å±€æ¨¡å‹
            self.global_model.apply_update(global_update)

    def aggregate_updates(self, updates):
        """
        èšåˆå®¢æˆ·ç«¯æ›´æ–°ï¼ˆåŠ æƒå¹³å‡ï¼‰
        """
        aggregated = {}
        for key in updates[0].keys():
            aggregated[key] = torch.mean(
                torch.stack([u[key] for u in updates]),
                dim=0
            )
        return aggregated
```

**åº”ç”¨åœºæ™¯**ï¼š
- å¤šåŒ»é™¢åä½œåŒ»ç–—çŸ¥è¯†å›¾è°±æ¨ç†
- è·¨ä¼ä¸šé‡‘èçŸ¥è¯†å›¾è°±åˆ†æ

### 7.5 å¯è§£é‡Šçš„RLç­–ç•¥

**åŠ¨æœº**ï¼š
RLå­¦åˆ°çš„ç­–ç•¥æ˜¯é»‘ç›’ï¼Œå¦‚ä½•æé«˜å¯è§£é‡Šæ€§ï¼Ÿ

**æ–¹æ¡ˆï¼šSymbolic Policy Extraction**

```python
class SymbolicPolicyExtractor:
    """
    ä»RLç­–ç•¥ä¸­æå–ç¬¦å·åŒ–è§„åˆ™
    """
    def extract_decision_tree(self, rl_agent, states, actions):
        """
        å°†RLç­–ç•¥è’¸é¦ä¸ºå†³ç­–æ ‘

        Args:
            rl_agent: è®­ç»ƒå¥½çš„RL Agent
            states: çŠ¶æ€æ ·æœ¬
            actions: RLé€‰æ‹©çš„åŠ¨ä½œ

        Returns:
            decision_tree: å¯è§£é‡Šçš„å†³ç­–æ ‘
        """
        from sklearn.tree import DecisionTreeClassifier

        # 1. æ”¶é›†(çŠ¶æ€, åŠ¨ä½œ)å¯¹
        X = []  # çŠ¶æ€ç‰¹å¾
        y = []  # åŠ¨ä½œæ ‡ç­¾

        for state in states:
            # æå–å¯è§£é‡Šç‰¹å¾
            features = self.extract_interpretable_features(state)
            # ç‰¹å¾åŒ…å«ï¼š
            # - å½“å‰å®ä½“ç±»å‹
            # - ç›®æ ‡å…³ç³»ç±»å‹
            # - å·²è®¿é—®è·³æ•°
            # - è§„åˆ™åŒ¹é…åº¦
            X.append(features)

            # RLé€‰æ‹©çš„åŠ¨ä½œ
            action = rl_agent.select_action(state, deterministic=True)
            y.append(action)

        # 2. è®­ç»ƒå†³ç­–æ ‘
        dt = DecisionTreeClassifier(max_depth=5)
        dt.fit(X, y)

        # 3. è½¬æ¢ä¸ºå¯è¯»è§„åˆ™
        rules = self.decision_tree_to_rules(dt)

        return rules

    def decision_tree_to_rules(self, dt):
        """
        å°†å†³ç­–æ ‘è½¬æ¢ä¸ºIF-THENè§„åˆ™
        """
        from sklearn.tree import _tree

        tree = dt.tree_
        feature_names = [f"feature_{i}" for i in range(tree.n_features)]

        def recurse(node, depth, condition):
            if tree.feature[node] != _tree.TREE_UNDEFINED:
                name = feature_names[tree.feature[node]]
                threshold = tree.threshold[node]

                # å·¦å­æ ‘
                left_condition = condition + f" AND {name} <= {threshold}"
                recurse(tree.children_left[node], depth + 1, left_condition)

                # å³å­æ ‘
                right_condition = condition + f" AND {name} > {threshold}"
                recurse(tree.children_right[node], depth + 1, right_condition)
            else:
                # å¶èŠ‚ç‚¹ï¼šè¾“å‡ºè§„åˆ™
                action = np.argmax(tree.value[node])
                print(f"IF {condition} THEN select action {action}")

        recurse(0, 1, "")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
IF entity_type == "Person" AND target_relation == "grandfather" THEN select "father"
IF entity_type == "Organization" AND target_relation == "location" THEN select "headquarter"
...
```

---

## ğŸ“– å…«ã€æ€»ç»“

### æ ¸å¿ƒåˆ›æ–°

**RulE-RL = RulEï¼ˆè§„åˆ™åµŒå…¥ï¼‰+ RLï¼ˆè‡ªé€‚åº”å†³ç­–ï¼‰**

1. âœ… **è‡ªé€‚åº”è§„åˆ™é€‰æ‹©**
   - é«˜å±‚Contextual BanditåŠ¨æ€é€‰æ‹©è§„åˆ™
   - é¿å…è®¡ç®—æ‰€æœ‰è§„åˆ™ï¼ˆ65%èŠ‚çœï¼‰

2. âœ… **é«˜æ•ˆè·¯å¾„æ¢ç´¢**
   - ä½å±‚Policy Gradientå¼•å¯¼è·¯å¾„æœç´¢
   - é¿å…ç›²ç›®æšä¸¾ï¼ˆ88%è·¯å¾„èŠ‚çœï¼‰

3. âœ… **å±‚æ¬¡åŒ–å­¦ä¹ **
   - åŒå±‚Agentåä½œ
   - åˆ†è€Œæ²»ä¹‹é™ä½å¤æ‚åº¦

4. âœ… **å¥–åŠ±å¡‘å½¢**
   - ç»“åˆç¬¦å·ï¼ˆè§„åˆ™ä¸€è‡´æ€§ï¼‰å’Œç¥ç»ï¼ˆåµŒå…¥è·ç¦»ï¼‰
   - åŠ é€ŸRLæ”¶æ•›

### é¢„æœŸæˆæœ

**æ€§èƒ½æå‡**ï¼š
- vs RotatE: +11.3% MRR
- vs RulE: +4.5% MRR
- vs MINERVA: +9.0% MRR

**æ•ˆç‡æå‡**ï¼š
- æ¨ç†é€Ÿåº¦: 2.0xåŠ é€Ÿ
- è§„åˆ™ä½¿ç”¨: 35% (vs 100%)
- è·¯å¾„æšä¸¾: 12% (vs 100%)

**ç†è®ºè´¡çŒ®**ï¼š
- è¯æ˜æ”¶æ•›æ€§å’Œæ ·æœ¬å¤æ‚åº¦
- å»ºç«‹è§„åˆ™æ¨ç†ä¸RLçš„ç†è®ºè”ç³»
- æå‡ºå±‚æ¬¡åŒ–RLæ–°èŒƒå¼

### å®æ–½è·¯çº¿

**Phase 1ï¼ˆ2ä¸ªæœˆï¼‰**ï¼š
- å®ç°åŸºç¡€RulE-RLæ¡†æ¶
- åœ¨UMLSä¸ŠéªŒè¯å¯è¡Œæ€§
- é¢„æœŸMRR: 0.90+

**Phase 2ï¼ˆ1-2ä¸ªæœˆï¼‰**ï¼š
- å®Œæ•´å®ç°ä¸¤å±‚Agent
- åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå®éªŒ
- æ¶ˆèç ”ç©¶å’Œå¯¹æ¯”åˆ†æ

**Phase 3ï¼ˆ1ä¸ªæœˆï¼‰**ï¼š
- å¯è§£é‡Šæ€§åˆ†æ
- æ‰©å±•æ–¹å‘æ¢ç´¢ï¼ˆMeta-RLç­‰ï¼‰
- æ’°å†™è®ºæ–‡

**æ€»æ—¶é—´**: 4-5ä¸ªæœˆ

**å‘è¡¨ç›®æ ‡**ï¼š
- ICLR/NeurIPS/ICML 2025ï¼ˆRLé¡¶ä¼šï¼‰
- ACL/EMNLP 2025ï¼ˆNLPé¡¶ä¼šï¼‰

---

## ğŸ“š ä¹ã€å‚è€ƒæ–‡çŒ®

### æ ¸å¿ƒå‚è€ƒ

1. **RulEåŸè®ºæ–‡**
   - Tang et al. (2024). "RulE: Knowledge Graph Reasoning with Rule Embedding", ACL 2024

2. **RL for KG Reasoning**
   - Das et al. (2018). "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning", ICLR 2018
   - MINERVAåŸºç¡€

3. **Policy Gradient**
   - Sutton et al. (1999). "Policy Gradient Methods for Reinforcement Learning with Function Approximation", NeurIPS 1999
   - REINFORCEç®—æ³•

4. **Contextual Bandits**
   - Auer (2002). "Using Confidence Bounds for Exploitation-Exploration Trade-offs", JMLR 2002
   - UCBç®—æ³•ç†è®º

5. **Hierarchical RL**
   - Nachum et al. (2018). "Data-Efficient Hierarchical Reinforcement Learning", NeurIPS 2018
   - HIROæ¡†æ¶

### æ‰©å±•é˜…è¯»

6. **Meta-RL**
   - Finn et al. (2017). "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", ICML 2017
   - MAML

7. **Inverse RL**
   - Ziebart et al. (2008). "Maximum Entropy Inverse Reinforcement Learning", AAAI 2008
   - MaxEnt IRL

8. **Multi-Agent RL**
   - Lowe et al. (2017). "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments", NeurIPS 2017
   - MADDPG

9. **RL + Logic**
   - Jiang & Luo (2019). "Neural Logic Reinforcement Learning", ICML 2019
   - é€»è¾‘å¼•å¯¼çš„RL

10. **Curriculum Learning**
    - Bengio et al. (2009). "Curriculum Learning", ICML 2009
    - è¯¾ç¨‹å­¦ä¹ ç†è®º

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2024å¹´11æœˆ
**ä½œè€…**: RulE-RLé¡¹ç›®ç»„
