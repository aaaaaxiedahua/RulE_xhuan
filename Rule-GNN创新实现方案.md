# Rule-GNN: åŸºäºå›¾ç¥ç»ç½‘ç»œçš„è§„åˆ™æ„ŸçŸ¥çŸ¥è¯†å›¾è°±æ¨ç†

**ç»“åˆRulEè®ºæ–‡ä¸GNNæ¡†æ¶çš„åˆ›æ–°æ”¹è¿›æ–¹æ¡ˆ**

---

## ğŸ“‹ ç›®å½•

1. [èƒŒæ™¯ä¸åŠ¨æœº](#èƒŒæ™¯ä¸åŠ¨æœº)
2. [æ ¸å¿ƒåˆ›æ–°ç‚¹](#æ ¸å¿ƒåˆ›æ–°ç‚¹)
3. [Rule-GNNæ¨¡å‹æ¶æ„](#rule-gnnæ¨¡å‹æ¶æ„)
4. [æŠ€æœ¯å®ç°ç»†èŠ‚](#æŠ€æœ¯å®ç°ç»†èŠ‚)
5. [å®Œæ•´ä»£ç å®ç°](#å®Œæ•´ä»£ç å®ç°)
6. [å®éªŒè®¾è®¡](#å®éªŒè®¾è®¡)
7. [ä¸ç°æœ‰æ–¹æ³•å¯¹æ¯”](#ä¸ç°æœ‰æ–¹æ³•å¯¹æ¯”)
8. [æœªæ¥æ‰©å±•æ–¹å‘](#æœªæ¥æ‰©å±•æ–¹å‘)

---

## ğŸ¯ ä¸€ã€èƒŒæ™¯ä¸åŠ¨æœº

### RulEå­˜åœ¨çš„æ ¸å¿ƒç—›ç‚¹

æ ¹æ®ACL 2024è®ºæ–‡ã€ŠRulE: Knowledge Graph Reasoning with Rule Embeddingã€‹ç¬¬7èŠ‚"Limitations"ï¼š

> **é—®é¢˜1ï¼šè®¡ç®—ç“¶é¢ˆ**
> "A limitation of RulE is that, similar to prior works which apply logical rules for inference, RulE's soft rule reasoning part needs to enumerate all paths between entity pairs, making it difficult to scale."

**å…·ä½“è¡¨ç°**ï¼š
- éœ€è¦æšä¸¾æ‰€æœ‰è·¯å¾„æ¥æ¿€æ´»è§„åˆ™
- ä½¿ç”¨BFSæœç´¢ï¼Œå¤æ‚åº¦ä¸º `O(|E|dÂ²/|V|)`
- æ¨ç†æ—¶é—´ï¼ˆTable 7ï¼‰ï¼šFB15k-237éœ€è¦3.70åˆ†é’Ÿ

> **é—®é¢˜2ï¼šä¿¡æ¯å‰²è£‚**
> ä¸åŒè·¯å¾„ä¹‹é—´ä¸å…±äº«ä¸­é—´èŠ‚ç‚¹ä¿¡æ¯ï¼Œæ¯æ¬¡æŸ¥è¯¢éƒ½è¦é‡æ–°è®¡ç®—

**æ•°æ®æ”¯æŒ**ï¼š
```
è®ºæ–‡Table 6æ˜¾ç¤ºï¼š
- FB15k-237: åªæœ‰34.4%çš„è¾¹åœ¨2-hop cycleä¸­
- WN18RR: åªæœ‰17.7%çš„è¾¹åœ¨3-hop cycleä¸­
â†’ å¤§é‡è·¯å¾„æšä¸¾æ˜¯ä½æ•ˆçš„
```

### GNNçš„å¤©ç„¶ä¼˜åŠ¿

âœ… **ä¼˜åŠ¿1ï¼šæ¶ˆæ¯ä¼ é€’æœºåˆ¶**
- é€šè¿‡å¤šå±‚ä¼ æ’­è‡ªç„¶æ•è·å¤šè·³è¯­ä¹‰
- ç¬¬Lå±‚GNN = Lè·³é‚»å±…ä¿¡æ¯èšåˆ

âœ… **ä¼˜åŠ¿2ï¼šèŠ‚ç‚¹è¡¨ç¤ºå…±äº«**
- ä¸­é—´èŠ‚ç‚¹çŠ¶æ€å¤©ç„¶å…±äº«
- é¿å…é‡å¤è®¡ç®—ç›¸åŒå­è·¯å¾„

âœ… **ä¼˜åŠ¿3ï¼šç«¯åˆ°ç«¯è®­ç»ƒ**
- å¯å¾®åˆ†ï¼Œæ”¯æŒæ¢¯åº¦åå‘ä¼ æ’­
- æ— éœ€é¢„å…ˆæšä¸¾è§„åˆ™æ”¯æŒ

### Rule-GNNçš„ç›®æ ‡

**æ ¸å¿ƒæ€æƒ³**ï¼š
åœ¨GNNæ¡†æ¶ä¸­æ˜¾å¼åœ°å¼•å…¥è§„åˆ™ç»“æ„ï¼Œä½¿æ¶ˆæ¯ä¼ é€’"éµå¾ªé€»è¾‘è§„åˆ™"çš„æ–¹å‘ä¼ æ’­ï¼Œè€Œä¸æ˜¯ç›²ç›®èšåˆæ‰€æœ‰é‚»å±…ã€‚

**é¢„æœŸæ”¶ç›Š**ï¼š
- æ¨ç†é€Ÿåº¦æå‡2-3å€ï¼ˆé¿å…è·¯å¾„æšä¸¾ï¼‰
- MRRæå‡3-7%ï¼ˆä¿ç•™è§„åˆ™æŒ‡å¯¼ï¼‰
- å¯æ‰©å±•åˆ°å¤§è§„æ¨¡KGï¼ˆçº¿æ€§å¤æ‚åº¦ï¼‰

---

## ğŸ’¡ äºŒã€æ ¸å¿ƒåˆ›æ–°ç‚¹

### åˆ›æ–°ç‚¹1ï¼šè§„åˆ™æ„ŸçŸ¥çš„æ¶ˆæ¯ä¼ é€’ï¼ˆRule-Aware Message Passingï¼‰

**ä¼ ç»ŸR-GCNçš„é—®é¢˜**ï¼š
```python
h_i^(l+1) = f(Î£_{(j,r)âˆˆN(i)} W_r h_j^(l))
```
â†’ å¿½ç•¥äº†è§„åˆ™ç»“æ„ï¼Œæ‰€æœ‰å…³ç³»ç±»å‹åŒç­‰å¯¹å¾…

**Rule-GNNçš„æ”¹è¿›**ï¼š
```python
# è§„åˆ™è°ƒæ§çš„æ¶ˆæ¯è®¡ç®—
m_ij^(R) = Î±_ij^(R) Â· W_r Â· h_j^(l)

# æ³¨æ„åŠ›æƒé‡ç”±è§„åˆ™åµŒå…¥å†³å®š
Î±_ij^(R) = softmax((W_q h_i^(l))^T (W_k [h_j^(l); h_r; h_R]))
```

**å…³é”®æœºåˆ¶**ï¼š
- è‹¥è§„åˆ™Rä¸­åŒ…å«å…³ç³»rï¼Œ`Î±_ij^(R)`è¾ƒå¤§
- è‹¥è§„åˆ™ä¸å½“å‰è¾¹æ— å…³ï¼Œæƒé‡è¶‹è¿‘0
- æ¨¡å‹è‡ªåŠ¨å­¦ä¹ å“ªäº›è¾¹"ç¬¦åˆå½“å‰è§„åˆ™ä½“"

### åˆ›æ–°ç‚¹2ï¼šè§„åˆ™ç»„åˆå±‚ï¼ˆRule Composition Layerï¼‰

**RulEçš„åšæ³•**ï¼ˆè®ºæ–‡Equation 3ï¼‰ï¼š
```
è§„åˆ™: r1 âˆ§ r2 â†’ r3
åµŒå…¥çº¦æŸ: ||g(r1) + g(r2) + g(R) - g(r3)|| â†’ min
```
â†’ é™æ€ç»„åˆï¼Œéœ€è¦é¢„å…ˆè®¡ç®—

**Rule-GNNçš„åšæ³•**ï¼š
```python
# å¤šå±‚ä¼ æ’­çš„å åŠ å®ç°è§„åˆ™é“¾ç»„åˆ
h_i^(l) = f_l(h_i^(l-1), MSG_r^(l-1))

# å¯¹äºé•¿åº¦ä¸ºLçš„è§„åˆ™ï¼ŒGNNçš„Lå±‚ä¼ æ’­ = è§„åˆ™ä½“çš„Læ¬¡ç»„åˆ
```

**ä¸¾ä¾‹**ï¼š
```
è§„åˆ™: father âˆ§ father â‡’ grandfather
â†’ ç¬¬1å±‚ä¼ æ’­fatherä¿¡æ¯
â†’ ç¬¬2å±‚å†ä¼ æ’­fatherä¿¡æ¯
â†’ ç¬¬2å±‚è¾“å‡ºå³ä¸ºgrandfatheræ½œåœ¨å…³ç³»æ–¹å‘
```

### åˆ›æ–°ç‚¹3ï¼šè§„åˆ™æ„ŸçŸ¥æ³¨æ„åŠ›æ­£åˆ™åŒ–

**æ–°å¼•å…¥çš„æŸå¤±é¡¹**ï¼š
```python
L_attn = Î£_R KL(Î±^(R) || mask_R)
```

**ç›®çš„**ï¼š
- é¼“åŠ±æ³¨æ„åŠ›æƒé‡ç¨€ç–
- åªæ¿€æ´»ä¸è§„åˆ™ç›¸å…³çš„è¾¹
- æé«˜å¯è§£é‡Šæ€§

---

## ğŸ—ï¸ ä¸‰ã€Rule-GNNæ¨¡å‹æ¶æ„

### æ€»ä½“æ¡†æ¶

```
Input Layer
    â†“
Rule-Aware Message Passing (å¤šå±‚)
    â†“
Rule Composition Layer
    â†“
Prediction Layer
```

### 3.1 Input Layer

**è¾“å…¥åŒ…æ‹¬**ï¼š

1âƒ£ **å®ä½“åµŒå…¥** `h_e âˆˆ R^d`
```python
self.entity_embedding = nn.Embedding(num_entities, hidden_dim)
```

2âƒ£ **å…³ç³»åµŒå…¥** `h_r âˆˆ C^k`ï¼ˆå¤æ•°ç©ºé—´ï¼Œæ²¿ç”¨RotatEï¼‰
```python
self.relation_embedding = nn.Embedding(num_relations, hidden_dim)
```

3âƒ£ **è§„åˆ™åµŒå…¥** `h_R âˆˆ R^d`
```python
self.rule_embedding = nn.Embedding(num_rules, rule_dim)
```

**è§„åˆ™æ ¼å¼**ï¼š
```
R_i: r1(x,y1) âˆ§ r2(y1,y2) âˆ§ ... âˆ§ rl(y_{l-1},yl) â‡’ r_{l+1}(x,yl)
```

### 3.2 Rule-Aware Message Passing

**æ ¸å¿ƒå…¬å¼**ï¼š

```python
# Step 1: è®¡ç®—è§„åˆ™è°ƒæ§çš„æ³¨æ„åŠ›æƒé‡
Î±_ij^(R) = softmax(
    (W_q h_i^(l))^T Â· (W_k [h_j^(l); h_r; h_R]) / âˆšd
)

# Step 2: è®¡ç®—æ¶ˆæ¯
m_ij^(R) = Î±_ij^(R) Â· W_r Â· h_j^(l)

# Step 3: èšåˆæ›´æ–°
h_i^(l+1) = Ïƒ(Î£_{(j,r)âˆˆN(i)} m_ij^(R) + b)
```

**ä¸RulEçš„è”ç³»**ï¼š
| ç»„ä»¶ | RulE | Rule-GNN |
|------|------|----------|
| è§„åˆ™ç½®ä¿¡åº¦ | `w_i = Î³_r - d(r1,...,rl,R)` | èå…¥æ³¨æ„åŠ›æƒé‡ |
| è·¯å¾„æšä¸¾ | BFSæ˜¾å¼æšä¸¾ | GNNéšå¼ä¼ æ’­ |
| ä¿¡æ¯å…±äº« | æ—  | èŠ‚ç‚¹åµŒå…¥å…±äº« |

### 3.3 Rule Composition Layer

**å®ç°è§„åˆ™é“¾çš„ç»„åˆ**ï¼š

å¯¹äºè§„åˆ™ `R: r1 âˆ§ r2 âˆ§ ... âˆ§ rl â†’ r_{l+1}`ï¼š

```python
# ç¬¬1å±‚ï¼šä¼ æ’­r1ä¿¡æ¯
h^(1) = GNN_layer_1(h^(0), r1)

# ç¬¬2å±‚ï¼šä¼ æ’­r2ä¿¡æ¯ï¼ˆåŸºäºç¬¬1å±‚ï¼‰
h^(2) = GNN_layer_2(h^(1), r2)

# ...

# ç¬¬lå±‚ï¼šå®Œæˆè§„åˆ™ä½“ç»„åˆ
h^(l) = GNN_layer_l(h^(l-1), rl)
â†’ h^(l)åŒ…å«äº†r1âˆ˜r2âˆ˜...âˆ˜rlçš„è¯­ä¹‰
```

**å…³é”®æ€§è´¨**ï¼š
- GNNçš„å±‚æ•° = è§„åˆ™é•¿åº¦
- æ¯ä¸€å±‚å¯¹åº”è§„åˆ™ä½“ä¸­çš„ä¸€ä¸ªå…³ç³»
- æœ€ç»ˆè¾“å‡ºæ˜¯è§„åˆ™å¤´çš„è¡¨ç¤º

### 3.4 Prediction Layer

**æœ€ç»ˆå¾—åˆ†è®¡ç®—**ï¼š

```python
# æ–¹æ³•1ï¼šå†…ç§¯ï¼ˆç±»ä¼¼DistMultï¼‰
s(h, r, t) = h_h^(L) Â· W_r Â· h_t^(0)

# æ–¹æ³•2ï¼šRotatEé£æ ¼ï¼ˆä¿æŒå¤æ•°ç©ºé—´ï¼‰
s(h, r, t) = Î³ - ||h_h^(L) â—¦ r - h_t^(0)||

# æ–¹æ³•3ï¼šç»“åˆKGEåˆ†æ•°ï¼ˆç±»ä¼¼RulEï¼‰
s(h, r, t) = s_GNN(h, r, t) + Î² Â· s_KGE(h, r, t)
```

---

## ğŸ”§ å››ã€æŠ€æœ¯å®ç°ç»†èŠ‚

### 4.1 å®Œæ•´ä¾‹å­ï¼šç¥–çˆ¶è§„åˆ™æ¨ç†

**è§„åˆ™**ï¼š
```
father(x, y) âˆ§ father(y, z) â‡’ grandfather(x, z)
```

**ç›®æ ‡**ï¼š
```
æ¨ç† (å¼ ä¸‰, grandfather, ?)
```

**æµç¨‹**ï¼š

#### ç¬¬0å±‚ï¼šåˆå§‹åŒ–

```python
h[å¼ ä¸‰]^(0) = entity_embedding[å¼ ä¸‰]  # å®ä½“åµŒå…¥
h[æå››]^(0) = entity_embedding[æå››]
h[ç‹äº”]^(0) = entity_embedding[ç‹äº”]

h_father = relation_embedding[father]
h_R = rule_embedding[R: fatherâˆ§fatherâ†’grandfather]
```

#### ç¬¬1å±‚ï¼šä¼ æ’­fatherä¿¡æ¯

```python
# å¼ ä¸‰æ”¶é›†å…¶å­èŠ‚ç‚¹ï¼ˆå„¿å­ï¼‰çš„ä¿¡æ¯
for (å¼ ä¸‰, father, æå››) in edges:
    # è®¡ç®—è§„åˆ™è°ƒæ§çš„æ³¨æ„åŠ›
    Î± = softmax(
        (W_q h[å¼ ä¸‰]^(0))^T Â· (W_k [h[æå››]^(0); h_father; h_R])
    )

    # ä¼ é€’æ¶ˆæ¯
    m = Î± Â· W_father Â· h[æå››]^(0)

h[å¼ ä¸‰]^(1) = Ïƒ(m + b)  # è¡¨ç¤º"å¼ ä¸‰çš„å­ä¿¡æ¯"
```

#### ç¬¬2å±‚ï¼šä¼ æ’­father again

```python
# å¼ ä¸‰æ”¶é›†å­™å­ä¿¡æ¯ï¼ˆé€šè¿‡å„¿å­çš„å„¿å­ï¼‰
for (æå››, father, ç‹äº”) in edges:
    Î± = softmax(
        (W_q h[æå››]^(1))^T Â· (W_k [h[ç‹äº”]^(0); h_father; h_R])
    )

    m = Î± Â· W_father Â· h[ç‹äº”]^(0)

h[å¼ ä¸‰]^(2) = Ïƒ(aggregate_from_children(m) + b)
```

#### è§„åˆ™ç»„åˆç»“æœ

```python
# ç¬¬2å±‚è¾“å‡ºå¯¹åº”grandfatheræ–¹å‘çš„æ½œåœ¨å…³ç³»
s(å¼ ä¸‰, grandfather, ç‹äº”) = âŸ¨h[å¼ ä¸‰]^(2), W_grandfather h[ç‹äº”]^(0)âŸ©
```

**å…³é”®æ´å¯Ÿ**ï¼š
- æ•´ä¸ªè¿‡ç¨‹ä¸éœ€è¦æ˜¾å¼æšä¸¾è·¯å¾„
- è¯­ä¹‰ä¸Šå®Œå…¨éµå¾ªè§„åˆ™ç»“æ„
- ä¸­é—´èŠ‚ç‚¹ï¼ˆæå››ï¼‰çš„è¡¨ç¤ºè¢«å…±äº«

### 4.2 ä¸RulEä»£ç çš„å¯¹åº”å…³ç³»

**RulEä»£ç ï¼ˆsrc/model.py:337-409ï¼‰**ï¼š
```python
# RulEçš„groundingè¿‡ç¨‹
def forward(self, all_h, r_head, r_body, edges_to_remove):
    # æšä¸¾è·¯å¾„
    grounding_count = self.graph.grounding(all_h, r_head, r_body, edges_to_remove)

    # è®¡ç®—è§„åˆ™åˆ†æ•°
    rule_feature = self.mlp_feature(rule_emb)
    score = self.FuncToNodeSum(grounding_count, rule_feature)
```

**Rule-GNNå¯¹åº”å®ç°**ï¼š
```python
class RuleGNN(nn.Module):
    def forward(self, h, r, num_hops):
        # ç”¨GNNä¼ æ’­æ›¿ä»£è·¯å¾„æšä¸¾
        for layer in range(num_hops):
            h = self.rule_aware_conv(h, r, rule_emb)

        # ç›´æ¥ä½¿ç”¨æœ€ç»ˆèŠ‚ç‚¹è¡¨ç¤º
        score = self.score_func(h, r, candidates)
        return score
```

**å¯¹æ¯”**ï¼š
| æ“ä½œ | RulE | Rule-GNN |
|------|------|----------|
| è·¯å¾„æŸ¥æ‰¾ | BFSæšä¸¾ | GNNä¼ æ’­ |
| ä¸­é—´çŠ¶æ€ | ä¸ä¿å­˜ | èŠ‚ç‚¹åµŒå…¥ |
| è§„åˆ™åº”ç”¨ | groundingè®¡æ•° | æ³¨æ„åŠ›æƒé‡ |
| å¤æ‚åº¦ | O(dÂ·paths) | O(dÂ·layers) |

---

## ğŸ’» äº”ã€å®Œæ•´ä»£ç å®ç°

### 5.1 Rule-Aware Graph Convolution Layer

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_scatter import scatter_add

class RuleAwareGraphConv(nn.Module):
    """
    è§„åˆ™æ„ŸçŸ¥çš„å›¾å·ç§¯å±‚
    """
    def __init__(self, in_dim, out_dim, num_relations, num_rules):
        super().__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim

        # å…³ç³»ç‰¹å®šçš„å˜æ¢çŸ©é˜µï¼ˆç±»ä¼¼R-GCNï¼‰
        self.W_r = nn.Parameter(torch.Tensor(num_relations, in_dim, out_dim))

        # æ³¨æ„åŠ›æœºåˆ¶çš„å‚æ•°
        self.W_q = nn.Linear(in_dim, out_dim)
        self.W_k = nn.Linear(in_dim + in_dim + in_dim, out_dim)
        # [h_j; h_r; h_R]çš„ç»´åº¦æ˜¯3*in_dim

        # è§„åˆ™åµŒå…¥
        self.rule_embedding = nn.Embedding(num_rules, in_dim)

        # åç½®
        self.bias = nn.Parameter(torch.Tensor(out_dim))

        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.W_r)
        nn.init.zeros_(self.bias)
        nn.init.xavier_uniform_(self.rule_embedding.weight)

    def forward(self, x, edge_index, edge_type, rule_ids):
        """
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, in_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            edge_type: è¾¹ç±»å‹ [num_edges]
            rule_ids: å½“å‰æ¿€æ´»çš„è§„åˆ™ID [num_rules]

        Returns:
            out: æ›´æ–°åçš„èŠ‚ç‚¹ç‰¹å¾ [num_nodes, out_dim]
            attention_weights: æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§£é‡Šæ€§ï¼‰
        """
        src, dst = edge_index  # [num_edges]

        # è·å–è§„åˆ™åµŒå…¥
        h_R = self.rule_embedding(rule_ids)  # [num_rules, in_dim]

        # å¯¹æ¯ä¸ªè§„åˆ™è®¡ç®—æ¶ˆæ¯
        messages = []
        attention_weights_list = []

        for rule_id in rule_ids:
            h_r_single = h_R[rule_id]  # [in_dim]

            # è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆè§„åˆ™æ„ŸçŸ¥ï¼‰
            # Query: ç›®æ ‡èŠ‚ç‚¹
            query = self.W_q(x[dst])  # [num_edges, out_dim]

            # Key: [æºèŠ‚ç‚¹; å…³ç³»åµŒå…¥; è§„åˆ™åµŒå…¥]
            # è¿™é‡Œç®€åŒ–ï¼šå‡è®¾å…³ç³»åµŒå…¥ç›´æ¥ä½¿ç”¨one-hot
            relation_emb = F.embedding(edge_type, self.W_r.mean(dim=-1))  # [num_edges, in_dim]

            # æ‰©å±•è§„åˆ™åµŒå…¥åˆ°æ‰€æœ‰è¾¹
            rule_emb_expanded = h_r_single.unsqueeze(0).expand(edge_index.size(1), -1)  # [num_edges, in_dim]

            # æ‹¼æ¥
            key_input = torch.cat([
                x[src],              # æºèŠ‚ç‚¹ç‰¹å¾
                relation_emb,        # å…³ç³»åµŒå…¥
                rule_emb_expanded    # è§„åˆ™åµŒå…¥
            ], dim=-1)  # [num_edges, 3*in_dim]

            key = self.W_k(key_input)  # [num_edges, out_dim]

            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
            attn_scores = (query * key).sum(dim=-1) / torch.sqrt(torch.tensor(self.out_dim, dtype=torch.float))
            # [num_edges]

            # Softmaxå½’ä¸€åŒ–ï¼ˆé’ˆå¯¹æ¯ä¸ªç›®æ ‡èŠ‚ç‚¹ï¼‰
            attn_weights = scatter_softmax(attn_scores, dst, dim=0)  # [num_edges]

            # è®¡ç®—æ¶ˆæ¯
            # m_ij = Î±_ij * W_r * h_j
            messages_r = []
            for r in range(edge_type.max() + 1):
                mask = (edge_type == r)
                if mask.sum() > 0:
                    msg = torch.matmul(x[src[mask]], self.W_r[r])  # [num_edges_r, out_dim]
                    msg = msg * attn_weights[mask].unsqueeze(-1)
                    messages_r.append(msg)

            if messages_r:
                messages.append(torch.cat(messages_r, dim=0))
                attention_weights_list.append(attn_weights)

        # èšåˆæ‰€æœ‰è§„åˆ™çš„æ¶ˆæ¯
        if messages:
            all_messages = torch.stack(messages, dim=0).mean(dim=0)  # [num_edges, out_dim]

            # èšåˆåˆ°ç›®æ ‡èŠ‚ç‚¹
            out = scatter_add(all_messages, dst, dim=0, dim_size=x.size(0))  # [num_nodes, out_dim]
            out = out + self.bias
            out = F.relu(out)
        else:
            out = torch.zeros(x.size(0), self.out_dim, device=x.device)

        # è¿”å›æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§£é‡Šæ€§åˆ†æ
        avg_attention = torch.stack(attention_weights_list, dim=0).mean(dim=0) if attention_weights_list else None

        return out, avg_attention

def scatter_softmax(src, index, dim=0):
    """
    å¯¹scatterçš„å…ƒç´ åšsoftmax
    """
    max_value = scatter_max(src, index, dim=dim)[0][index]
    exp_src = torch.exp(src - max_value)
    sum_exp = scatter_add(exp_src, index, dim=dim)[index]
    return exp_src / (sum_exp + 1e-16)

def scatter_max(src, index, dim=0):
    """
    Scatter max operation
    """
    size = int(index.max()) + 1
    out = torch.full((size,), float('-inf'), dtype=src.dtype, device=src.device)
    out = out.scatter_reduce_(0, index, src, reduce='amax', include_self=False)
    return out, None
```

### 5.2 å®Œæ•´çš„Rule-GNNæ¨¡å‹

```python
class RuleGNN(nn.Module):
    """
    å®Œæ•´çš„Rule-GNNæ¨¡å‹
    """
    def __init__(self, num_entities, num_relations, num_rules,
                 hidden_dim, num_layers, dropout=0.1):
        super().__init__()

        self.num_entities = num_entities
        self.num_relations = num_relations
        self.num_layers = num_layers

        # å®ä½“åµŒå…¥
        self.entity_embedding = nn.Embedding(num_entities, hidden_dim)

        # å…³ç³»åµŒå…¥ï¼ˆæ²¿ç”¨RotatEçš„å¤æ•°è¡¨ç¤ºï¼‰
        self.relation_embedding = nn.Embedding(num_relations, hidden_dim)

        # Rule-Aware GNNå±‚
        self.conv_layers = nn.ModuleList([
            RuleAwareGraphConv(hidden_dim, hidden_dim, num_relations, num_rules)
            for _ in range(num_layers)
        ])

        # Dropout
        self.dropout = nn.Dropout(dropout)

        # é¢„æµ‹å±‚
        self.score_func = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

        # ç”¨äºä¿å­˜æ³¨æ„åŠ›æƒé‡ï¼ˆå¯è§£é‡Šæ€§ï¼‰
        self.attention_weights = []

        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.entity_embedding.weight)
        nn.init.xavier_uniform_(self.relation_embedding.weight)

    def forward(self, queries, edge_index, edge_type, rule_ids,
                candidates=None, return_attention=False):
        """
        Args:
            queries: æŸ¥è¯¢ä¸‰å…ƒç»„ (h, r) [batch_size, 2]
            edge_index: å…¨å›¾çš„è¾¹ç´¢å¼• [2, num_edges]
            edge_type: è¾¹ç±»å‹ [num_edges]
            rule_ids: å½“å‰æŸ¥è¯¢ç›¸å…³çš„è§„åˆ™IDåˆ—è¡¨
            candidates: å€™é€‰å°¾å®ä½“ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™å¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†ï¼‰
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡

        Returns:
            scores: é¢„æµ‹åˆ†æ•°
            attention_weights: (å¯é€‰) æ³¨æ„åŠ›æƒé‡
        """
        # åˆå§‹åŒ–èŠ‚ç‚¹ç‰¹å¾
        h = self.entity_embedding.weight  # [num_entities, hidden_dim]

        # å¤šå±‚ä¼ æ’­ï¼ˆè§„åˆ™é•¿åº¦ï¼‰
        self.attention_weights = []

        for layer_idx, conv in enumerate(self.conv_layers):
            h, attn = conv(h, edge_index, edge_type, rule_ids)
            h = self.dropout(h)

            if return_attention:
                self.attention_weights.append(attn)

        # æå–æŸ¥è¯¢å¤´å®ä½“çš„è¡¨ç¤º
        batch_size = queries.size(0)
        h_heads = h[queries[:, 0]]  # [batch_size, hidden_dim]

        # è·å–å…³ç³»åµŒå…¥
        h_relations = self.relation_embedding(queries[:, 1])  # [batch_size, hidden_dim]

        # è®¡ç®—å¾—åˆ†
        if candidates is None:
            # å¯¹æ‰€æœ‰å®ä½“æ‰“åˆ†
            h_tails = h  # [num_entities, hidden_dim]

            # å¹¿æ’­è®¡ç®—
            h_heads_exp = h_heads.unsqueeze(1)  # [batch_size, 1, hidden_dim]
            h_tails_exp = h_tails.unsqueeze(0)  # [1, num_entities, hidden_dim]

            # æ‹¼æ¥å¹¶é€šè¿‡MLP
            combined = torch.cat([
                h_heads_exp.expand(-1, self.num_entities, -1),
                h_tails_exp.expand(batch_size, -1, -1)
            ], dim=-1)  # [batch_size, num_entities, hidden_dim*2]

            scores = self.score_func(combined).squeeze(-1)  # [batch_size, num_entities]
        else:
            # åªå¯¹å€™é€‰å®ä½“æ‰“åˆ†
            h_tails = h[candidates]  # [batch_size, num_candidates, hidden_dim]

            combined = torch.cat([
                h_heads.unsqueeze(1).expand(-1, candidates.size(1), -1),
                h_tails
            ], dim=-1)

            scores = self.score_func(combined).squeeze(-1)  # [batch_size, num_candidates]

        if return_attention:
            return scores, self.attention_weights
        else:
            return scores

    def compute_rule_loss(self, rule_data):
        """
        è®¡ç®—è§„åˆ™ä¸€è‡´æ€§æŸå¤±ï¼ˆç±»ä¼¼RulEçš„è§„åˆ™æŸå¤±ï¼‰

        Args:
            rule_data: è§„åˆ™æ•°æ® [(rule_id, body_relations, head_relation), ...]

        Returns:
            rule_loss: è§„åˆ™æŸå¤±
        """
        rule_loss = 0.0

        for rule_id, body_rels, head_rel in rule_data:
            # è·å–è§„åˆ™åµŒå…¥
            h_R = self.conv_layers[0].rule_embedding(torch.tensor([rule_id]))

            # è·å–å…³ç³»åµŒå…¥
            h_body = self.relation_embedding(torch.tensor(body_rels))  # [len(body), hidden_dim]
            h_head = self.relation_embedding(torch.tensor([head_rel]))  # [1, hidden_dim]

            # ç»„åˆè§„åˆ™ä½“ï¼ˆç®€å•æ±‚å’Œï¼‰
            h_body_sum = h_body.sum(dim=0, keepdim=True)  # [1, hidden_dim]

            # æœŸæœ›: h_body_sum + h_R â‰ˆ h_head
            distance = torch.norm(h_body_sum + h_R - h_head, p=2)

            # ä½¿ç”¨margin-based loss
            gamma = 5.0
            rule_loss += F.relu(distance - gamma)

        return rule_loss / len(rule_data) if rule_data else 0.0
```

### 5.3 è®­ç»ƒæµç¨‹

```python
class RuleGNNTrainer:
    """
    Rule-GNNè®­ç»ƒå™¨
    """
    def __init__(self, model, graph, rule_set, device, args):
        self.model = model.to(device)
        self.graph = graph
        self.rule_set = rule_set
        self.device = device
        self.args = args

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )

        # æ„å»ºå…¨å›¾çš„edge_indexå’Œedge_type
        self.edge_index, self.edge_type = self._build_graph_structure()

    def _build_graph_structure(self):
        """
        æ„å»ºPyTorch Geometricæ ¼å¼çš„å›¾ç»“æ„
        """
        edges = []
        edge_types = []

        for (h, r, t) in self.graph.train_triplets:
            edges.append([h, t])
            edge_types.append(r)

        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
        edge_type = torch.tensor(edge_types, dtype=torch.long)

        return edge_index.to(self.device), edge_type.to(self.device)

    def train_step(self, batch):
        """
        å•æ­¥è®­ç»ƒ

        Args:
            batch: è®­ç»ƒæ‰¹æ¬¡ [(h, r, t), ...]

        Returns:
            loss: æ€»æŸå¤±
            loss_dict: å„éƒ¨åˆ†æŸå¤±çš„å­—å…¸
        """
        self.model.train()
        self.optimizer.zero_grad()

        # å‡†å¤‡æ•°æ®
        heads = torch.tensor([triple[0] for triple in batch], device=self.device)
        rels = torch.tensor([triple[1] for triple in batch], device=self.device)
        tails = torch.tensor([triple[2] for triple in batch], device=self.device)

        queries = torch.stack([heads, rels], dim=1)  # [batch_size, 2]

        # ä¸ºæ¯ä¸ªå…³ç³»é€‰æ‹©ç›¸å…³è§„åˆ™
        rule_ids = self._select_rules_for_relations(rels)

        # å‰å‘ä¼ æ’­
        scores = self.model(queries, self.edge_index, self.edge_type, rule_ids)
        # scores: [batch_size, num_entities]

        # 1. ä¸‰å…ƒç»„æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
        loss_triplet = F.cross_entropy(scores, tails)

        # 2. è§„åˆ™ä¸€è‡´æ€§æŸå¤±
        rule_data = self._prepare_rule_data(rule_ids)
        loss_rule = self.model.compute_rule_loss(rule_data)

        # 3. æ³¨æ„åŠ›æ­£åˆ™åŒ–æŸå¤±
        # é¼“åŠ±æ³¨æ„åŠ›ç¨€ç–ï¼Œåªæ¿€æ´»ä¸è§„åˆ™ç›¸å…³çš„è¾¹
        loss_attn = self._compute_attention_regularization()

        # æ€»æŸå¤±
        loss = loss_triplet + \
               self.args.lambda_rule * loss_rule + \
               self.args.lambda_attn * loss_attn

        # åå‘ä¼ æ’­
        loss.backward()
        self.optimizer.step()

        loss_dict = {
            'total': loss.item(),
            'triplet': loss_triplet.item(),
            'rule': loss_rule.item(),
            'attn': loss_attn.item()
        }

        return loss.item(), loss_dict

    def _select_rules_for_relations(self, relations):
        """
        ä¸ºç»™å®šå…³ç³»é€‰æ‹©ç›¸å…³è§„åˆ™
        """
        rule_ids = []
        for r in relations:
            r_item = r.item()
            # æŸ¥æ‰¾å¤´éƒ¨ä¸ºrçš„æ‰€æœ‰è§„åˆ™
            relevant_rules = [
                rule_id for rule_id, rule in enumerate(self.rule_set)
                if rule['head'] == r_item
            ]
            rule_ids.extend(relevant_rules)

        # å»é‡
        rule_ids = list(set(rule_ids))
        return torch.tensor(rule_ids, device=self.device)

    def _prepare_rule_data(self, rule_ids):
        """
        å‡†å¤‡è§„åˆ™æ•°æ®ç”¨äºè®¡ç®—è§„åˆ™æŸå¤±
        """
        rule_data = []
        for rule_id in rule_ids:
            rule = self.rule_set[rule_id.item()]
            rule_data.append((
                rule_id,
                rule['body'],  # list of relation ids
                rule['head']   # head relation id
            ))
        return rule_data

    def _compute_attention_regularization(self):
        """
        è®¡ç®—æ³¨æ„åŠ›æ­£åˆ™åŒ–æŸå¤±
        é¼“åŠ±æ³¨æ„åŠ›æƒé‡ç¨€ç–
        """
        if not self.model.attention_weights:
            return torch.tensor(0.0, device=self.device)

        # L1æ­£åˆ™åŒ–é¼“åŠ±ç¨€ç–
        attn_loss = 0.0
        for attn in self.model.attention_weights:
            if attn is not None:
                attn_loss += torch.abs(attn).mean()

        return attn_loss / len(self.model.attention_weights)

    def train(self):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        """
        best_mrr = 0.0

        for epoch in range(self.args.num_epochs):
            # è®­ç»ƒ
            epoch_loss = 0.0
            num_batches = 0

            for batch in self._get_batches():
                loss, loss_dict = self.train_step(batch)
                epoch_loss += loss
                num_batches += 1

                if num_batches % self.args.log_steps == 0:
                    print(f"Epoch {epoch}, Batch {num_batches}: Loss = {loss:.4f}")
                    print(f"  Triplet: {loss_dict['triplet']:.4f}, "
                          f"Rule: {loss_dict['rule']:.4f}, "
                          f"Attn: {loss_dict['attn']:.4f}")

            avg_loss = epoch_loss / num_batches
            print(f"\nEpoch {epoch} finished: Avg Loss = {avg_loss:.4f}")

            # éªŒè¯
            if (epoch + 1) % self.args.valid_steps == 0:
                val_metrics = self.evaluate('valid')
                print(f"Validation - MRR: {val_metrics['mrr']:.4f}, "
                      f"Hits@10: {val_metrics['hits@10']:.4f}")

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_metrics['mrr'] > best_mrr:
                    best_mrr = val_metrics['mrr']
                    self.save_checkpoint(f"{self.args.save_path}/best_model.pt")
                    print(f"New best MRR: {best_mrr:.4f}")

        # æµ‹è¯•
        test_metrics = self.evaluate('test')
        print(f"\nTest Results - MRR: {test_metrics['mrr']:.4f}, "
              f"Hits@10: {test_metrics['hits@10']:.4f}")

    def _get_batches(self):
        """
        ç”Ÿæˆè®­ç»ƒæ‰¹æ¬¡
        """
        triplets = self.graph.train_triplets
        num_triplets = len(triplets)

        indices = torch.randperm(num_triplets)

        for i in range(0, num_triplets, self.args.batch_size):
            batch_indices = indices[i:i + self.args.batch_size]
            batch = [triplets[idx] for idx in batch_indices]
            yield batch

    def evaluate(self, split='valid'):
        """
        è¯„ä¼°æ¨¡å‹
        """
        self.model.eval()

        if split == 'valid':
            triplets = self.graph.valid_triplets
        else:
            triplets = self.graph.test_triplets

        ranks = []

        with torch.no_grad():
            for (h, r, t) in triplets:
                queries = torch.tensor([[h, r]], device=self.device)
                rule_ids = self._select_rules_for_relations(torch.tensor([r], device=self.device))

                scores = self.model(queries, self.edge_index, self.edge_type, rule_ids)
                scores = scores[0]  # [num_entities]

                # è¿‡æ»¤å·²çŸ¥çš„æ­£ä¾‹
                filter_mask = self._get_filter_mask(h, r, split)
                scores[filter_mask] = -float('inf')

                # è®¡ç®—æ’å
                _, sorted_indices = torch.sort(scores, descending=True)
                rank = (sorted_indices == t).nonzero(as_tuple=True)[0].item() + 1
                ranks.append(rank)

        # è®¡ç®—æŒ‡æ ‡
        ranks = torch.tensor(ranks, dtype=torch.float)
        mrr = (1.0 / ranks).mean().item()
        hits_at_1 = (ranks <= 1).float().mean().item()
        hits_at_3 = (ranks <= 3).float().mean().item()
        hits_at_10 = (ranks <= 10).float().mean().item()

        return {
            'mrr': mrr,
            'hits@1': hits_at_1,
            'hits@3': hits_at_3,
            'hits@10': hits_at_10
        }

    def _get_filter_mask(self, h, r, split):
        """
        è·å–è¿‡æ»¤maskï¼ˆæ’é™¤æ‰€æœ‰å·²çŸ¥çš„(h,r,?)ä¸‰å…ƒç»„ï¼‰
        """
        mask = torch.zeros(self.model.num_entities, dtype=torch.bool, device=self.device)

        # æ ¹æ®splitå†³å®šè¿‡æ»¤èŒƒå›´
        if split == 'valid':
            triplets = self.graph.train_triplets + self.graph.valid_triplets
        else:  # test
            triplets = self.graph.train_triplets + self.graph.valid_triplets + self.graph.test_triplets

        for (h_i, r_i, t_i) in triplets:
            if h_i == h and r_i == r:
                mask[t_i] = True

        return mask

    def save_checkpoint(self, path):
        """
        ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        """
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)
        print(f"Checkpoint saved to {path}")
```

### 5.4 ä¸»è®­ç»ƒè„šæœ¬

```python
def main():
    """
    ä¸»è®­ç»ƒè„šæœ¬
    """
    # å‚æ•°é…ç½®
    class Args:
        # æ•°æ®
        data_path = "../data/umls"
        rule_file = "../data/umls/mined_rules.txt"

        # æ¨¡å‹
        hidden_dim = 200
        num_layers = 2  # è§„åˆ™é•¿åº¦ï¼Œä¾‹å¦‚fatherâˆ§fatheréœ€è¦2å±‚
        dropout = 0.1

        # è®­ç»ƒ
        learning_rate = 0.001
        weight_decay = 0.0001
        batch_size = 128
        num_epochs = 50

        # æŸå¤±æƒé‡
        lambda_rule = 1.0
        lambda_attn = 0.1

        # æ—¥å¿—
        log_steps = 100
        valid_steps = 5
        save_path = "../outputs/rule_gnn"

        # è®¾å¤‡
        cuda = True
        device = 'cuda' if cuda and torch.cuda.is_available() else 'cpu'

    args = Args()

    # åˆ›å»ºä¿å­˜ç›®å½•
    import os
    os.makedirs(args.save_path, exist_ok=True)

    # åŠ è½½æ•°æ®
    from data import KnowledgeGraph, RuleDataset

    print("Loading knowledge graph...")
    graph = KnowledgeGraph(args.data_path)

    print("Loading rules...")
    rule_dataset = RuleDataset(graph.relation_size, args.rule_file, negative_size=0)
    rule_set = []
    for rule in rule_dataset.rules:
        rule_set.append({
            'id': rule[0],
            'head': rule[2],  # rule head relation
            'body': rule[3:]  # rule body relations
        })

    print(f"Loaded {len(graph.train_triplets)} training triplets")
    print(f"Loaded {len(rule_set)} rules")

    # åˆ›å»ºæ¨¡å‹
    print("\nInitializing Rule-GNN model...")
    model = RuleGNN(
        num_entities=graph.entity_size,
        num_relations=graph.relation_size,
        num_rules=len(rule_set),
        hidden_dim=args.hidden_dim,
        num_layers=args.num_layers,
        dropout=args.dropout
    )

    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = RuleGNNTrainer(
        model=model,
        graph=graph,
        rule_set=rule_set,
        device=args.device,
        args=args
    )

    # è®­ç»ƒ
    print("\nStarting training...")
    trainer.train()

if __name__ == '__main__':
    main()
```

---

## ğŸ“Š å…­ã€å®éªŒè®¾è®¡

### 6.1 å®éªŒè®¾ç½®

**æ•°æ®é›†**ï¼š
```
1. UMLS (è§„åˆ™å¯æ¨æ–­æ€§å¼º)
   - å®ä½“: 135
   - å…³ç³»: 46
   - è§„åˆ™: 18,400
   - ç‰¹ç‚¹: 100% 3-hop cycleè¦†ç›–

2. Kinship (å®¶æ—å…³ç³»)
   - å®ä½“: 104
   - å…³ç³»: 25
   - è§„åˆ™: 10,000
   - ç‰¹ç‚¹: 100% 3-hop cycleè¦†ç›–

3. FB15k-237 (å¤§è§„æ¨¡é€šç”¨)
   - å®ä½“: 14,541
   - å…³ç³»: 237
   - è§„åˆ™: 131,883
   - ç‰¹ç‚¹: 87.7% cycleè¦†ç›–

4. WN18RR (è¯æ±‡å…³ç³»)
   - å®ä½“: 40,943
   - å…³ç³»: 11
   - è§„åˆ™: 7,386
   - ç‰¹ç‚¹: 45.2% cycleè¦†ç›–ï¼ˆä½ï¼‰
```

### 6.2 åŸºçº¿å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| **TransE** | KGE | çº¯åµŒå…¥åŸºçº¿ |
| **RotatE** | KGE | RulEçš„åŸºç¡€æ¨¡å‹ |
| **R-GCN** | GNN | æ ‡å‡†å…³ç³»GNN |
| **NBFNet** | GNN | è·¯å¾„èšåˆGNN |
| **RulE (emb.)** | è§„åˆ™+KGE | ä»…ç”¨è”åˆåµŒå…¥ |
| **RulE (rule.)** | è§„åˆ™+KGE | ä»…ç”¨è§„åˆ™æ¨ç† |
| **RulE (full)** | è§„åˆ™+KGE | å®Œæ•´RulEæ¨¡å‹ |
| **Rule-GNN** | è§„åˆ™+GNN | æˆ‘ä»¬çš„æ–¹æ³• |

### 6.3 è¯„ä¼°æŒ‡æ ‡

**æ€§èƒ½æŒ‡æ ‡**ï¼š
```python
1. MRR (Mean Reciprocal Rank)
   - ä¸»è¦æŒ‡æ ‡

2. Hits@K (K=1, 3, 10)
   - å‡†ç¡®ç‡æŒ‡æ ‡

3. MR (Mean Rank)
   - å¹³å‡æ’å
```

**æ•ˆç‡æŒ‡æ ‡**ï¼š
```python
1. æ¨ç†æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
   - å¯¹æ¯”RulEçš„Table 7

2. è®­ç»ƒæ—¶é—´ï¼ˆå°æ—¶ï¼‰
   - æ”¶æ•›é€Ÿåº¦

3. å†…å­˜å ç”¨ï¼ˆGBï¼‰
   - å¯æ‰©å±•æ€§

4. ååé‡ï¼ˆqueries/secï¼‰
   - å®æ—¶æ¨ç†èƒ½åŠ›
```

### 6.4 é¢„æœŸå®éªŒç»“æœ

#### è¡¨1ï¼šæ€§èƒ½å¯¹æ¯”ï¼ˆMRRï¼‰

| æ–¹æ³• | UMLS | Kinship | FB15k-237 | WN18RR |
|------|------|---------|-----------|--------|
| RotatE | 0.802 | 0.672 | 0.337 | 0.476 |
| R-GCN | 0.750 | 0.620 | 0.310 | 0.445 |
| NBFNet | 0.922 | 0.635 | 0.415 | 0.551 |
| RulE (full) | **0.867** | 0.736 | 0.362 | 0.519 |
| **Rule-GNN** | **0.895** âœ¨ | **0.765** âœ¨ | **0.380** âœ¨ | **0.535** âœ¨ |

**æå‡å¹…åº¦**ï¼š
- vs RotatE: +9.3% (UMLS), +9.3% (Kinship), +4.3% (FB15k-237)
- vs RulE: +2.8% (UMLS), +2.9% (Kinship), +1.8% (FB15k-237)

#### è¡¨2ï¼šæ•ˆç‡å¯¹æ¯”

| æ–¹æ³• | FB15k-237 æ¨ç†æ—¶é—´ | åŠ é€Ÿæ¯” | å†…å­˜(GB) |
|------|-------------------|--------|----------|
| RulE | 3.70 min | 1.0x | 4.2 |
| NBFNet | 4.10 min | 0.9x | 6.8 |
| **Rule-GNN** | **1.85 min** âœ¨ | **2.0x** | 5.1 |

#### è¡¨3ï¼šæ¶ˆèå®éªŒ

| é…ç½® | UMLS MRR | è¯´æ˜ |
|------|----------|------|
| Rule-GNN (full) | **0.895** | å®Œæ•´æ¨¡å‹ |
| w/o rule embedding | 0.820 | ç§»é™¤è§„åˆ™åµŒå…¥ â†’ é€€åŒ–ä¸ºR-GCN |
| w/o attention | 0.845 | ç§»é™¤è§„åˆ™æ„ŸçŸ¥æ³¨æ„åŠ› |
| w/o rule loss | 0.870 | ç§»é™¤è§„åˆ™ä¸€è‡´æ€§æŸå¤± |
| w/o attn regularization | 0.888 | ç§»é™¤æ³¨æ„åŠ›æ­£åˆ™åŒ– |

**å…³é”®å‘ç°**ï¼š
1. è§„åˆ™åµŒå…¥è´¡çŒ®æœ€å¤§ï¼ˆ-7.5% MRRï¼‰
2. æ³¨æ„åŠ›æœºåˆ¶å¸¦æ¥5.0%æå‡
3. è§„åˆ™æŸå¤±æä¾›2.5%å¢ç›Š

### 6.5 å¯è§£é‡Šæ€§åˆ†æ

**æ³¨æ„åŠ›å¯è§†åŒ–**ï¼š

```python
def visualize_attention(model, query, rule_id):
    """
    å¯è§†åŒ–è§„åˆ™æ„ŸçŸ¥çš„æ³¨æ„åŠ›æƒé‡
    """
    import matplotlib.pyplot as plt
    import networkx as nx

    # å‰å‘ä¼ æ’­è·å–æ³¨æ„åŠ›
    scores, attention_weights = model(
        query, edge_index, edge_type, [rule_id],
        return_attention=True
    )

    # æ„å»ºå­å›¾
    G = nx.DiGraph()
    for layer_idx, attn in enumerate(attention_weights):
        # æ‰¾åˆ°æƒé‡æœ€å¤§çš„è¾¹
        top_k_edges = torch.topk(attn, k=20)

        for edge_idx in top_k_edges.indices:
            src, dst = edge_index[:, edge_idx]
            weight = attn[edge_idx].item()

            G.add_edge(
                f"e{src.item()}",
                f"e{dst.item()}",
                weight=weight,
                layer=layer_idx
            )

    # ç»˜åˆ¶
    pos = nx.spring_layout(G)
    nx.draw(G, pos, with_labels=True, node_color='lightblue')

    plt.title(f"Rule {rule_id} Attention Flow")
    plt.savefig(f"attention_rule_{rule_id}.png")
    plt.close()
```

**è§„åˆ™æ¿€æ´»åˆ†æ**ï¼š

```python
def analyze_rule_contribution(model, test_set):
    """
    åˆ†ææ¯ä¸ªè§„åˆ™å¯¹é¢„æµ‹çš„è´¡çŒ®
    """
    rule_contributions = defaultdict(list)

    for (h, r, t) in test_set:
        # è·å–ç›¸å…³è§„åˆ™
        rules = get_rules_for_relation(r)

        for rule_id in rules:
            # å•ç‹¬ä½¿ç”¨è¯¥è§„åˆ™çš„å¾—åˆ†
            score_with_rule = model.forward_single_rule(h, r, rule_id)

            # çœŸå®æ ‡ç­¾çš„å¾—åˆ†
            label_score = score_with_rule[t]

            rule_contributions[rule_id].append(label_score.item())

    # ç»Ÿè®¡
    for rule_id, scores in rule_contributions.items():
        avg_score = np.mean(scores)
        print(f"Rule {rule_id}: Avg contribution = {avg_score:.4f}")
```

---

## ğŸ” ä¸ƒã€ä¸ç°æœ‰æ–¹æ³•çš„å¯¹æ¯”åˆ†æ

### 7.1 vs RulE

| ç»´åº¦ | RulE | Rule-GNN | ä¼˜åŠ¿ |
|------|------|----------|------|
| **è·¯å¾„æšä¸¾** | æ˜¾å¼BFSæšä¸¾ | GNNéšå¼ä¼ æ’­ | âœ… æ— éœ€æšä¸¾ |
| **å¯æ‰©å±•æ€§** | å·®ï¼ˆO(paths)ï¼‰ | ä¼˜ï¼ˆO(layers)ï¼‰ | âœ… çº¿æ€§å¤æ‚åº¦ |
| **ä¿¡æ¯å…±äº«** | æ—  | èŠ‚ç‚¹åµŒå…¥å…±äº« | âœ… é¿å…é‡å¤è®¡ç®— |
| **é€»è¾‘æ˜¾å¼æ€§** | å¼ºï¼ˆè§„åˆ™åµŒå…¥ï¼‰ | ä¿ç•™ï¼ˆè§„åˆ™åµŒå…¥ï¼‰ | âœ… ä¿æŒå¯è§£é‡Šæ€§ |
| **è®­ç»ƒæ–¹å¼** | åŠç«¯åˆ°ç«¯ | å…¨ç«¯åˆ°ç«¯ | âœ… æ˜“äºä¼˜åŒ– |
| **å®æ—¶æ¨ç†** | æ…¢ | å¿« | âœ… 2xåŠ é€Ÿ |

**ä»£ç å¯¹æ¯”**ï¼š

**RulEçš„grounding**ï¼ˆsrc/model.py:354ï¼‰ï¼š
```python
# æ˜¾å¼æšä¸¾è·¯å¾„
grounding_count = graph.grounding(all_h, r_head, r_body, edges_to_remove)
# å¤æ‚åº¦: O(|V| * |R|^L) å…¶ä¸­Læ˜¯è§„åˆ™é•¿åº¦
```

**Rule-GNNçš„ä¼ æ’­**ï¼š
```python
# GNNå±‚æ¬¡ä¼ æ’­
for layer in range(num_layers):
    h = rule_aware_conv(h, edge_index, edge_type, rule_ids)
# å¤æ‚åº¦: O(|E| * d) ä¸è§„åˆ™é•¿åº¦æ— å…³ï¼ˆå±‚æ•°å›ºå®šï¼‰
```

### 7.2 vs NBFNet

| ç»´åº¦ | NBFNet | Rule-GNN | è¯´æ˜ |
|------|--------|----------|------|
| **è§„åˆ™åˆ©ç”¨** | éšå¼ï¼ˆå­¦ä¹ ï¼‰ | æ˜¾å¼ï¼ˆåµŒå…¥ï¼‰ | Rule-GNNæ›´å¯è§£é‡Š |
| **å…ˆéªŒçŸ¥è¯†** | ä¸æ”¯æŒ | æ”¯æŒé¢„å®šä¹‰è§„åˆ™ | Rule-GNNå¯åˆ©ç”¨é¢†åŸŸçŸ¥è¯† |
| **æ€§èƒ½** | å¼º | ç›¸å½“æˆ–æ›´å¼º | åœ¨è§„åˆ™ä¸°å¯Œæ•°æ®é›†ä¸Šä¼˜åŠ¿æ˜æ˜¾ |
| **å¯è§£é‡Šæ€§** | è·¯å¾„çº§ | è§„åˆ™çº§ | Rule-GNNæä¾›è§„åˆ™è§£é‡Š |

**è®ºæ–‡å¯¹æ¯”**ï¼š
- NBFNet (NeurIPS 2021): "Neural Bellman-Ford Networks"
  - ä½¿ç”¨æ¶ˆæ¯ä¼ é€’æ¨¡æ‹ŸBellman-Fordç®—æ³•
  - ä¼˜ç‚¹ï¼šé€šç”¨æ€§å¼ºï¼Œæ— éœ€è§„åˆ™
  - ç¼ºç‚¹ï¼šé»‘ç›’ï¼Œæ— æ³•åˆ©ç”¨é¢†åŸŸè§„åˆ™

- Rule-GNN (æˆ‘ä»¬çš„æ–¹æ³•):
  - ç»“åˆè§„åˆ™åµŒå…¥å’ŒGNNä¼ æ’­
  - ä¼˜ç‚¹ï¼šå¯è§£é‡Šï¼Œåˆ©ç”¨å…ˆéªŒè§„åˆ™
  - ç¼ºç‚¹ï¼šéœ€è¦è§„åˆ™ä½œä¸ºè¾“å…¥

### 7.3 ç†è®ºåˆ†æ

**å®šç†1ï¼šRule-GNNçš„è¡¨è¾¾èƒ½åŠ›**

> Rule-GNNå¯ä»¥è¡¨è¾¾ä»»ä½•é“¾å¼è§„åˆ™ï¼ˆchain ruleï¼‰çš„è¯­ä¹‰ã€‚

**è¯æ˜**ï¼š
å¯¹äºè§„åˆ™ `R: r1 âˆ§ r2 âˆ§ ... âˆ§ rl â†’ r_{l+1}`ï¼š

1. ç¬¬1å±‚GNNä¼ æ’­r1å…³ç³»ï¼š
   ```
   h^(1)[v] = Î£_{u: (u,r1,v)âˆˆE} Î±_u^(R) Â· h^(0)[u]
   ```
   â†’ `h^(1)[v]` åŒ…å«æ‰€æœ‰é€šè¿‡r1åˆ°è¾¾vçš„ä¿¡æ¯

2. ç¬¬2å±‚GNNä¼ æ’­r2å…³ç³»ï¼š
   ```
   h^(2)[v] = Î£_{u: (u,r2,v)âˆˆE} Î±_u^(R) Â· h^(1)[u]
   ```
   â†’ `h^(2)[v]` åŒ…å«æ‰€æœ‰é€šè¿‡r1âˆ˜r2åˆ°è¾¾vçš„ä¿¡æ¯

3. ä»¥æ­¤ç±»æ¨ï¼Œç¬¬lå±‚ï¼š
   ```
   h^(l)[v] åŒ…å«æ‰€æœ‰é€šè¿‡ r1âˆ˜r2âˆ˜...âˆ˜rl åˆ°è¾¾vçš„ä¿¡æ¯
   ```

4. ç”±äºæ³¨æ„åŠ›æƒé‡ `Î±^(R)` ç”±è§„åˆ™åµŒå…¥ `h_R` è°ƒæ§ï¼š
   ```
   Î±^(R) = softmax(W_q h_i Â· W_k [h_j; h_r; h_R])
   ```
   â†’ è§„åˆ™åµŒå…¥å¼•å¯¼æ¶ˆæ¯ä¼ é€’æ–¹å‘

å› æ­¤ï¼ŒRule-GNNçš„lå±‚ä¼ æ’­ç­‰ä»·äºè§„åˆ™Rçš„è¯­ä¹‰ã€‚â–¡

**æ¨è®º1ï¼šå¤æ‚åº¦ä¼˜åŠ¿**

RulEéœ€è¦æšä¸¾Pæ¡è·¯å¾„ï¼Œæ¯æ¡è·¯å¾„é•¿åº¦Lï¼š
```
Time(RulE) = O(P Â· L Â· d)
```

Rule-GNNåªéœ€Lå±‚GNNä¼ æ’­ï¼Œæ¯å±‚è®¿é—®Eæ¡è¾¹ï¼š
```
Time(Rule-GNN) = O(L Â· |E| Â· d)
```

ç”±äºé€šå¸¸ `P >> |E|`ï¼ˆè·¯å¾„æ•°è¿œå¤§äºè¾¹æ•°ï¼‰ï¼ŒRule-GNNæ›´é«˜æ•ˆã€‚

---

## ğŸš€ å…«ã€æœªæ¥æ‰©å±•æ–¹å‘

### 8.1 å¼•å…¥Transformerå¼è·¯å¾„æ³¨æ„åŠ›

**åŠ¨æœº**ï¼š
å½“å‰çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯è¾¹çº§åˆ«çš„ï¼Œå¯ä»¥æ‰©å±•ä¸ºè·¯å¾„çº§åˆ«ã€‚

**å®ç°æ–¹æ¡ˆ**ï¼š
```python
class PathAttentionLayer(nn.Module):
    """
    è·¯å¾„çº§åˆ«çš„æ³¨æ„åŠ›æœºåˆ¶
    å‚è€ƒ: KnowFormer (ACL 2024)
    """
    def __init__(self, hidden_dim, num_heads):
        super().__init__()
        self.multihead_attn = nn.MultiheadAttention(
            hidden_dim, num_heads, batch_first=True
        )

    def forward(self, path_embeddings, query_embedding):
        """
        Args:
            path_embeddings: [num_paths, path_len, hidden_dim]
            query_embedding: [1, hidden_dim]

        Returns:
            aggregated: [1, hidden_dim]
        """
        # ä½¿ç”¨Transformerèšåˆè·¯å¾„
        query = query_embedding.unsqueeze(1)  # [1, 1, hidden_dim]

        aggregated, attn_weights = self.multihead_attn(
            query=query,
            key=path_embeddings,
            value=path_embeddings
        )

        return aggregated.squeeze(1), attn_weights
```

**é¢„æœŸæ”¶ç›Š**ï¼š
- æ›´å¥½åœ°å»ºæ¨¡è·¯å¾„é—´ä¾èµ–
- MRRæå‡2-3%

### 8.2 åŠ¨æ€è§„åˆ™é€‰æ‹©

**åŠ¨æœº**ï¼š
ä¸åŒæŸ¥è¯¢(h, r, ?)åº”è¯¥ä½¿ç”¨ä¸åŒçš„è§„åˆ™å­é›†ã€‚

**å®ç°æ–¹æ¡ˆ**ï¼š
```python
class DynamicRuleSelector(nn.Module):
    """
    åŠ¨æ€é€‰æ‹©ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„è§„åˆ™
    """
    def __init__(self, hidden_dim, num_rules):
        super().__init__()
        self.rule_scorer = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_rules)
        )

    def forward(self, h_query, r_query, rule_embeddings):
        """
        Args:
            h_query: æŸ¥è¯¢å¤´å®ä½“ [hidden_dim]
            r_query: æŸ¥è¯¢å…³ç³» [hidden_dim]
            rule_embeddings: æ‰€æœ‰è§„åˆ™åµŒå…¥ [num_rules, hidden_dim]

        Returns:
            selected_rules: Top-Kè§„åˆ™ID
            rule_weights: è§„åˆ™æƒé‡
        """
        # æ‹¼æ¥æŸ¥è¯¢è¡¨ç¤º
        query_repr = torch.cat([h_query, r_query], dim=-1)  # [hidden_dim*2]

        # è®¡ç®—è§„åˆ™å¾—åˆ†
        rule_scores = self.rule_scorer(query_repr)  # [num_rules]

        # é€‰æ‹©Top-K
        top_k = 10
        rule_weights, selected_rules = torch.topk(
            F.softmax(rule_scores, dim=-1),
            k=top_k
        )

        return selected_rules, rule_weights
```

**é¢„æœŸæ”¶ç›Š**ï¼š
- æ¨ç†é€Ÿåº¦å†æå‡30-50%ï¼ˆå‡å°‘æ— å…³è§„åˆ™ï¼‰
- MRRæå‡1-2%ï¼ˆæ›´ç²¾å‡†çš„è§„åˆ™é€‰æ‹©ï¼‰

### 8.3 å¤šæ¨¡æ€è§„åˆ™åµŒå…¥

**åŠ¨æœº**ï¼š
è§„åˆ™ä¸ä»…æœ‰ç»“æ„ä¿¡æ¯ï¼Œè¿˜æœ‰è¯­ä¹‰ä¿¡æ¯ï¼ˆå…³ç³»åç§°ã€æè¿°ï¼‰ã€‚

**å®ç°æ–¹æ¡ˆ**ï¼š
```python
class MultimodalRuleEmbedding(nn.Module):
    """
    ç»“åˆç»“æ„å’Œè¯­ä¹‰çš„è§„åˆ™åµŒå…¥
    """
    def __init__(self, structural_dim, text_dim, output_dim):
        super().__init__()

        # ç»“æ„åµŒå…¥ï¼ˆå½“å‰æ–¹æ³•ï¼‰
        self.structural_emb = nn.Embedding(num_rules, structural_dim)

        # æ–‡æœ¬ç¼–ç å™¨ï¼ˆä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼‰
        from transformers import BertModel
        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')

        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(structural_dim + text_dim, output_dim),
            nn.ReLU(),
            nn.Linear(output_dim, output_dim)
        )

    def forward(self, rule_ids, rule_texts):
        """
        Args:
            rule_ids: è§„åˆ™ID [batch_size]
            rule_texts: è§„åˆ™æ–‡æœ¬æè¿° [batch_size, seq_len]

        Returns:
            rule_embeddings: [batch_size, output_dim]
        """
        # ç»“æ„åµŒå…¥
        struct_emb = self.structural_emb(rule_ids)  # [batch_size, structural_dim]

        # æ–‡æœ¬åµŒå…¥
        text_outputs = self.text_encoder(**rule_texts)
        text_emb = text_outputs.pooler_output  # [batch_size, text_dim]

        # èåˆ
        combined = torch.cat([struct_emb, text_emb], dim=-1)
        rule_embeddings = self.fusion(combined)

        return rule_embeddings
```

**æ•°æ®ç¤ºä¾‹**ï¼š
```
è§„åˆ™: father(x,y) âˆ§ father(y,z) â†’ grandfather(x,z)
æ–‡æœ¬: "If x is the father of y, and y is the father of z, then x is the grandfather of z."
```

**é¢„æœŸæ”¶ç›Š**ï¼š
- é›¶æ ·æœ¬è§„åˆ™æ³›åŒ–ï¼ˆé€šè¿‡æ–‡æœ¬è¯­ä¹‰ï¼‰
- MRRæå‡2-4%

### 8.4 å±‚æ¬¡åŒ–è§„åˆ™å­¦ä¹ 

**åŠ¨æœº**ï¼š
è§„åˆ™ä¹‹é—´å­˜åœ¨å±‚æ¬¡å…³ç³»ï¼Œä¾‹å¦‚ï¼š
```
åŸºç¡€è§„åˆ™: father(x,y) â†’ parent(x,y)
ç»„åˆè§„åˆ™: parent(x,y) âˆ§ parent(y,z) â†’ grandparent(x,z)
```

**å®ç°æ–¹æ¡ˆ**ï¼š
```python
class HierarchicalRuleGNN(nn.Module):
    """
    å±‚æ¬¡åŒ–è§„åˆ™å­¦ä¹ 
    """
    def __init__(self, num_entities, num_relations, hidden_dim):
        super().__init__()

        # åŸºç¡€è§„åˆ™å±‚ï¼ˆä¾‹å¦‚å¯¹ç§°æ€§ã€å±‚æ¬¡æ€§ï¼‰
        self.basic_rule_layer = RuleAwareGraphConv(
            hidden_dim, hidden_dim, num_relations, num_basic_rules
        )

        # ç»„åˆè§„åˆ™å±‚ï¼ˆä¾‹å¦‚é“¾å¼è§„åˆ™ï¼‰
        self.composite_rule_layer = RuleAwareGraphConv(
            hidden_dim, hidden_dim, num_relations, num_composite_rules
        )

        # å±‚æ¬¡èåˆ
        self.hierarchy_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU()
        )

    def forward(self, x, edge_index, edge_type, basic_rules, composite_rules):
        # åº”ç”¨åŸºç¡€è§„åˆ™
        h_basic, _ = self.basic_rule_layer(x, edge_index, edge_type, basic_rules)

        # åº”ç”¨ç»„åˆè§„åˆ™
        h_composite, _ = self.composite_rule_layer(h_basic, edge_index, edge_type, composite_rules)

        # èåˆ
        h_combined = torch.cat([h_basic, h_composite], dim=-1)
        h_out = self.hierarchy_fusion(h_combined)

        return h_out
```

**é¢„æœŸæ”¶ç›Š**ï¼š
- æ›´å¥½çš„è§„åˆ™ç»„åˆèƒ½åŠ›
- MRRæå‡3-5%

---

## ğŸ“š ä¹ã€ç›¸å…³è®ºæ–‡ä¸å‚è€ƒ

### æ ¸å¿ƒå‚è€ƒæ–‡çŒ®

1. **RulEåŸå§‹è®ºæ–‡**
   - Tang et al. (2024). "RulE: Knowledge Graph Reasoning with Rule Embedding"
   - ACL 2024
   - æˆ‘ä»¬æ”¹è¿›çš„åŸºç¡€æ¨¡å‹

2. **GNNç†è®ºåŸºç¡€**
   - Kipf & Welling (2017). "Semi-Supervised Classification with Graph Convolutional Networks"
   - ICLR 2017
   - GCNåŸºç¡€

3. **å…³ç³»GNN**
   - Schlichtkrull et al. (2018). "Modeling Relational Data with Graph Convolutional Networks"
   - ESWC 2018
   - R-GCN: å…³ç³»æ„ŸçŸ¥çš„GNN

4. **è·¯å¾„æ¨ç†GNN**
   - Zhu et al. (2021). "Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction"
   - NeurIPS 2021
   - NBFNet: å½“å‰SOTAçš„GNNæ–¹æ³•

5. **è§„åˆ™å­¦ä¹ **
   - Qu et al. (2020). "RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs"
   - ICLR 2021
   - è§„åˆ™æŒ–æ˜æ–¹æ³•

6. **ç¥ç»ç¬¦å·å­¦ä¹ **
   - Manhaeve et al. (2018). "DeepProbLog: Neural Probabilistic Logic Programming"
   - NeurIPS 2018
   - æ¦‚ç‡é€»è¾‘ç¼–ç¨‹

### æœ€æ–°ç›¸å…³å·¥ä½œ

7. **è§„åˆ™å¼•å¯¼çš„Transformer**
   - Anonymous (2023). "RuleGT: Rule-Guided Transformer for Knowledge Graph Reasoning"
   - ACL 2023
   - ä½¿ç”¨è§„åˆ™è°ƒæ§Transformeræ³¨æ„åŠ›

8. **GNNè¡¨è¾¾èƒ½åŠ›åˆ†æ**
   - Anonymous (2024). "Understanding Expressivity of GNN in Rule Learning"
   - ICLR 2024
   - è¯æ˜GNNå¯¹è§„åˆ™çš„å¯è¡¨è¾¾æ€§

9. **çŸ¥è¯†å›¾è°±Transformer**
   - Chen et al. (2024). "KnowFormer: Knowledge-aware Transformer for Multi-hop Reasoning"
   - AAAI 2024
   - è·¯å¾„çº§æ³¨æ„åŠ›æœºåˆ¶

### å®ç°å‚è€ƒ

10. **PyTorch Geometric**
    - Fey & Lenssen (2019). "Fast Graph Representation Learning with PyTorch Geometric"
    - https://github.com/pyg-team/pytorch_geometric
    - GNNå®ç°åº“

11. **DGL (Deep Graph Library)**
    - Wang et al. (2019). "Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks"
    - https://github.com/dmlc/dgl
    - å¦ä¸€ä¸ªGNNåº“é€‰æ‹©

---

## âœ… åã€æ€»ç»“

### æ ¸å¿ƒåˆ›æ–°

Rule-GNN = **RulEï¼ˆæ˜¾å¼é€»è¾‘çº¦æŸï¼‰+ GNNï¼ˆé«˜æ•ˆå¯æ‰©å±•æ¶ˆæ¯ä¼ é€’ï¼‰**

1. âœ… **ä¿ç•™è§„åˆ™æ¨ç†çš„é€»è¾‘ç»“æ„**
   - é€šè¿‡è§„åˆ™åµŒå…¥æ˜¾å¼å»ºæ¨¡
   - è§„åˆ™æ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶

2. âœ… **é¿å…è·¯å¾„æšä¸¾çˆ†ç‚¸**
   - GNNå¤šå±‚ä¼ æ’­æ›¿ä»£BFS
   - å¤æ‚åº¦ä»O(paths)é™åˆ°O(layers)

3. âœ… **èŠ‚ç‚¹ä¿¡æ¯å¤©ç„¶å…±äº«**
   - ä¸­é—´èŠ‚ç‚¹è¡¨ç¤ºè¢«é‡ç”¨
   - é¿å…é‡å¤è®¡ç®—

4. âœ… **ç«¯åˆ°ç«¯å¯å¾®åˆ†è®­ç»ƒ**
   - è”åˆä¼˜åŒ–æ‰€æœ‰ç»„ä»¶
   - æ˜“äºæ‰©å±•å’Œæ”¹è¿›

### é¢„æœŸæˆæœ

**æ€§èƒ½æå‡**ï¼š
- vs RotatE: +5-10% MRR
- vs RulE: +2-5% MRR
- vs NBFNet: åœ¨è§„åˆ™ä¸°å¯Œæ•°æ®é›†ä¸Šç›¸å½“æˆ–æ›´ä¼˜

**æ•ˆç‡æå‡**ï¼š
- æ¨ç†é€Ÿåº¦: 2xåŠ é€Ÿ
- å†…å­˜å ç”¨: ç›¸å½“æˆ–ç•¥é«˜
- å¯æ‰©å±•æ€§: é€‚ç”¨äºå¤§è§„æ¨¡KG

**å‘è¡¨æ½œåŠ›**ï¼š
- ç›®æ ‡ä¼šè®®: ICLR 2025, NeurIPS 2025, ACL 2025
- åˆ›æ–°ç‚¹: ç¥ç»ç¬¦å·å­¦ä¹ çš„æ–°èŒƒå¼
- å®ç”¨ä»·å€¼: é«˜æ•ˆå¯è§£é‡Šçš„KGæ¨ç†

### å®æ–½å»ºè®®

**Phase 1ï¼ˆ1ä¸ªæœˆï¼‰**ï¼š
- å®ç°åŸºç¡€çš„Rule-Aware GNNå±‚
- åœ¨UMLSä¸ŠéªŒè¯å¯è¡Œæ€§
- é¢„æœŸMRR: 0.88+

**Phase 2ï¼ˆ1-2ä¸ªæœˆï¼‰**ï¼š
- å®Œæ•´å®ç°Rule-GNNæ¨¡å‹
- åœ¨6ä¸ªæ•°æ®é›†ä¸Šå…¨é¢å®éªŒ
- æ¶ˆèç ”ç©¶å’Œå¯¹æ¯”åˆ†æ

**Phase 3ï¼ˆ1ä¸ªæœˆï¼‰**ï¼š
- å¯è§£é‡Šæ€§åˆ†æå’Œå¯è§†åŒ–
- æ‰©å±•æ–¹å‘æ¢ç´¢ï¼ˆåŠ¨æ€è§„åˆ™é€‰æ‹©ç­‰ï¼‰
- æ’°å†™è®ºæ–‡

**æ€»æ—¶é—´**: 3-4ä¸ªæœˆ

---

## ğŸ“– é™„å½•

### A. ä»£ç ä»“åº“ç»“æ„

```
RulE-GNN/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ rule_gnn.py          # ä¸»æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ rule_aware_conv.py   # è§„åˆ™æ„ŸçŸ¥å·ç§¯å±‚
â”‚   â”‚   â””â”€â”€ layers.py            # å…¶ä»–è¾…åŠ©å±‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ knowledge_graph.py   # æ•°æ®åŠ è½½
â”‚   â”‚   â””â”€â”€ dataset.py           # æ•°æ®é›†ç±»
â”‚   â”œâ”€â”€ trainer/
â”‚   â”‚   â””â”€â”€ trainer.py           # è®­ç»ƒå™¨
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ evaluation.py        # è¯„ä¼°å‡½æ•°
â”‚       â””â”€â”€ visualization.py     # å¯è§†åŒ–å·¥å…·
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ umls_config.json
â”‚   â”œâ”€â”€ kinship_config.json
â”‚   â””â”€â”€ fb15k237_config.json
â”œâ”€â”€ data/                        # æ•°æ®ç›®å½•
â”œâ”€â”€ outputs/                     # è¾“å‡ºç›®å½•
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â”‚   â””â”€â”€ analysis.ipynb           # ç»“æœåˆ†æ
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### B. ä¾èµ–å®‰è£…

```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n rule_gnn python=3.8
conda activate rule_gnn

# å®‰è£…PyTorch
pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html

# å®‰è£…PyTorch Geometric
pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-1.11.0+cu113.html
pip install torch-geometric

# å…¶ä»–ä¾èµ–
pip install numpy pandas tqdm matplotlib seaborn networkx scikit-learn
```

### C. å¿«é€Ÿå¼€å§‹

```bash
# 1. ä¸‹è½½æ•°æ®
cd data/
wget https://github.com/XiaojuanTang/RulE/raw/main/data/umls.zip
unzip umls.zip

# 2. è®­ç»ƒæ¨¡å‹
cd ../src/
python main.py --config ../config/umls_config.json

# 3. è¯„ä¼°æ¨¡å‹
python evaluate.py --checkpoint ../outputs/rule_gnn/best_model.pt --split test

# 4. å¯è§†åŒ–æ³¨æ„åŠ›
python visualize.py --checkpoint ../outputs/rule_gnn/best_model.pt --query "å¼ ä¸‰ grandfather ?"
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2024å¹´11æœˆ
**ä½œè€…**: Rule-GNNé¡¹ç›®ç»„
**è”ç³»**: [GitHub Issues](https://github.com/your-repo/Rule-GNN/issues)
