# 深度学习核心概念详解

## 🎓 基础概念

### 1. `loss.backward()` - 反向传播

#### 是什么？
反向传播（Backpropagation）是神经网络训练的核心算法，用于计算损失函数对每个参数的梯度。

#### 通俗解释
想象你在山上迷路了，想下山（最小化损失）：
- `loss.backward()` 就是计算"当前位置每个方向的坡度"
- 告诉你："往东走会下降3米，往西走会上升2米"
- 但**只是计算坡度**，还没有真正移动

#### 数学原理
```python
loss = 5.2  # 当前损失值

# 假设模型有参数 w1=2.0, w2=3.0
loss.backward()  # 使用链式法则计算梯度

# 计算后得到：
w1.grad = -0.5  # ∂loss/∂w1 = -0.5
w2.grad = 1.2   # ∂loss/∂w2 = 1.2
```

**含义**：
- `w1.grad = -0.5`：如果 w1 增加 0.1，loss 会减少约 0.05
- `w2.grad = 1.2`：如果 w2 增加 0.1，loss 会增加约 0.12

#### RulE代码中的例子

```python
# src/trainer.py:210
loss = loss_rule + loss_fact  # 总损失 = 规则损失 + 事实损失
loss.backward()  # ← 计算所有参数的梯度
```

计算梯度链：
```
loss
 ├─ loss_fact
 │   ├─ entity_embedding.weight.grad  ← 实体嵌入的梯度
 │   └─ relation_embedding.weight.grad ← 关系嵌入的梯度
 └─ loss_rule
     └─ rule_emb.weight.grad  ← 规则嵌入的梯度
```

---

### 2. `optimizer.step()` - 参数更新

#### 是什么？
根据计算好的梯度，实际更新模型参数。

#### 通俗解释
继续下山的比喻：
- `loss.backward()` 计算了坡度 → "知道往哪个方向走"
- `optimizer.step()` 真正迈步 → "实际移动"

#### 数学原理（以SGD为例）

```python
# 更新前
w1 = 2.0
w1.grad = -0.5  # 梯度（由backward计算）
learning_rate = 0.01

optimizer.step()  # 执行更新

# 更新后
w1 = w1 - learning_rate × w1.grad
   = 2.0 - 0.01 × (-0.5)
   = 2.0 + 0.005
   = 2.005  # ← 参数被更新了！
```

#### RulE代码中的例子

```python
# src/trainer.py:210-212 (预训练)
loss.backward()      # 1. 计算梯度
optimizer.step()     # 2. 更新参数 ← 这里真正改变模型！
```

```python
# src/trainer.py:389-393 (Grounding训练)
loss.backward()      # 1. 计算梯度
optimizer.step()     # 2. 更新MLP参数
optimizer.zero_grad()  # 3. 清空梯度，准备下一轮
```

#### 为什么评估时没有这两步？

```python
# src/trainer.py:226-327 (评估函数)
@torch.no_grad()  # ← 禁用梯度计算
def evaluate(self, split, expectation=True):
    model.eval()
    # ... 只有前向传播，计算预测结果 ...
    return mrr  # ← 没有 backward() 和 step()!
```

**评估时的流程**：
```
输入 → 模型前向传播 → 预测结果 → 计算指标(MRR) → 返回
                                    ↑
                        没有反向传播，参数不变！
```

---

### 3. Dropout - 随机失活

#### 是什么？
训练时**随机"关闭"**一部分神经元，防止过拟合。

#### 通俗解释
想象一个团队做项目：
- **训练时**：随机让50%的人"休假"，强迫其他人学会所有技能
- **测试时**：所有人都回来工作，团队能力最强

这样每个人都学会了多种技能，不会过度依赖某几个"大神"。

#### 可视化示例

```
训练时 (Dropout=0.5):
输入层: [1.0, 2.0, 3.0, 4.0]
       ↓  ×    ↓    ×     ← 随机关闭50%
隐藏层: [1.0, 0, 3.0, 0]

测试时 (Dropout关闭):
输入层: [1.0, 2.0, 3.0, 4.0]
       ↓   ↓    ↓    ↓    ← 全部激活
隐藏层: [1.0, 2.0, 3.0, 4.0]
```

#### 代码实现

```python
# 定义模型
self.dropout = nn.Dropout(p=0.5)

# 训练时
model.train()  # ← 启用Dropout
x = self.dropout(x)  # 随机丢弃50%的神经元

# 测试时
model.eval()  # ← 关闭Dropout
x = self.dropout(x)  # 不丢弃，但缩放输出
```

#### RulE中的使用

RulE模型中**没有使用Dropout**（因为嵌入层通常不需要），但在 `layers.py:18-21` 有Dropout的定义：

```python
class MLP(nn.Module):
    def __init__(self, ..., dropout=0):
        if dropout:
            self.dropout = nn.Dropout(dropout)  # 默认为0，不使用
```

---

### 4. BatchNorm - 批归一化

#### 是什么？
在训练过程中，对每个batch的数据进行归一化，加速训练和提高稳定性。

#### 通俗解释
考试前给每个班级的分数"标准化"：
- 班级A平均分80，标准差10
- 班级B平均分60，标准差5
- BatchNorm让两个班的分数都转换到均值0、方差1
- 这样不同班级可以"公平比较"

#### 数学原理

```python
# 训练时
batch = [50, 60, 70, 80, 90]  # 一个batch的激活值

# 1. 计算统计量
mean = 70
std = 14.14

# 2. 归一化
normalized = (batch - mean) / std
           = [-1.41, -0.71, 0, 0.71, 1.41]

# 3. 可学习的缩放和平移
output = γ × normalized + β  # γ和β是可学习参数
```

#### 训练 vs 测试的区别

```python
# 训练时
model.train()
bn = BatchNorm1d(...)
output = bn(x)  # 使用当前batch的均值和方差

# 测试时
model.eval()
output = bn(x)  # 使用训练时积累的全局均值和方差
```

**为什么测试时不同？**
- 训练时：每个batch都有足够样本计算统计量
- 测试时：可能只有1个样本，无法计算batch统计量
- 解决：使用训练过程中记录的**移动平均**统计量

#### RulE中的使用

```python
# src/layers.py:27-29
if batch_norm:
    self.batch_norms = nn.ModuleList()
    for i in range(len(self.dims) - 2):
        self.batch_norms.append(nn.BatchNorm1d(self.dims[i + 1]))
```

RulE的默认配置中**没有启用BatchNorm**。

---

## 🔄 `model.train()` vs `model.eval()`

### `model.train()` - 训练模式

```python
model.train()
```

**效果**：
- ✅ Dropout **启用** - 随机丢弃神经元
- ✅ BatchNorm 使用**当前batch**的统计量
- ✅ 梯度计算**启用**（如果没有 `@torch.no_grad()`）

**典型使用场景**：
```python
# src/trainer.py:137 (预训练)
model.train()
loss.backward()
optimizer.step()

# src/trainer.py:354 (Grounding训练)
model.train()
loss.backward()
optimizer.step()
```

---

### `model.eval()` - 评估模式

```python
model.eval()
```

**效果**：
- ❌ Dropout **关闭** - 使用所有神经元
- ❌ BatchNorm 使用**全局**统计量
- ⚠️ 梯度计算仍然**可能启用**（需要配合 `@torch.no_grad()`）

**典型使用场景**：
```python
# src/trainer.py:236 (评估)
@torch.no_grad()  # ← 完全禁用梯度
model.eval()      # ← 切换到评估模式
# 只做前向传播，不更新参数
```

---

## 📊 完整训练循环示例

### RulE预训练的一个训练步骤

```python
# src/trainer.py:126-224
def train_step(self, optimizer, ...):
    model = self.model
    model.train()  # 1️⃣ 设置为训练模式

    optimizer.zero_grad()  # 2️⃣ 清空上一步的梯度

    # 3️⃣ 前向传播
    positive_sample, negative_sample, ... = next(triplets_iterator)
    negative_fact_score, _ = model.compute_KGE((positive_sample, negative_sample), mode)
    positive_fact_score, ent = model.compute_KGE(positive_sample)

    # 4️⃣ 计算损失
    positive_fact_loss = - (subsampling_weight * positive_fact_score).sum()
    negative_fact_loss = - (subsampling_weight * negative_fact_score).sum()
    loss_fact = (positive_fact_loss + negative_fact_loss) / 2
    loss = loss_rule + loss_fact

    # 5️⃣ 反向传播 - 计算梯度
    loss.backward()  # ← 计算 ∂loss/∂w 对所有参数w

    # 6️⃣ 参数更新
    optimizer.step()  # ← 更新参数: w = w - lr × ∂loss/∂w

    return log  # 7️⃣ 返回损失值（用于日志）
```

### 对比：评估步骤

```python
# src/trainer.py:226-327
@torch.no_grad()  # 1️⃣ 禁用梯度计算（内存优化 + 防止更新）
def evaluate(self, split, expectation=True):
    model.eval()  # 2️⃣ 设置为评估模式

    # 3️⃣ 前向传播
    for batch in dataloader:
        all_h, all_r, all_t, flag = batch
        KGE_score = model.compute_g_KGE(all_h, all_r)
        logits = KGE_score
        concat_logits.append(logits)

    # 4️⃣ 计算指标
    ranks = []
    for k in range(concat_all_t.size(0)):
        val = concat_logits[k, t]
        L = (concat_logits[k][concat_flag[k]] > val).sum().item() + 1
        ranks += [[h, r, t, L, H]]

    # 5️⃣ 计算MRR
    mrr = sum([1.0 / rank for rank in ranks]) / len(ranks)

    return mrr  # 6️⃣ 只返回指标，没有 backward() 和 step()！
```

**关键区别**：
| 操作 | 训练 | 评估 |
|------|------|------|
| `model.train()` | ✅ | ❌ |
| `model.eval()` | ❌ | ✅ |
| `@torch.no_grad()` | ❌ | ✅ |
| `loss.backward()` | ✅ | ❌ |
| `optimizer.step()` | ✅ | ❌ |

---

## 🔄 训练-测试时机详解

### 问题：多少轮测试一次？

让我用UMLS日志来说明：

### 📈 预训练阶段 (30,000步)

```python
# src/trainer.py:93-122
for step in range(0, args.max_steps + 1):  # 0 到 30000
    log = self.train_step(...)  # 训练一步

    if step % args.valid_steps == 0:  # 每1000步验证一次
        mrr = self.evaluate("valid", ...)  # ← 在验证集上评估
        if mrr > best_mrr:
            save_model(...)  # 保存最优模型
```

**时间线**：
```
Step 0:     训练 → 验证集评估 (MRR=0.0589)
Step 1-999: 训练（1000步）
Step 1000:  验证集评估 (MRR=0.7914) ← 提升巨大！
Step 1001-1999: 训练
Step 2000:  验证集评估 (MRR=0.8000)
...
Step 17000: 验证集评估 (MRR=0.8091) ← 最优
...
Step 30000: 训练结束
```

**关键点**：
- ✅ 每1000步在**验证集**上评估一次
- ✅ 保存验证集上MRR最高的模型（Step 17000附近）
- ❌ **从不使用测试集**

---

### 🎯 Grounding阶段 (20轮)

```python
# src/trainer.py:305-331
for k in range(args.num_iters):  # 20轮
    self.train_step(...)  # 训练一轮（282个batch）

    valid_mrr_iter = self.evaluate('valid', ...)  # ← 每轮在验证集评估

    if valid_mrr_iter > best_valid_mrr:
        self.save(...)  # 保存最优模型
```

**时间线**：
```
Iter 1:  训练（282批） → 验证集评估 (MRR=0.7884)
Iter 2:  训练（282批） → 验证集评估 (MRR=0.7940)
Iter 3:  训练（282批） → 验证集评估 (MRR=0.7957)
...
Iter 15: 训练（282批） → 验证集评估 (MRR=0.8025) ← 最优
Iter 16: 训练（282批） → 验证集评估 (MRR=0.8020)
...
Iter 20: 训练（282批） → 验证集评估 (MRR=0.7966)
训练结束！
```

**关键点**：
- ✅ 每训练完一轮，在**验证集**上评估一次
- ✅ 保存验证集上MRR最高的模型（Iter 15）
- ❌ **从不使用测试集**

---

### 🏆 最终测试 (只一次)

```python
# src/trainer.py:338-343
# 训练完全结束后
checkpoint = torch.load(os.path.join(self.args.save_path, 'grounding.pt'))
self.model.load_state_dict(checkpoint['model'])  # 加载最优模型

# 第一次也是唯一一次使用测试集
test_mrr_iter = self.evaluate('valid', ...)   # 再次验证
test_mrr_iter = self.evaluate('test', ...)    # ← 测试集评估
test_mrr_iter = self.evaluate_t('test_kge', ...)  # 组合模式测试
```

**时间线**：
```
[预训练完成 + Grounding完成]
    ↓
加载验证集上最优的模型
    ↓
验证集最终评估: MRR=0.7963
    ↓
测试集评估（第1次）: MRR=0.7944  ← 仅规则
    ↓
测试集评估（第2次）: MRR=0.8617  ← 规则+KGE组合
    ↓
完成！报告最终结果
```

---

## 📊 完整训练流程图

```
┌─────────────────────────────────────────────────────────┐
│                  预训练阶段 (6小时)                      │
├─────────────────────────────────────────────────────────┤
│ 训练数据: train_facts (5216条)                          │
│ 目标: 学习实体、关系、规则嵌入                           │
│                                                         │
│ Step 0 ──────► 训练 ──────► 验证集评估                  │
│ Step 1-999 ──► 训练 (1000步)                           │
│ Step 1000 ───► 训练 ──────► 验证集评估 ──► 保存最优      │
│ Step 1001-1999 ► 训练                                  │
│ Step 2000 ───► 训练 ──────► 验证集评估 ──► 更新最优？    │
│ ...                                                    │
│ Step 17000 ──► 训练 ──────► 验证集评估 ──► 保存最优！    │
│ ...                                                    │
│ Step 30000 ──► 训练结束                                │
│                                                         │
│ 输出: checkpoint (验证集上最优的模型)                     │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                Grounding阶段 (1小时)                     │
├─────────────────────────────────────────────────────────┤
│ 训练数据: ground_train_facts (10432条，含逆关系)         │
│ 目标: 训练MLP参数（冻结嵌入）                            │
│                                                         │
│ Iter 1 ──► 训练(282批) ──► 验证集评估 ──► 保存最优       │
│ Iter 2 ──► 训练(282批) ──► 验证集评估 ──► 更新最优？     │
│ ...                                                    │
│ Iter 15 ─► 训练(282批) ──► 验证集评估 ──► 保存最优！     │
│ ...                                                    │
│ Iter 20 ─► 训练(282批) ──► 验证集评估 ──► 训练结束       │
│                                                         │
│ 输出: grounding.pt (验证集上最优的模型)                  │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                  最终测试 (一次性)                        │
├─────────────────────────────────────────────────────────┤
│ 加载 grounding.pt (验证集上最优)                         │
│         ↓                                               │
│ 验证集评估 (确认): MRR=0.7963                            │
│         ↓                                               │
│ 测试集评估 (仅规则): MRR=0.7944  ← 第一次用测试集！       │
│         ↓                                               │
│ 测试集评估 (规则+KGE): MRR=0.8617 ← 最终结果             │
│         ↓                                               │
│ 完成！发表论文 📝                                        │
└─────────────────────────────────────────────────────────┘
```

---

## 🎯 总结表格

### 训练vs评估对比

| 特性 | 训练 (train_step) | 评估 (evaluate) |
|------|------------------|----------------|
| **模式** | `model.train()` | `model.eval()` |
| **梯度** | 计算梯度 | `@torch.no_grad()` |
| **Dropout** | 启用 | 关闭 |
| **BatchNorm** | 用batch统计 | 用全局统计 |
| **前向传播** | ✅ | ✅ |
| **计算损失** | ✅ | ❌ |
| **反向传播** | `loss.backward()` | ❌ |
| **参数更新** | `optimizer.step()` | ❌ |
| **返回值** | 损失值 | 评估指标(MRR) |

### 数据集使用对比

| 阶段 | 训练数据 | 验证频率 | 测试频率 |
|------|---------|---------|---------|
| **预训练** | train_facts | 每1000步 | ❌ 不使用 |
| **Grounding** | ground_train_facts | 每1轮 | ❌ 不使用 |
| **最终测试** | ❌ 不训练 | 1次确认 | ✅ 1次评估 |

---

## 💡 关键要点

1. **`loss.backward()`**: 计算梯度，但不更新参数
2. **`optimizer.step()`**: 根据梯度更新参数
3. **Dropout/BatchNorm**: 训练和测试时行为不同，需要 `model.train()` / `model.eval()` 切换
4. **训练频率**:
   - 预训练每1000步验证一次
   - Grounding每1轮验证一次
   - 测试集只在最后用一次
5. **数据安全**: `@torch.no_grad()` + `model.eval()` 双重保护，评估时无法更新参数

希望这个详细解释帮助您理解了深度学习训练的核心机制！🎓
